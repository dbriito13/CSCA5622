{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9824880,"sourceType":"datasetVersion","datasetId":6024843},{"sourceId":9867252,"sourceType":"datasetVersion","datasetId":2401195},{"sourceId":9876776,"sourceType":"datasetVersion","datasetId":2490157}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":20755.809834,"end_time":"2024-12-02T19:05:01.903624","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-02T13:19:06.093790","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Nifty 50 Partial Replication \nIndex funds have become a household occurence in the financial sphere, especially for retail investors, due to their simplicity, diversification and low cost compared to actively managed funds. Following an index can be done in one of two ways:\n- **Full replication:** The most common approach, where a fund holds all constituent stocks present in the index, with the same weights as the index.\n- **Partial Replication:** With this approach, only a subset of the index's constituents is selected, which should aim to approximate the overall index performance.\n\nPartial replication can offer a number of benefits over full replication, but mainly:\n- Lower transaction costs: Reducing the number of stocks traded minimizes fees such as brokerage fees.\n- Reduced Trading fees: By potentially avoiding low-liquidity stocks, we avoid have wider bid-ask spreads, as well as having less market impact.\n- Higher flexibility: Funds can add other constraints on top of avoiding low-liquidity stocks, such as those on sectors or specific stocks, giving them better control over their risk management.\nOur project's goal is to apply supervised learning techniques to develop a partial replication strategy, minimizing the tracking error of our portfolio.","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.007739,"end_time":"2024-12-02T13:19:08.745185","exception":false,"start_time":"2024-12-02T13:19:08.737446","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Methodology\nThe project will focus on the NIFTY 100 index, which includes the top 100 companies in the National Stock Exchange of India (NSE) based on market capitalization.\nWe will begin by gathering the minute-level data for both the index as well as the constituents data. Combining the data into a single dataframe, and perform some basic cleaning tasks, removing certain columns with a large number of empty values, as well as rows without target value, etc. Since our goal is to reduce the number of constituents to use only a subset of the total 100 stocks, our models will follow a 2 step approach:\n1. Feature Selection: We will first apply different feature selection techniques, (Lasso regression, Correlation matrix, Mutual information) to identify the most relevant stocks for our model.\n2. Model Training: Once the most relevant features (stocks) are selected, we will train different supervised learning models on the reduced dataset, these models will predict the weights for each of the relevant stocks.\nTo ensure the robustness of the test results, while accounting for the nature of the data (ordered time series) we will use a **sliding window approach**, where:\n* A fixed-size window (roughly 1-year) is used to train the model.\n* The model will be tested on roughly the next 3 months.\n* The window will slide forward, repeating until we use all available data.\n\nFor a certain model, the overall out-of-sample error will be the average error over all the windows. The error we will aim to minimize is the **tracking error**, for simplicity models will minimize MSE. Finally, we will compare models by plotting the error vs the number of stocks, to visualize how it evolves as we reduce the size of the subset of constituents chosen.\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error, r2_score, make_scorer\nfrom sklearn.model_selection import cross_val_score, TimeSeriesSplit, ParameterGrid\nfrom sklearn.linear_model import LassoCV\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nfrom tqdm import tqdm\nimport warnings\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"always\", category=ConvergenceWarning)\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nimport xgboost as xgb\nfrom sklearn.feature_selection import SelectFromModel,mutual_info_regression","metadata":{"execution":{"iopub.status.busy":"2024-12-09T13:33:01.652831Z","iopub.execute_input":"2024-12-09T13:33:01.653494Z","iopub.status.idle":"2024-12-09T13:33:03.383928Z","shell.execute_reply.started":"2024-12-09T13:33:01.653454Z","shell.execute_reply":"2024-12-09T13:33:03.382944Z"},"papermill":{"duration":3.174104,"end_time":"2024-12-02T13:19:11.926534","exception":false,"start_time":"2024-12-02T13:19:08.752430","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Tracking Error\nTracking error is the metric most commonly used to judge how well a replicating portfolio tracks the benchmark index, for full replication we would expect this to be close to 0 (slightly higher than 0 due to transaction costs or rebalancing delays).\n\nThe tracking error is defined as the standard deviation of the differences between the index and replicating portfolio over a specified time period.\n\n$$\n\\text{Tracking Error} = \\sqrt{\\frac{\\sum_{t=1}^T \\left( R_{portfolio}(t) - R_{index}(t) \\right)^2}{T}}\n$$\n\nWhere: \n*  $R_{portfolio}(t)$ is the portfolio return at time $t$,  \n*  $R_{index}(t)$ is the index's return at time $t$,  \n*  $T$  is the total number of time periods.\n\nWe base ourselves on this metric, and add a small adjustment, where from the first return of the portfolio, the trading cost is subtracted, this is a simulated trading cost, which we will simply set to 0.2%.","metadata":{}},{"cell_type":"code","source":"def tracking_error(y_true, y_pred):\n\n    # Convert y_true and y_pred to pandas Series for easy handling of pct_change()\n    y_true = pd.Series(y_true)\n    y_pred = pd.Series(y_pred)\n    \n    # Calculate the difference between the percentage changes\n    diff_pct_change = y_true - y_pred    \n\n    # Return the standard deviation of the differences (tracking error)\n    return np.std(diff_pct_change.dropna())  # dropna() to avoid NaN values in pct_change","metadata":{"execution":{"iopub.status.busy":"2024-12-09T13:33:03.385889Z","iopub.execute_input":"2024-12-09T13:33:03.386451Z","iopub.status.idle":"2024-12-09T13:33:03.391733Z","shell.execute_reply.started":"2024-12-09T13:33:03.386408Z","shell.execute_reply":"2024-12-09T13:33:03.390744Z"},"papermill":{"duration":0.015566,"end_time":"2024-12-02T13:19:11.949194","exception":false,"start_time":"2024-12-02T13:19:11.933628","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def adj_tracking_error(y_true, X_test, model):\n    cost = 0.002  # 0.1% cost\n    X_test_adjusted = X_test.copy()\n    X_test_adjusted[0, :] -= cost  # Subtract cost from the first return of each stock\n\n    # Predict with adjustment\n    y_pred_adjusted = model.predict(X_test_adjusted)\n    return tracking_error(y_true, y_pred_adjusted)","metadata":{"execution":{"iopub.status.busy":"2024-12-09T13:33:03.392959Z","iopub.execute_input":"2024-12-09T13:33:03.393245Z","iopub.status.idle":"2024-12-09T13:33:03.407079Z","shell.execute_reply.started":"2024-12-09T13:33:03.393217Z","shell.execute_reply":"2024-12-09T13:33:03.406081Z"},"papermill":{"duration":0.014912,"end_time":"2024-12-02T13:19:11.970710","exception":false,"start_time":"2024-12-02T13:19:11.955798","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Data\nThe data for this project will be taken from 2 Kaggle datasets:\n\n* [Nifty 100 Constituents](https://www.kaggle.com/datasets/debashis74017/stock-market-data-nifty-50-stocks-1-min-data): This data set will be used to gather price data for all constituents for the NIFTY 100 index. The dataset contains a file per stock, which contains OHLC (Open, High, Low, Close) as well as Volume data.\n* [Nifty 100 Index Data](https://www.kaggle.com/datasets/debashis74017/nifty-50-minute-data): This data set will be used to get the price data for the NIFTY 100 index price.\n\nIn order to speed up the execution, an intermediate dataset is created where we keep only closing prices for all stocks, in one single file. This dataset is [Nifty 100 Closing Only Data](https://www.kaggle.com/datasets/danielbrito99/nifty-50-closing-data). The script used to extract this intermediate data, is described in the cell below.","metadata":{"papermill":{"duration":0.005994,"end_time":"2024-12-02T13:19:11.984132","exception":false,"start_time":"2024-12-02T13:19:11.978138","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"directory_path = '/kaggle/input/stock-market-data-nifty-50-stocks-1-min-data'\npattern = os.path.join(directory_path, \"*.csv\")  # Change \"*.csv\" to match other patterns if needed\nGet the list of files that match the pattern\nfile_list = glob.glob(pattern)\n\ndef fetch_data(cols, file_list):\n    df_list = []\n    for file in tqdm(file_list):\n        stock_name = os.path.basename(file).split(\"_\")[0]\n        file_df = pd.read_csv(file, index_col=0)\n        file_df = file_df[[cols]]\n        file_df.columns = pd.MultiIndex.from_product([[stock_name], file_df.columns])\n        df_list.append(file_df)\n    return pd.concat(df_list, axis=1)\n\ndf = fetch_data('close')\ndf.to_csv('/kaggle/input/nifty_50_closing.csv')","metadata":{"execution":{"iopub.status.busy":"2024-12-09T13:33:03.408616Z","iopub.execute_input":"2024-12-09T13:33:03.408909Z","iopub.status.idle":"2024-12-09T13:33:03.420882Z","shell.execute_reply.started":"2024-12-09T13:33:03.408879Z","shell.execute_reply":"2024-12-09T13:33:03.419748Z"},"papermill":{"duration":0.015652,"end_time":"2024-12-02T13:19:12.026995","exception":false,"start_time":"2024-12-02T13:19:12.011343","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nifty-50-closing-data/nifty_50_closing.csv', index_col=0, header=[0, 1])\ndf = df.xs('close', level=1, axis=1)\ndf.index = pd.to_datetime(df.index)\nnan_rows = df.isna().any(axis=1)\n\n# Get the number of rows with NaN value\nnum_nan_rows = nan_rows.sum()\n\n\nprint(f\"Number of rows with NaN values: {num_nan_rows}\")\n\nnifty_100_closing = pd.read_csv('/kaggle/input/nifty-50-minute-data/NIFTY 100_minute.csv', index_col=0)[['close']]\nnifty_100_closing.index = pd.to_datetime(nifty_100_closing.index)\nnifty_100_closing.rename(columns={'close': 'NIFTY_100'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-09T15:24:54.905293Z","iopub.execute_input":"2024-12-09T15:24:54.906043Z","iopub.status.idle":"2024-12-09T15:25:09.593562Z","shell.execute_reply.started":"2024-12-09T15:24:54.906007Z","shell.execute_reply":"2024-12-09T15:25:09.592687Z"},"papermill":{"duration":20.950899,"end_time":"2024-12-02T13:19:33.005118","exception":false,"start_time":"2024-12-02T13:19:12.054219","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Number of rows with NaN values: 639555\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"## Data Cleaning\nOur data cleaning will be simple, we will focus on remediating the high number of NaN's present in the dataset, to do so, we will:\n* First remove all columns that contain less than 90% of non-NaN values (or equivalently, over 10% NaN values)\n* We ","metadata":{}},{"cell_type":"code","source":"threshold = 0.25\n\n# Drop columns with more than the threshold percentage of missing values\ndf = df.dropna(thresh=int((1-threshold) * len(df)), axis=1)\n\nnan_rows = df.isna().any(axis=1)\n# Get the number of rows with NaN values\nnum_nan_rows = nan_rows.sum()\nprint(f\"Number of rows with NaN values: {num_nan_rows}\")\n# Forward fill NaN values\n\n# Select rows with any NaNs\nrows_with_nans = df[df.isna().any(axis=1)]\n\n# Print rows with NaNs\nrows_with_nans","metadata":{"execution":{"iopub.status.busy":"2024-12-09T15:25:09.595437Z","iopub.execute_input":"2024-12-09T15:25:09.596487Z","iopub.status.idle":"2024-12-09T15:25:10.088389Z","shell.execute_reply.started":"2024-12-09T15:25:09.596439Z","shell.execute_reply":"2024-12-09T15:25:10.087514Z"},"papermill":{"duration":0.644122,"end_time":"2024-12-02T13:19:35.807873","exception":false,"start_time":"2024-12-02T13:19:35.163751","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Number of rows with NaN values: 178385\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"                           APOLLOHOSP  BOSCHLTD  INDUSTOWER  BERGEPAINT  \\\ndate                                                                      \n2018-08-09 09:15:00+05:30      968.40  19277.95      291.30      316.30   \n2018-08-09 09:16:00+05:30      964.70  19280.00      290.80      315.85   \n2018-08-21 09:15:00+05:30     1192.45  18832.00      285.75      334.15   \n2018-08-21 09:16:00+05:30     1194.45  18869.00      286.10      334.35   \n2018-08-29 09:15:00+05:30     1188.00  18900.00      283.30      342.00   \n...                               ...       ...         ...         ...   \n2022-10-25 12:27:00+05:30         NaN       NaN         NaN         NaN   \n2022-10-25 12:28:00+05:30         NaN       NaN         NaN         NaN   \n2022-10-25 12:29:00+05:30         NaN       NaN         NaN         NaN   \n2022-10-25 12:30:00+05:30         NaN       NaN         NaN         NaN   \n2022-10-25 12:31:00+05:30         NaN       NaN         NaN         NaN   \n\n                           MARICO  ICICIPRULI    SBIN  SUNPHARMA  COALINDIA  \\\ndate                                                                          \n2018-08-09 09:15:00+05:30  361.50      412.30  311.40     574.20     277.45   \n2018-08-09 09:16:00+05:30  360.40      412.05  310.75     574.25     277.40   \n2018-08-21 09:15:00+05:30  363.10      389.80  307.75     626.55     286.00   \n2018-08-21 09:16:00+05:30  362.85      388.95  307.65     625.00     286.30   \n2018-08-29 09:15:00+05:30  371.50      381.15  307.95     626.00     292.10   \n...                           ...         ...     ...        ...        ...   \n2022-10-25 12:27:00+05:30     NaN         NaN     NaN        NaN        NaN   \n2022-10-25 12:28:00+05:30     NaN         NaN     NaN        NaN        NaN   \n2022-10-25 12:29:00+05:30     NaN         NaN     NaN        NaN        NaN   \n2022-10-25 12:30:00+05:30     NaN         NaN     NaN        NaN        NaN   \n2022-10-25 12:31:00+05:30     NaN         NaN     NaN        NaN        NaN   \n\n                            WIPRO  ...  BIOCON   DABUR   SAIL  BRITANNIA  \\\ndate                               ...                                     \n2018-08-09 09:15:00+05:30  208.20  ...  283.10  444.20  79.90    3160.00   \n2018-08-09 09:16:00+05:30  207.93  ...  283.15  442.75  79.65    3158.17   \n2018-08-21 09:15:00+05:30  214.59  ...  302.85  454.45  77.80    3390.67   \n2018-08-21 09:16:00+05:30  214.59  ...  303.10  454.10  77.65    3386.00   \n2018-08-29 09:15:00+05:30  223.20  ...  308.75  485.60  77.85    3388.42   \n...                           ...  ...     ...     ...    ...        ...   \n2022-10-25 12:27:00+05:30     NaN  ...     NaN     NaN    NaN        NaN   \n2022-10-25 12:28:00+05:30     NaN  ...     NaN     NaN    NaN        NaN   \n2022-10-25 12:29:00+05:30     NaN  ...     NaN     NaN    NaN        NaN   \n2022-10-25 12:30:00+05:30     NaN  ...     NaN     NaN    NaN        NaN   \n2022-10-25 12:31:00+05:30     NaN  ...     NaN     NaN    NaN        NaN   \n\n                           ADANIPORTS    ONGC  EICHERMOT   TECHM  JINDALSTEL  \\\ndate                                                                           \n2018-08-09 09:15:00+05:30      375.85  172.30    2760.70  656.25      220.05   \n2018-08-09 09:16:00+05:30      374.75  171.60    2764.25  654.20      219.10   \n2018-08-21 09:15:00+05:30      379.95  168.95    2854.25  702.30      206.30   \n2018-08-21 09:16:00+05:30      380.85  169.05    2858.05  700.20      207.20   \n2018-08-29 09:15:00+05:30      387.20  174.95    2879.35  740.25      208.30   \n...                               ...     ...        ...     ...         ...   \n2022-10-25 12:27:00+05:30         NaN     NaN        NaN     NaN         NaN   \n2022-10-25 12:28:00+05:30         NaN     NaN        NaN     NaN         NaN   \n2022-10-25 12:29:00+05:30         NaN     NaN        NaN     NaN         NaN   \n2022-10-25 12:30:00+05:30         NaN     NaN        NaN     NaN         NaN   \n2022-10-25 12:31:00+05:30         NaN     NaN        NaN     NaN         NaN   \n\n                           HEROMOTOCO  \ndate                                   \n2018-08-09 09:15:00+05:30     3295.40  \n2018-08-09 09:16:00+05:30     3306.00  \n2018-08-21 09:15:00+05:30     3318.95  \n2018-08-21 09:16:00+05:30     3318.05  \n2018-08-29 09:15:00+05:30     3225.50  \n...                               ...  \n2022-10-25 12:27:00+05:30     2589.95  \n2022-10-25 12:28:00+05:30     2589.30  \n2022-10-25 12:29:00+05:30     2590.20  \n2022-10-25 12:30:00+05:30     2591.00  \n2022-10-25 12:31:00+05:30     2591.00  \n\n[178385 rows x 91 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>APOLLOHOSP</th>\n      <th>BOSCHLTD</th>\n      <th>INDUSTOWER</th>\n      <th>BERGEPAINT</th>\n      <th>MARICO</th>\n      <th>ICICIPRULI</th>\n      <th>SBIN</th>\n      <th>SUNPHARMA</th>\n      <th>COALINDIA</th>\n      <th>WIPRO</th>\n      <th>...</th>\n      <th>BIOCON</th>\n      <th>DABUR</th>\n      <th>SAIL</th>\n      <th>BRITANNIA</th>\n      <th>ADANIPORTS</th>\n      <th>ONGC</th>\n      <th>EICHERMOT</th>\n      <th>TECHM</th>\n      <th>JINDALSTEL</th>\n      <th>HEROMOTOCO</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-08-09 09:15:00+05:30</th>\n      <td>968.40</td>\n      <td>19277.95</td>\n      <td>291.30</td>\n      <td>316.30</td>\n      <td>361.50</td>\n      <td>412.30</td>\n      <td>311.40</td>\n      <td>574.20</td>\n      <td>277.45</td>\n      <td>208.20</td>\n      <td>...</td>\n      <td>283.10</td>\n      <td>444.20</td>\n      <td>79.90</td>\n      <td>3160.00</td>\n      <td>375.85</td>\n      <td>172.30</td>\n      <td>2760.70</td>\n      <td>656.25</td>\n      <td>220.05</td>\n      <td>3295.40</td>\n    </tr>\n    <tr>\n      <th>2018-08-09 09:16:00+05:30</th>\n      <td>964.70</td>\n      <td>19280.00</td>\n      <td>290.80</td>\n      <td>315.85</td>\n      <td>360.40</td>\n      <td>412.05</td>\n      <td>310.75</td>\n      <td>574.25</td>\n      <td>277.40</td>\n      <td>207.93</td>\n      <td>...</td>\n      <td>283.15</td>\n      <td>442.75</td>\n      <td>79.65</td>\n      <td>3158.17</td>\n      <td>374.75</td>\n      <td>171.60</td>\n      <td>2764.25</td>\n      <td>654.20</td>\n      <td>219.10</td>\n      <td>3306.00</td>\n    </tr>\n    <tr>\n      <th>2018-08-21 09:15:00+05:30</th>\n      <td>1192.45</td>\n      <td>18832.00</td>\n      <td>285.75</td>\n      <td>334.15</td>\n      <td>363.10</td>\n      <td>389.80</td>\n      <td>307.75</td>\n      <td>626.55</td>\n      <td>286.00</td>\n      <td>214.59</td>\n      <td>...</td>\n      <td>302.85</td>\n      <td>454.45</td>\n      <td>77.80</td>\n      <td>3390.67</td>\n      <td>379.95</td>\n      <td>168.95</td>\n      <td>2854.25</td>\n      <td>702.30</td>\n      <td>206.30</td>\n      <td>3318.95</td>\n    </tr>\n    <tr>\n      <th>2018-08-21 09:16:00+05:30</th>\n      <td>1194.45</td>\n      <td>18869.00</td>\n      <td>286.10</td>\n      <td>334.35</td>\n      <td>362.85</td>\n      <td>388.95</td>\n      <td>307.65</td>\n      <td>625.00</td>\n      <td>286.30</td>\n      <td>214.59</td>\n      <td>...</td>\n      <td>303.10</td>\n      <td>454.10</td>\n      <td>77.65</td>\n      <td>3386.00</td>\n      <td>380.85</td>\n      <td>169.05</td>\n      <td>2858.05</td>\n      <td>700.20</td>\n      <td>207.20</td>\n      <td>3318.05</td>\n    </tr>\n    <tr>\n      <th>2018-08-29 09:15:00+05:30</th>\n      <td>1188.00</td>\n      <td>18900.00</td>\n      <td>283.30</td>\n      <td>342.00</td>\n      <td>371.50</td>\n      <td>381.15</td>\n      <td>307.95</td>\n      <td>626.00</td>\n      <td>292.10</td>\n      <td>223.20</td>\n      <td>...</td>\n      <td>308.75</td>\n      <td>485.60</td>\n      <td>77.85</td>\n      <td>3388.42</td>\n      <td>387.20</td>\n      <td>174.95</td>\n      <td>2879.35</td>\n      <td>740.25</td>\n      <td>208.30</td>\n      <td>3225.50</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2022-10-25 12:27:00+05:30</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2589.95</td>\n    </tr>\n    <tr>\n      <th>2022-10-25 12:28:00+05:30</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2589.30</td>\n    </tr>\n    <tr>\n      <th>2022-10-25 12:29:00+05:30</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2590.20</td>\n    </tr>\n    <tr>\n      <th>2022-10-25 12:30:00+05:30</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2591.00</td>\n    </tr>\n    <tr>\n      <th>2022-10-25 12:31:00+05:30</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2591.00</td>\n    </tr>\n  </tbody>\n</table>\n<p>178385 rows × 91 columns</p>\n</div>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"df.dropna(axis=0, inplace=True)\ndf.index = df.index.tz_localize(None)\nzero_rows = (df == 0.0).any(axis=1)\nnum_zero_rows = zero_rows.sum()\n\n\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T15:21:27.838105Z","iopub.execute_input":"2024-12-09T15:21:27.838585Z","iopub.status.idle":"2024-12-09T15:21:27.906530Z","shell.execute_reply.started":"2024-12-09T15:21:27.838545Z","shell.execute_reply":"2024-12-09T15:21:27.905544Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [APOLLOHOSP, BOSCHLTD, INDUSTOWER, BERGEPAINT, MARICO, SBIN, SUNPHARMA, COALINDIA, WIPRO, BAJAJ-AUTO, GODREJCP, UPL, SIEMENS, BPCL, TATAMOTORS, BANKBARODA, GAIL, HDFCBANK, BHARTIARTL, NMDC, INDUSINDBK, NTPC, JSWSTEEL, HCLTECH, AMBUJACEM, YESBANK, PNB, NESTLEIND, TORNTPHARM, NAUKRI, ULTRACEMCO, PIDILITIND, BAJAJHLDNG, HINDALCO, IOC, ADANIENT, MUTHOOTFIN, TATACONSUM, ICICIBANK, POWERGRID, SHREECEM, HAVELLS, DRREDDY, NIFTY BANK, MCDOWELL-N, ITC, LUPIN, IGL, COLPAL, VEDL, BAJFINANCE, AUROPHARMA, BAJAJFINSV, LT, RELIANCE, PIIND, HINDPETRO, CIPLA, INFY, CHOLAFIN, PGHH, TITAN, ACC, PEL, KOTAKBANK, TATASTEEL, HINDUNILVR, DIVISLAB, TCS, DLF, NIFTY 50, MM, HDFC, AXISBANK, GRASIM, JUBLFOOD, MARUTI, ASIANPAINT, BIOCON, DABUR, SAIL, BRITANNIA, ADANIPORTS, ONGC, EICHERMOT, TECHM, JINDALSTEL, HEROMOTOCO]\nIndex: []\n\n[0 rows x 88 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>APOLLOHOSP</th>\n      <th>BOSCHLTD</th>\n      <th>INDUSTOWER</th>\n      <th>BERGEPAINT</th>\n      <th>MARICO</th>\n      <th>SBIN</th>\n      <th>SUNPHARMA</th>\n      <th>COALINDIA</th>\n      <th>WIPRO</th>\n      <th>BAJAJ-AUTO</th>\n      <th>...</th>\n      <th>BIOCON</th>\n      <th>DABUR</th>\n      <th>SAIL</th>\n      <th>BRITANNIA</th>\n      <th>ADANIPORTS</th>\n      <th>ONGC</th>\n      <th>EICHERMOT</th>\n      <th>TECHM</th>\n      <th>JINDALSTEL</th>\n      <th>HEROMOTOCO</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 88 columns</p>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"df_total = df.join(nifty_100_closing, how='left')","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:09.938311Z","iopub.execute_input":"2024-12-09T14:39:09.938644Z","iopub.status.idle":"2024-12-09T14:39:10.065294Z","shell.execute_reply.started":"2024-12-09T14:39:09.938612Z","shell.execute_reply":"2024-12-09T14:39:10.064159Z"},"papermill":{"duration":0.32365,"end_time":"2024-12-02T13:19:36.161854","exception":false,"start_time":"2024-12-02T13:19:35.838204","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"df_total.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:10.068354Z","iopub.execute_input":"2024-12-09T14:39:10.068850Z","iopub.status.idle":"2024-12-09T14:39:10.226045Z","shell.execute_reply.started":"2024-12-09T14:39:10.068799Z","shell.execute_reply":"2024-12-09T14:39:10.224971Z"},"papermill":{"duration":0.29123,"end_time":"2024-12-02T13:19:36.459880","exception":false,"start_time":"2024-12-02T13:19:36.168650","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Calculate the percentage change\nX_pct_change = df_total.pct_change()\n\n# Define the threshold for large changes (e.g., 100% change)\nthreshold = 1.0  # 1.0 corresponds to 100% when working with pct_change()\n\n# Find rows with any column having a percentage change greater than the threshold\nlarge_change_rows = X_pct_change[(X_pct_change.abs() > threshold).any(axis=1)]\n\n# Get the number of rows where there are large percentage changes\nnum_large_changes = large_change_rows.shape[0]\n\nprint(f\"Number of rows with large changes (>{threshold * 100}%): {num_large_changes}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:10.227410Z","iopub.execute_input":"2024-12-09T14:39:10.227782Z","iopub.status.idle":"2024-12-09T14:39:11.488746Z","shell.execute_reply.started":"2024-12-09T14:39:10.227746Z","shell.execute_reply":"2024-12-09T14:39:11.487623Z"},"papermill":{"duration":1.91081,"end_time":"2024-12-02T13:19:38.377587","exception":false,"start_time":"2024-12-02T13:19:36.466777","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Number of rows with large changes (>100.0%): 0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(len(df_total))","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:11.491068Z","iopub.execute_input":"2024-12-09T14:39:11.491453Z","iopub.status.idle":"2024-12-09T14:39:11.497219Z","shell.execute_reply.started":"2024-12-09T14:39:11.491415Z","shell.execute_reply":"2024-12-09T14:39:11.495987Z"},"papermill":{"duration":0.015787,"end_time":"2024-12-02T13:19:38.400422","exception":false,"start_time":"2024-12-02T13:19:38.384635","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"340461\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"df_total.sort_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:11.498891Z","iopub.execute_input":"2024-12-09T14:39:11.499387Z","iopub.status.idle":"2024-12-09T14:39:11.515341Z","shell.execute_reply.started":"2024-12-09T14:39:11.499310Z","shell.execute_reply":"2024-12-09T14:39:11.514282Z"},"papermill":{"duration":0.221444,"end_time":"2024-12-02T13:19:38.628543","exception":false,"start_time":"2024-12-02T13:19:38.407099","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Models\n\nIn this section, we will utilize a combined approach to limit the number of features (stocks) and train our models.\n\n\n\n1. Feature Selection (using Lasso w/ high alpha, Mutual Info, ANOVA, RFE)\n\n2. Define threshold to drop features\n\n3. Train model on reduced feature space (Hyperparameter tuned)\n\n4. Get tracking error\n\n5. Save pair (number of stocks, tracking error) for plotting","metadata":{"papermill":{"duration":0.006446,"end_time":"2024-12-02T13:19:41.019240","exception":false,"start_time":"2024-12-02T13:19:41.012794","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Correlation - Lasso","metadata":{}},{"cell_type":"code","source":"def remove_correlated_features(X_train, threshold):\n    X_train = pd.DataFrame(X_train)\n    \n    # Compute the correlation matrix (absolute values)\n    corr_matrix = X_train.corr().abs()\n    \n    # Create a mask to select only the upper triangle of the correlation matrix\n    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    \n    # Find features with correlation greater than the threshold\n    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n    print(f\"Features to drop (correlation > {threshold}): {to_drop}\")\n    selected_features = [i for i in range(X_train.shape[1]) if i not in to_drop]\n    \n    return selected_features\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:39:11.516663Z","iopub.execute_input":"2024-12-09T14:39:11.517025Z","iopub.status.idle":"2024-12-09T14:39:11.525981Z","shell.execute_reply.started":"2024-12-09T14:39:11.516992Z","shell.execute_reply":"2024-12-09T14:39:11.525127Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n\ndef Corr_Lasso(threshold, X, y, debug=False):\n    tscv = TimeSeriesSplit(n_splits=18, max_train_size=85000, test_size=30000)\n    stocks = []\n    num_stocks = []\n    fold_scores = []\n    adj_fold_scores = []\n\n    for train_index, test_index in tscv.split(X):\n        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X_train_cv)\n\n        # Feature selection using correlation matrix\n        selected_indices = remove_correlated_features(X_scaled, threshold)\n\n        X_train_selected = X_scaled[:, selected_indices]\n        X_test_selected = X_test_cv.to_numpy()[:, selected_indices]\n        if debug == True:\n            print(f\"Selected {len(selected_indices)} stocks\")\n        num_stocks.append(len(selected_indices))\n        stocks.append(selected_indices)\n\n        best_alpha = None\n        best_score = float('inf')  # Assuming lower is better for your metric\n        for alph in [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025]:\n            lasso = Lasso(alpha=alph, max_iter=15000)\n            lasso.fit(X_train_selected, y_train_cv)\n            y_pred = lasso.predict(X_test_selected)\n            fold_score = tracking_error(y_test_cv.values, y_pred)\n            adj_fold_score = adj_tracking_error(y_test_cv.values, X_test_selected, lasso)\n            \n            if fold_score < best_score:\n                best_score = fold_score\n                best_alpha = alph\n\n        print(f\"Best Alpha for this fold: {best_alpha}, Best Fold Score: {best_score}\")\n        fold_scores.append(best_score)\n        adj_fold_scores.append(adj_fold_score)\n\n    # Calculate the average score across all folds\n    average_score = np.mean(fold_scores)\n    average_adj_score = np.mean(adj_fold_scores)\n\n    # Get average stocks chosen over all folds\n    average_selected_features = np.mean(num_stocks)\n\n    # Print the tracking error for this alpha\n    print(f\"Threshold: {threshold}, Tracking Error: {average_score}, Adj Tracking Error: {average_adj_score}\")\n    return stocks, average_selected_features, average_score, average_adj_score","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:11.528126Z","iopub.execute_input":"2024-12-09T14:39:11.528644Z","iopub.status.idle":"2024-12-09T14:39:11.546115Z","shell.execute_reply.started":"2024-12-09T14:39:11.528594Z","shell.execute_reply":"2024-12-09T14:39:11.545065Z"},"papermill":{"duration":0.04052,"end_time":"2024-12-02T13:32:09.093172","exception":false,"start_time":"2024-12-02T13:32:09.052652","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def Corr_Lasso_plot(X,y):\n    thresholds = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n    tracking_errors = []\n    adj_tracking_errors = []\n    num_stocks_list = []\n    \n    for threshold in thresholds:\n        print(f\"----------Starting threshold: {threshold}-------------\")\n        _, num_stocks, tracking_err, adj_tracking_err = Corr_Lasso(threshold, X, y, debug=False)\n        num_stocks_list.append(num_stocks)\n        tracking_errors.append(tracking_err)\n        adj_tracking_errors.append(adj_tracking_err)\n    \n    # Plotting the results\n    fig, ax1 = plt.subplots()\n    \n    # Plotting number of stocks on the primary y-axis\n    color = 'tab:blue'    \n    ax1.set_xlabel('Alpha')\n    ax1.set_ylabel('Number of Stocks', color=color)    \n    ax1.plot(thresholds, num_stocks_list, color=color, marker='o', label='Number of Stocks')\n    ax1.tick_params(axis='y', labelcolor=color)\n    \n    # Creating a second y-axis for tracking error\n    ax2 = ax1.twinx()\n    color = 'tab:red'\n    ax2.set_ylabel('Tracking Error', color=color)\n    #ax2.plot(alphas, tracking_errors, color='tab:green', marker='o', linestyle='--', label='Tracking Error')\n    ax2.plot(thresholds, adj_tracking_errors, color=color, marker='x', linestyle='--', label='Adjusted Tracking Error')\n    ax2.tick_params(axis='y', labelcolor=color)\n    \n    # Adding a title and legend\n    plt.title('Number of Stocks and Tracking Error vs. Alpha')    \n    fig.tight_layout()  # Adjust layout to prevent overlap\n    plt.savefig('Corr-Lasso.png')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:39:11.548997Z","iopub.execute_input":"2024-12-09T14:39:11.549402Z","iopub.status.idle":"2024-12-09T14:39:11.564090Z","shell.execute_reply.started":"2024-12-09T14:39:11.549353Z","shell.execute_reply":"2024-12-09T14:39:11.562872Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Lasso - Lasso\n\n\n\nWe will utilize a Lasso model with a high alpha value to perform feature selection, this will drop the coefficients for less important features to 0. After that we retrain a Lasso model with hyperparameter tuning.","metadata":{"papermill":{"duration":0.006463,"end_time":"2024-12-02T13:19:41.045638","exception":false,"start_time":"2024-12-02T13:19:41.039175","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n\n\ndef Lasso_Lasso(alpha, X, y):    \n    tscv = TimeSeriesSplit(n_splits=18, max_train_size=85000, test_size=30000)\n    stocks = []\n    num_stocks = []\n    fold_scores = []\n    adj_fold_scores = []\n\n    for train_index, test_index in tscv.split(X):\n        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n        train_period = X.iloc[train_index].index\n        test_period = X.iloc[test_index].index\n\n        # Convert to date format\n        train_start_date, train_end_date = train_period.min(), train_period.max()\n        test_start_date, test_end_date = test_period.min(), test_period.max()\n\n        print(f\"Train period: {train_start_date} to {train_end_date}\")\n        print(f\"Test period: {test_start_date} to {test_end_date}\")\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X_train_cv)\n\n        # Feature selection with alpha from list\n        lasso_fs = Lasso(alpha=alpha, max_iter=200)\n        lasso_fs.fit(X_scaled, y_train_cv)\n\n        #for feature, coef in zip(X_train_cv.columns, lasso_fs.coef_)\n        #    print(f\"{feature}: {coef}\")\n\n        selector = SelectFromModel(lasso_fs, threshold=10e-10, max_features=None)  # Adjust threshold as needed\n        X_selected_train = selector.transform(X_scaled)\n        X_selected_test = selector.transform(X_test_cv)\n\n        selected_features = np.where(selector.get_support())[0] \n        print(f\"Selected {len(selected_features)} stocks\")\n        num_stocks.append(len(selected_features))\n        stocks.append(selected_features)\n\n        best_alpha = None\n        best_score = float('inf')  # Assuming lower is better for your metric\n        for alph in [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025]:\n            lasso = Lasso(alpha=alph, max_iter=15000)\n            lasso.fit(X_selected_train, y_train_cv)\n            y_pred = lasso.predict(X_selected_test)\n            fold_score = tracking_error(y_test_cv.values, y_pred)\n            adj_fold_score = adj_tracking_error(y_test_cv.values, X_selected_test, lasso)\n            # print(f\"Alpha: {alpha}, Fold Score: {fold_score}\"\n            \n            if fold_score < best_score:\n                best_score = fold_score\n                best_alpha = alph\n\n        print(f\"Best Alpha for this fold: {best_alpha}, Best Fold Score: {best_score}\")\n        fold_scores.append(best_score)\n        adj_fold_scores.append(adj_fold_score)\n\n    # Calculate the average score across all folds\n    average_score = np.mean(fold_scores)\n    average_adj_score = np.mean(adj_fold_scores)\n\n    # Get average stocks chosen over all folds\n    average_selected_features = np.mean(num_stocks)\n\n    # Print the tracking error for this alpha\n    print(f\"Alpha: {alpha}, Average Tracking Error: {average_score}\")\n    return stocks, average_selected_features, average_score, average_adj_score","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-12-09T14:39:11.565809Z","iopub.execute_input":"2024-12-09T14:39:11.566145Z","iopub.status.idle":"2024-12-09T14:39:11.583759Z","shell.execute_reply.started":"2024-12-09T14:39:11.566114Z","shell.execute_reply":"2024-12-09T14:39:11.582533Z"},"papermill":{"duration":0.022764,"end_time":"2024-12-02T13:19:41.075078","exception":false,"start_time":"2024-12-02T13:19:41.052314","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def Lasso_Lasso_plot(X,y):\n    alphas = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009]\n    tracking_errors = []\n    adj_tracking_errors = []\n    num_stocks_list = []\n    \n    for alpha in alphas:\n        print(f\"----------Starting alpha {alpha}-------------\")\n        _, num_stocks, tracking_err, adj_tracking_err = Lasso_Lasso(alpha, X, y)\n        num_stocks_list.append(num_stocks)\n        tracking_errors.append(tracking_err)\n        adj_tracking_errors.append(adj_tracking_err)\n    \n    # Plotting the results\n    fig, ax1 = plt.subplots()\n    \n    # Plotting number of stocks on the primary y-axis\n    color = 'tab:blue'    \n    ax1.set_xlabel('Alpha')\n    ax1.set_ylabel('Number of Stocks', color=color)    \n    ax1.plot(alphas, num_stocks_list, color=color, marker='o', label='Number of Stocks')\n    ax1.tick_params(axis='y', labelcolor=color)\n    \n    # Creating a second y-axis for tracking error\n    ax2 = ax1.twinx()\n    color = 'tab:red'\n    ax2.set_ylabel('Tracking Error', color=color)\n    #ax2.plot(alphas, tracking_errors, color='tab:green', marker='o', linestyle='--', label='Tracking Error')\n    ax2.plot(alphas, adj_tracking_errors, color=color, marker='x', linestyle='--', label='Adjusted Tracking Error')\n    ax2.tick_params(axis='y', labelcolor=color)\n    \n    # Adding a title and legend\n    plt.title('Number of Stocks and Tracking Error vs. Alpha')    \n    fig.tight_layout()  # Adjust layout to prevent overlap\n    plt.savefig('Lasso-Lasso.png')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:11.585119Z","iopub.execute_input":"2024-12-09T14:39:11.585505Z","iopub.status.idle":"2024-12-09T14:39:11.601024Z","shell.execute_reply.started":"2024-12-09T14:39:11.585469Z","shell.execute_reply":"2024-12-09T14:39:11.599862Z"},"papermill":{"duration":747.843259,"end_time":"2024-12-02T13:32:08.925401","exception":false,"start_time":"2024-12-02T13:19:41.082142","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Mutual Info - Lasso\n\nUsing mutual information between variables, which measures the dependency between them, 0 value being independence. By getting the mutual information between the features (stocks) and the target (NIFTY 100) we can \"discard\" the variables with lower dependencies. After feature selection, we traing a Lasso model with CV.","metadata":{"papermill":{"duration":0.025132,"end_time":"2024-12-02T13:32:09.027191","exception":false,"start_time":"2024-12-02T13:32:09.002059","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n\ndef MI_Experiment(percentile, X, y, model_class, params, debug=False):\n    tscv = TimeSeriesSplit(n_splits=15, max_train_size=85000, test_size=30000)\n    stocks = []\n    num_stocks = []\n    fold_scores = []\n    adj_fold_scores = []\n\n    for train_index, test_index in tscv.split(X):\n        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X_train_cv)\n\n        # Feature selection with alpha from list\n        mi_scores = mutual_info_regression(X_train_cv, y_train_cv)\n        mi_scores_series = pd.Series(mi_scores, index=X_train_cv.columns)\n\n        threshold_value = mi_scores_series.quantile(percentile)        \n\n        selected_features = mi_scores_series[mi_scores_series > threshold_value].index\n        selected_indices = [X_train_cv.columns.get_loc(feature) for feature in selected_features]\n\n        # Filter the NumPy arrays based on the selected feature indices\n        X_train_selected = X_scaled[:, selected_indices]\n        X_test_selected = X_test_cv.to_numpy()[:, selected_indices]\n\n        if debug == True:\n            print(f\"Selected {len(selected_features)} stocks\")\n        num_stocks.append(len(selected_features))\n        stocks.append(selected_features)\n\n        best_params = None\n        best_score = float('inf')  # Assuming lower is better for your metric\n        best_adj_score = float('inf')\n        for param in ParameterGrid(params):\n            if debug == True:\n                print(f\"Trying combination {param}\")\n            model = model_class(**param)\n            model.fit(X_train_selected, y_train_cv)\n\n            y_pred = model.predict(X_test_selected)\n            fold_score = tracking_error(y_test_cv.values, y_pred)\n            adj_fold_score = adj_tracking_error(y_test_cv.values, X_test_selected, model)\n\n            if adj_fold_score < best_adj_score:\n                best_score = fold_score\n                best_adj_score = adj_fold_score\n                best_params = param\n        if debug == True:\n            print(f\"Best Params for this fold: {best_params}, Best Fold Score: {best_score}\")\n        fold_scores.append(best_score)\n        adj_fold_scores.append(best_adj_score)\n\n    # Calculate the average score across all folds\n    average_score = np.mean(fold_scores)\n    average_adj_score = np.mean(adj_fold_scores)\n\n    # Get average stocks chosen over all folds\n    average_selected_features = np.mean(num_stocks)\n\n    # Print the tracking error for this alpha\n    print(f\"Percentile: {percentile}, Tracking Error: {average_score}, Adj Tracking Error: {average_adj_score}\")\n    return stocks, average_selected_features, average_score, average_adj_score","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:11.602228Z","iopub.execute_input":"2024-12-09T14:39:11.602633Z","iopub.status.idle":"2024-12-09T14:39:11.620857Z","shell.execute_reply.started":"2024-12-09T14:39:11.602601Z","shell.execute_reply":"2024-12-09T14:39:11.619739Z"},"papermill":{"duration":0.04052,"end_time":"2024-12-02T13:32:09.093172","exception":false,"start_time":"2024-12-02T13:32:09.052652","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### MI - Lasso","metadata":{}},{"cell_type":"code","source":"def MI_Lasso_plot(X,y):\n    tracking_errors = []\n    adj_tracking_errors = []\n    num_stocks_list = []\n    \n    percentiles = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n    param_grid = {\n        'alpha': [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025],\n        'max_iter': [15000]\n    }\n    \n    for percentile in percentiles:\n        print(f\"Starting percentile: {percentile}\")\n        _, num_stocks, tracking_err, adj_tracking_err = MI_Experiment(percentile, X, y, Lasso, param_grid)\n        num_stocks_list.append(num_stocks)\n        tracking_errors.append(tracking_err)\n        adj_tracking_errors.append(adj_tracking_err)\n    \n    # Plotting the results\n    fig, ax1 = plt.subplots()\n    \n    # Plotting number of stocks on the primary y-axis\n    plt.plot(num_stocks_list, adj_tracking_errors, marker='x')\n    plt.xlabel('Number of Stocks')\n    plt.ylabel('Tracking Error')\n    plt.title('Lasso - Tracking Errors vs Number of Stocks')\n    plt.grid(True)\n    plt.savefig(\"MI-Lasso.png\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:11.622526Z","iopub.execute_input":"2024-12-09T14:39:11.622989Z","iopub.status.idle":"2024-12-09T14:39:11.638525Z","shell.execute_reply.started":"2024-12-09T14:39:11.622941Z","shell.execute_reply":"2024-12-09T14:39:11.637097Z"},"papermill":{"duration":7820.791352,"end_time":"2024-12-02T15:42:29.909960","exception":false,"start_time":"2024-12-02T13:32:09.118608","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### MI - ElasticNet","metadata":{"papermill":{"duration":0.07641,"end_time":"2024-12-02T15:42:30.064196","exception":false,"start_time":"2024-12-02T15:42:29.987786","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def MI_ElasticNet_plot(X,y):\n    tracking_errors = []\n    adj_tracking_errors = []\n    num_stocks_list = []\n    \n    percentiles = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n    param_grid = {\n        'alpha': [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025],\n        'l1_ratio': [0.1, 0.5, 0.7, 0.9]\n    }\n    \n    for percentile in percentiles:\n        print(f\"Starting percentile: {percentile}\")\n        _, num_stocks, tracking_err, adj_tracking_err = MI_Experiment(percentile, X, y, ElasticNet, param_grid)\n        num_stocks_list.append(num_stocks)\n        tracking_errors.append(tracking_err)\n        adj_tracking_errors.append(adj_tracking_err)\n    \n    # Plotting the results\n    fig, ax1 = plt.subplots()\n    \n    # Plotting number of stocks on the primary y-axis\n    plt.plot(num_stocks_list, adj_tracking_errors, marker='x')\n    plt.xlabel('Number of Stocks')\n    plt.ylabel('Tracking Error')\n    plt.title('ElasticNet - Tracking Errors vs Number of Stocks ')\n    plt.grid(True)\n    plt.savefig(\"MI-ElasticNet.png\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:11.640225Z","iopub.execute_input":"2024-12-09T14:39:11.640756Z","iopub.status.idle":"2024-12-09T14:39:11.653433Z","shell.execute_reply.started":"2024-12-09T14:39:11.640701Z","shell.execute_reply":"2024-12-09T14:39:11.652110Z"},"papermill":{"duration":9653.410145,"end_time":"2024-12-02T18:23:23.552432","exception":false,"start_time":"2024-12-02T15:42:30.142287","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"TODO: Theil-Sein Linear Regression","metadata":{"execution":{"iopub.execute_input":"2024-11-12T21:50:37.572889Z","iopub.status.busy":"2024-11-12T21:50:37.570440Z"},"papermill":{"duration":0.269963,"end_time":"2024-12-02T18:23:24.098548","exception":false,"start_time":"2024-12-02T18:23:23.828585","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### MI-RandomForestRegressor","metadata":{}},{"cell_type":"code","source":"def MI_RF_plot(X,y):\n    tracking_errors = []\n    adj_tracking_errors = []\n    num_stocks_list = []\n    \n    percentiles = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n    param_grid = {\n        'n_estimators': [10, 20, 30, 40, 50],       # Number of trees in the forest\n        'max_depth': [5, 10, 15],      # Maximum depth of the tree\n        'max_samples': [0.5],\n        'min_samples_split': [0.05],\n        'bootstrap': [True],\n        'n_jobs': [-1],\n        'max_features': ['sqrt']\n    }\n    \n    for percentile in percentiles:\n        print(f\"Starting percentile: {percentile}\")\n        _, num_stocks, tracking_err, adj_tracking_err = MI_Experiment(percentile, X, y, RandomForestRegressor, param_grid)\n        num_stocks_list.append(num_stocks)\n        tracking_errors.append(tracking_err)\n        adj_tracking_errors.append(adj_tracking_err)\n    \n    # Plotting the results\n    fig, ax1 = plt.subplots()\n    \n    # Plotting number of stocks on the primary y-axis\n    plt.plot(num_stocks_list, adj_tracking_errors, marker='x')\n    plt.xlabel('Number of Stocks')\n    plt.ylabel('Tracking Error')\n    plt.title('RandomForestRegressor - Tracking Errors vs Number of Stocks ')\n    plt.grid(True)\n    plt.savefig(\"MI-RandomForestRegressor.png\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-12-09T14:39:12.908573Z","iopub.execute_input":"2024-12-09T14:39:12.909009Z","iopub.status.idle":"2024-12-09T14:39:12.918184Z","shell.execute_reply.started":"2024-12-09T14:39:12.908973Z","shell.execute_reply":"2024-12-09T14:39:12.916893Z"},"papermill":{"duration":2495.391087,"end_time":"2024-12-02T19:04:59.756301","exception":false,"start_time":"2024-12-02T18:23:24.365214","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### MI - XGBoost","metadata":{}},{"cell_type":"code","source":"def MI_XGBoost_plot(X,y):\n    tracking_errors = []\n    adj_tracking_errors = []\n    num_stocks_list = []\n    \n    percentiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n    param_grid = {\n        'n_estimators': [50, 100, 200, 250],  # Number of boosting rounds\n        'learning_rate': [0.01, 0.1, 0.2, 0.25],  # Step size shrinkage\n        'max_depth': [3, 5, 7, 10],  # Maximum depth of a tree\n        'n_jobs': [-1],\n        'tree_method': ['gpu_hist'],\n    }\n    \n    for percentile in percentiles:\n        print(f\"Starting percentile: {percentile}\")\n        _, num_stocks, tracking_err, adj_tracking_err = MI_Experiment(percentile, X, y, xgb.XGBRegressor, param_grid)\n        num_stocks_list.append(num_stocks)\n        tracking_errors.append(tracking_err)\n        adj_tracking_errors.append(adj_tracking_err)\n    \n    # Plotting the results\n    fig, ax1 = plt.subplots()\n    \n    # Plotting number of stocks on the primary y-axis\n    plt.plot(num_stocks_list, adj_tracking_errors, marker='x')\n    plt.xlabel('Number of Stocks')\n    plt.ylabel('Tracking Error')\n    plt.title('XGBRegressor - Tracking Errors vs Number of Stocks ')\n    plt.grid(True)\n    plt.savefig(\"MI-XGBRegressor.png\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:39:14.221549Z","iopub.execute_input":"2024-12-09T14:39:14.221959Z","iopub.status.idle":"2024-12-09T14:39:14.230533Z","shell.execute_reply.started":"2024-12-09T14:39:14.221924Z","shell.execute_reply":"2024-12-09T14:39:14.229411Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"df_total_z = df_total.replace(0, np.nan)  # Convert zeroes back to NaNs\ndf_total_returns = df_total_z.pct_change().dropna()\ndf_total_returns = 100*df_total_returns\nX = df_total_returns.drop(columns=['NIFTY 50', 'NIFTY BANK', 'NIFTY_100'])\ny = df_total_returns['NIFTY_100']","metadata":{"papermill":{"duration":0.284962,"end_time":"2024-12-02T19:05:00.328044","exception":false,"start_time":"2024-12-02T19:05:00.043082","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:39:16.208257Z","iopub.execute_input":"2024-12-09T14:39:16.209369Z","iopub.status.idle":"2024-12-09T14:39:17.822707Z","shell.execute_reply.started":"2024-12-09T14:39:16.209285Z","shell.execute_reply":"2024-12-09T14:39:17.821681Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"MI_Lasso_plot(X,y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T14:39:30.533960Z","iopub.execute_input":"2024-12-09T14:39:30.534405Z","iopub.status.idle":"2024-12-09T14:39:31.143786Z","shell.execute_reply.started":"2024-12-09T14:39:30.534342Z","shell.execute_reply":"2024-12-09T14:39:31.142274Z"}},"outputs":[{"name":"stdout","text":"Starting percentile: 0.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mMI_Lasso_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[20], line 14\u001b[0m, in \u001b[0;36mMI_Lasso_plot\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m percentile \u001b[38;5;129;01min\u001b[39;00m percentiles:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting percentile: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     _, num_stocks, tracking_err, adj_tracking_err \u001b[38;5;241m=\u001b[39m \u001b[43mMI_Experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpercentile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLasso\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     num_stocks_list\u001b[38;5;241m.\u001b[39mappend(num_stocks)\n\u001b[1;32m     16\u001b[0m     tracking_errors\u001b[38;5;241m.\u001b[39mappend(tracking_err)\n","Cell \u001b[0;32mIn[19], line 11\u001b[0m, in \u001b[0;36mMI_Experiment\u001b[0;34m(percentile, X, y, model_class, params, debug)\u001b[0m\n\u001b[1;32m      8\u001b[0m fold_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m adj_fold_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m tscv\u001b[38;5;241m.\u001b[39msplit(X):\n\u001b[1;32m     12\u001b[0m     X_train_cv, X_test_cv \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[train_index], X\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m     13\u001b[0m     y_train_cv, y_test_cv \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_index], y\u001b[38;5;241m.\u001b[39miloc[test_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:1134\u001b[0m, in \u001b[0;36mTimeSeriesSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of folds=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_folds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m     )\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m-\u001b[39m gap \u001b[38;5;241m-\u001b[39m (test_size \u001b[38;5;241m*\u001b[39m n_splits) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToo many splits=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_splits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for number of samples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with test_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and gap=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1137\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(n_samples)\n\u001b[1;32m   1140\u001b[0m test_starts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(n_samples \u001b[38;5;241m-\u001b[39m n_splits \u001b[38;5;241m*\u001b[39m test_size, n_samples, test_size)\n","\u001b[0;31mValueError\u001b[0m: Too many splits=18 for number of samples=340460 with test_size=30000 and gap=0."],"ename":"ValueError","evalue":"Too many splits=18 for number of samples=340460 with test_size=30000 and gap=0.","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}