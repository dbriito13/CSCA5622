{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2e9d72",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.011143,
     "end_time": "2024-12-30T11:37:50.962128",
     "exception": false,
     "start_time": "2024-12-30T11:37:50.950985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Nifty 50 Partial Replication \n",
    "Index funds have become a household occurence in the financial sphere, especially for retail investors, due to their simplicity, diversification and low cost compared to actively managed funds. Following an index can be done in one of two ways:\n",
    "- **Full replication:** The most common approach, where a fund holds all constituent stocks present in the index, with the same weights as the index.\n",
    "- **Partial Replication:** With this approach, only a subset of the index's constituents is selected, which should aim to approximate the overall index performance.\n",
    "\n",
    "Partial replication can offer a number of benefits over full replication, but mainly:\n",
    "- Lower transaction costs: Reducing the number of stocks traded minimizes fees such as brokerage fees.\n",
    "- Reduced Trading fees: By potentially avoiding low-liquidity stocks, we avoid have wider bid-ask spreads, as well as having less market impact.\n",
    "- Higher flexibility: Funds can add other constraints on top of avoiding low-liquidity stocks, such as those on sectors or specific stocks, giving them better control over their risk management.\n",
    "Our project's goal is to apply supervised learning techniques to develop a partial replication strategy, minimizing the tracking error of our portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4554af",
   "metadata": {
    "papermill": {
     "duration": 0.011335,
     "end_time": "2024-12-30T11:37:50.983494",
     "exception": false,
     "start_time": "2024-12-30T11:37:50.972159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Methodology\n",
    "The project will focus on the NIFTY 100 index, which includes the top 100 companies in the National Stock Exchange of India (NSE) based on market capitalization. \n",
    "\n",
    "We will begin by gathering the minute-level data for both the index as well as the constituents data. Combining the data into a single dataframe, and perform some basic cleaning tasks, removing certain columns with a large number of empty values, as well as rows without target value, etc. Our target variable will be the **NIFTY 100 index price**.\n",
    "\n",
    "Since our goal is to reduce the number of constituents to use only a subset of the total 100 stocks, our models will follow a 2 step approach:\n",
    "1. Feature Selection: We will first apply different feature selection techniques, (Lasso regression, Correlation matrix, Mutual information) to identify the most relevant stocks for our model.\n",
    "2. Model Training: Once the most relevant features (stocks) are selected, we will train different supervised learning models on the reduced dataset, these models will predict the weights for each of the relevant stocks.\n",
    "To ensure the robustness of the test results, while accounting for the nature of the data (ordered time series) we will use a **sliding window approach**, where:\n",
    "* A fixed-size window (roughly 1-year) is used to train the model.\n",
    "* The model will be tested on roughly the next 3 months.\n",
    "* The window will slide forward, repeating until we use all available data.\n",
    "\n",
    "For a certain model, the overall out-of-sample error will be the average error over all the windows. The error we will aim to minimize is the **tracking error**, for simplicity models will minimize MSE. Finally, we will compare models by plotting the error vs the number of stocks, to visualize how it evolves as we reduce the size of the subset of constituents chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39942545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:37:51.005472Z",
     "iopub.status.busy": "2024-12-30T11:37:51.004949Z",
     "iopub.status.idle": "2024-12-30T11:38:08.920910Z",
     "shell.execute_reply": "2024-12-30T11:38:08.919619Z"
    },
    "papermill": {
     "duration": 17.929879,
     "end_time": "2024-12-30T11:38:08.923554",
     "exception": false,
     "start_time": "2024-12-30T11:37:50.993675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit, ParameterGrid\n",
    "from sklearn.linear_model import LassoCV\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"always\", category=ConvergenceWarning)\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectFromModel,mutual_info_regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d0a52",
   "metadata": {
    "papermill": {
     "duration": 0.009707,
     "end_time": "2024-12-30T11:38:08.943873",
     "exception": false,
     "start_time": "2024-12-30T11:38:08.934166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tracking Error\n",
    "Tracking error is the metric most commonly used to judge how well a replicating portfolio tracks the benchmark index, for full replication we would expect this to be close to 0 (slightly higher than 0 due to transaction costs or rebalancing delays).\n",
    "\n",
    "The tracking error is defined as the standard deviation of the differences between the index and replicating portfolio over a specified time period. Central to this metric is the time period that the difference in returns is calculated over, usually this is done daily, weekly or monthly.\n",
    "\n",
    "$$\n",
    "\\text{Tracking Error} = \\sqrt{\\frac{\\sum_{t=1}^T \\left( R_{portfolio}(t) - R_{index}(t) \\right)^2}{T}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "*  $R_{portfolio}(t)$ is the portfolio return at time $t$,  \n",
    "*  $R_{index}(t)$ is the index's return at time $t$,  \n",
    "*  $T$  is the total number of time periods.\n",
    "\n",
    "We base ourselves on this metric, and add a small adjustment, where from the first return of the portfolio, the trading cost is subtracted, this is a simulated trading cost, which we will simply set to 0.2%.\n",
    "\n",
    "In the next sections we will see that our data comes in minute intervals, so, to obtain the tracking error for daily returns, we will first aggregate the returns per day, and then utilize the formula above. To aggregate returns we do:\n",
    "\n",
    "$$\n",
    "R_{\\text{daily}} = \\left( \\prod_{j=1}^{m} (1 + R_{\\text{minute},j}) \\right) - 1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $R_{\\text{daily}}$ is the aggregated daily return\n",
    "* $R_{\\text{minute},j}$ is the minute level return\n",
    "* m: is the number of minute level returns we have per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a62b220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:38:08.965767Z",
     "iopub.status.busy": "2024-12-30T11:38:08.965094Z",
     "iopub.status.idle": "2024-12-30T11:38:08.972126Z",
     "shell.execute_reply": "2024-12-30T11:38:08.970945Z"
    },
    "papermill": {
     "duration": 0.0206,
     "end_time": "2024-12-30T11:38:08.974343",
     "exception": false,
     "start_time": "2024-12-30T11:38:08.953743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tracking_error(y_true, y_pred):\n",
    "    # Step 1: Join y_pred into y_true DataFrame\n",
    "    y_true = pd.DataFrame(y_true)\n",
    "    y_true.columns = ['target'] \n",
    "    \n",
    "    # Join target and y_pred based on datetime\n",
    "    data = y_true.copy()\n",
    "    data['predicted'] = y_pred\n",
    "\n",
    "    def accumulated_return(returns):\n",
    "        return (1+returns).prod() - 1\n",
    "    \n",
    "    # Group by date, aggregate returns by cumulative product\n",
    "    daily_data = data.resample('D').apply(accumulated_return)\n",
    "    # Calculate the differences between the cumulative returns of target and predicted\n",
    "    diff = daily_data['target'] - daily_data['predicted']\n",
    "    # print(len(diff), \"ERROR: \", np.std(diff))\n",
    "    # Calculate the tracking error as the standard deviation of the differences\n",
    "    return np.std(diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dfd12b",
   "metadata": {
    "papermill": {
     "duration": 0.009557,
     "end_time": "2024-12-30T11:38:08.994352",
     "exception": false,
     "start_time": "2024-12-30T11:38:08.984795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data\n",
    "The data for this project will be taken from 2 Kaggle datasets:\n",
    "\n",
    "* [Nifty 100 Constituents](https://www.kaggle.com/datasets/debashis74017/stock-market-data-nifty-50-stocks-1-min-data): This data set will be used to gather price data for all constituents for the NIFTY 100 index. The dataset contains a file per stock, which contains OHLC (Open, High, Low, Close) as well as Volume data.\n",
    "* [Nifty 100 Index Data](https://www.kaggle.com/datasets/debashis74017/nifty-50-minute-data): This data set will be used to get the price data for the NIFTY 100 index price.\n",
    "\n",
    "In order to speed up the execution, an intermediate dataset is created where we keep only closing prices for all stocks, in one single file. This dataset is [Nifty 100 Closing Only Data](https://www.kaggle.com/datasets/danielbrito99/nifty-50-closing-data). The script used to extract this intermediate data, is described in the cell below.\n",
    "\n",
    "As was mentioned before our target variable will be the **NIFTY 100 Index Price**, by training the model with a reduced number of constituents to predict the index price based on that reduced subset, we will obtain weights for each of them. During the testing phase, we will use these weights to evaluate the model's performance and verify how well the selected subset mimics the index price. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce15c96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T13:33:03.408909Z",
     "iopub.status.busy": "2024-12-09T13:33:03.408616Z",
     "iopub.status.idle": "2024-12-09T13:33:03.420882Z",
     "shell.execute_reply": "2024-12-09T13:33:03.419748Z",
     "shell.execute_reply.started": "2024-12-09T13:33:03.408879Z"
    },
    "papermill": {
     "duration": 0.009692,
     "end_time": "2024-12-30T11:38:09.013766",
     "exception": false,
     "start_time": "2024-12-30T11:38:09.004074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "directory_path = '/kaggle/input/stock-market-data-nifty-50-stocks-1-min-data'\n",
    "pattern = os.path.join(directory_path, \"*.csv\")  # Change \"*.csv\" to match other patterns if needed\n",
    "Get the list of files that match the pattern\n",
    "file_list = glob.glob(pattern)\n",
    "\n",
    "def fetch_data(cols, file_list):\n",
    "    df_list = []\n",
    "    for file in tqdm(file_list):\n",
    "        stock_name = os.path.basename(file).split(\"_\")[0]\n",
    "        file_df = pd.read_csv(file, index_col=0)\n",
    "        file_df = file_df[[cols]]\n",
    "        file_df.columns = pd.MultiIndex.from_product([[stock_name], file_df.columns])\n",
    "        df_list.append(file_df)\n",
    "    return pd.concat(df_list, axis=1)\n",
    "\n",
    "df = fetch_data('close')\n",
    "df.to_csv('/kaggle/input/nifty_50_closing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5cd855d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:38:09.035967Z",
     "iopub.status.busy": "2024-12-30T11:38:09.035513Z",
     "iopub.status.idle": "2024-12-30T11:38:31.116279Z",
     "shell.execute_reply": "2024-12-30T11:38:31.115119Z"
    },
    "papermill": {
     "duration": 22.095362,
     "end_time": "2024-12-30T11:38:31.118862",
     "exception": false,
     "start_time": "2024-12-30T11:38:09.023500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values: 639555\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/nifty-50-closing-data/nifty_50_closing.csv', index_col=0, header=[0, 1])\n",
    "df = df.xs('close', level=1, axis=1)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "nan_rows = df.isna().any(axis=1)\n",
    "\n",
    "# Get the number of rows with NaN value\n",
    "num_nan_rows = nan_rows.sum()\n",
    "\n",
    "\n",
    "print(f\"Number of rows with NaN values: {num_nan_rows}\")\n",
    "\n",
    "nifty_100_closing = pd.read_csv('/kaggle/input/nifty-50-minute-data/NIFTY 100_minute.csv', index_col=0)[['close']]\n",
    "nifty_100_closing.index = pd.to_datetime(nifty_100_closing.index)\n",
    "nifty_100_closing.rename(columns={'close': 'NIFTY_100'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45513180",
   "metadata": {
    "papermill": {
     "duration": 0.010222,
     "end_time": "2024-12-30T11:38:31.139322",
     "exception": false,
     "start_time": "2024-12-30T11:38:31.129100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Cleaning\n",
    "Our data cleaning will be simple, we will focus on remediating the high number of NaN's present in the dataset.\n",
    "Most importantly, we will focus on sequences of NaN values, which are problematic for our problem, as conventional techniques such as forward fill or interpolation become less effective the longer the period of missing data. Conversely, sparse or isolated NaNs are less of an issue, as they can be effectively substituted using these techniques with minimal loss of information. \n",
    "\n",
    "\n",
    "With this in mind, we perform the following steps:\n",
    "\n",
    "* Remove NIFTY 50 and NIFTY BANK stocks, as they represent indices, not stocks.\n",
    "* To get rid of periods where a large number of stocks all are missing data, we remove the rows with higher than 25 NaNs.\n",
    "* Inspecting the longest NaN period for each column, we will get rid of those columns where the value is large:\n",
    "    * HDFCAMC\n",
    "    * ICICIPRULI\n",
    "    * HDFCLIFE\n",
    "    * GLAND\n",
    "    * ADANIGREEN\n",
    "    * DMART\n",
    "    * ICICIGI\n",
    "    * BANDHANBNK\n",
    "    * INDIGO\n",
    "    * SBILIFE\n",
    "    * LICI\n",
    "    * SBICARD\n",
    "    * LTI\n",
    "*  At this point the number of NaN rows and consecutive NaNs is much lower, so we can use forward fill to populate them with the last know values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9853648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:38:31.161039Z",
     "iopub.status.busy": "2024-12-30T11:38:31.160599Z",
     "iopub.status.idle": "2024-12-30T11:38:31.166871Z",
     "shell.execute_reply": "2024-12-30T11:38:31.165698Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.01975,
     "end_time": "2024-12-30T11:38:31.169064",
     "exception": false,
     "start_time": "2024-12-30T11:38:31.149314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def longest_nan_period(series):\n",
    "    max_nan_period = 0\n",
    "    current_nan_period = 0\n",
    "    for value in series:\n",
    "        if pd.isna(value):\n",
    "            current_nan_period += 1\n",
    "            max_nan_period = max(max_nan_period, current_nan_period)\n",
    "        else:\n",
    "            current_nan_period = 0\n",
    "    return max_nan_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee364d34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:38:31.191080Z",
     "iopub.status.busy": "2024-12-30T11:38:31.190681Z",
     "iopub.status.idle": "2024-12-30T11:38:31.318060Z",
     "shell.execute_reply": "2024-12-30T11:38:31.316823Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.141224,
     "end_time": "2024-12-30T11:38:31.320418",
     "exception": false,
     "start_time": "2024-12-30T11:38:31.179194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HDFCAMC</th>\n",
       "      <th>APOLLOHOSP</th>\n",
       "      <th>BOSCHLTD</th>\n",
       "      <th>INDUSTOWER</th>\n",
       "      <th>BERGEPAINT</th>\n",
       "      <th>MARICO</th>\n",
       "      <th>ICICIPRULI</th>\n",
       "      <th>HDFCLIFE</th>\n",
       "      <th>SBIN</th>\n",
       "      <th>SUNPHARMA</th>\n",
       "      <th>...</th>\n",
       "      <th>SBICARD</th>\n",
       "      <th>DABUR</th>\n",
       "      <th>SAIL</th>\n",
       "      <th>BRITANNIA</th>\n",
       "      <th>ADANIPORTS</th>\n",
       "      <th>ONGC</th>\n",
       "      <th>EICHERMOT</th>\n",
       "      <th>TECHM</th>\n",
       "      <th>JINDALSTEL</th>\n",
       "      <th>HEROMOTOCO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-08-06 10:47:00+05:30</th>\n",
       "      <td>1803.95</td>\n",
       "      <td>974.30</td>\n",
       "      <td>19187.45</td>\n",
       "      <td>290.0</td>\n",
       "      <td>316.85</td>\n",
       "      <td>357.95</td>\n",
       "      <td>408.50</td>\n",
       "      <td>485.00</td>\n",
       "      <td>306.70</td>\n",
       "      <td>579.65</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>435.30</td>\n",
       "      <td>79.35</td>\n",
       "      <td>3194.92</td>\n",
       "      <td>400.70</td>\n",
       "      <td>167.75</td>\n",
       "      <td>2711.30</td>\n",
       "      <td>670.45</td>\n",
       "      <td>207.65</td>\n",
       "      <td>3259.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06 10:48:00+05:30</th>\n",
       "      <td>1803.90</td>\n",
       "      <td>973.70</td>\n",
       "      <td>19187.45</td>\n",
       "      <td>290.0</td>\n",
       "      <td>316.50</td>\n",
       "      <td>357.55</td>\n",
       "      <td>408.10</td>\n",
       "      <td>484.95</td>\n",
       "      <td>306.70</td>\n",
       "      <td>579.30</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>435.15</td>\n",
       "      <td>79.30</td>\n",
       "      <td>3193.02</td>\n",
       "      <td>400.55</td>\n",
       "      <td>167.55</td>\n",
       "      <td>2710.60</td>\n",
       "      <td>670.55</td>\n",
       "      <td>207.80</td>\n",
       "      <td>3260.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06 10:49:00+05:30</th>\n",
       "      <td>1806.00</td>\n",
       "      <td>973.70</td>\n",
       "      <td>19187.45</td>\n",
       "      <td>290.0</td>\n",
       "      <td>316.85</td>\n",
       "      <td>357.20</td>\n",
       "      <td>408.85</td>\n",
       "      <td>485.00</td>\n",
       "      <td>306.75</td>\n",
       "      <td>579.45</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>435.70</td>\n",
       "      <td>79.30</td>\n",
       "      <td>3192.50</td>\n",
       "      <td>400.30</td>\n",
       "      <td>167.55</td>\n",
       "      <td>2709.80</td>\n",
       "      <td>670.65</td>\n",
       "      <td>207.55</td>\n",
       "      <td>3259.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06 10:50:00+05:30</th>\n",
       "      <td>1806.35</td>\n",
       "      <td>974.00</td>\n",
       "      <td>19174.90</td>\n",
       "      <td>290.0</td>\n",
       "      <td>316.80</td>\n",
       "      <td>357.65</td>\n",
       "      <td>408.90</td>\n",
       "      <td>484.10</td>\n",
       "      <td>306.75</td>\n",
       "      <td>578.10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>435.65</td>\n",
       "      <td>79.30</td>\n",
       "      <td>3192.67</td>\n",
       "      <td>399.80</td>\n",
       "      <td>167.50</td>\n",
       "      <td>2708.05</td>\n",
       "      <td>670.90</td>\n",
       "      <td>207.90</td>\n",
       "      <td>3258.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-06 10:51:00+05:30</th>\n",
       "      <td>1809.00</td>\n",
       "      <td>975.15</td>\n",
       "      <td>19190.00</td>\n",
       "      <td>290.0</td>\n",
       "      <td>316.80</td>\n",
       "      <td>357.75</td>\n",
       "      <td>408.85</td>\n",
       "      <td>484.20</td>\n",
       "      <td>306.85</td>\n",
       "      <td>578.50</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436.00</td>\n",
       "      <td>79.30</td>\n",
       "      <td>3194.02</td>\n",
       "      <td>400.30</td>\n",
       "      <td>167.40</td>\n",
       "      <td>2709.95</td>\n",
       "      <td>671.20</td>\n",
       "      <td>207.80</td>\n",
       "      <td>3258.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-25 12:27:00+05:30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2589.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-25 12:28:00+05:30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2589.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-25 12:29:00+05:30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2590.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-25 12:30:00+05:30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2591.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-25 12:31:00+05:30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2591.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>677188 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           HDFCAMC  APOLLOHOSP  BOSCHLTD  INDUSTOWER  \\\n",
       "date                                                                   \n",
       "2018-08-06 10:47:00+05:30  1803.95      974.30  19187.45       290.0   \n",
       "2018-08-06 10:48:00+05:30  1803.90      973.70  19187.45       290.0   \n",
       "2018-08-06 10:49:00+05:30  1806.00      973.70  19187.45       290.0   \n",
       "2018-08-06 10:50:00+05:30  1806.35      974.00  19174.90       290.0   \n",
       "2018-08-06 10:51:00+05:30  1809.00      975.15  19190.00       290.0   \n",
       "...                            ...         ...       ...         ...   \n",
       "2022-10-25 12:27:00+05:30      NaN         NaN       NaN         NaN   \n",
       "2022-10-25 12:28:00+05:30      NaN         NaN       NaN         NaN   \n",
       "2022-10-25 12:29:00+05:30      NaN         NaN       NaN         NaN   \n",
       "2022-10-25 12:30:00+05:30      NaN         NaN       NaN         NaN   \n",
       "2022-10-25 12:31:00+05:30      NaN         NaN       NaN         NaN   \n",
       "\n",
       "                           BERGEPAINT  MARICO  ICICIPRULI  HDFCLIFE    SBIN  \\\n",
       "date                                                                          \n",
       "2018-08-06 10:47:00+05:30      316.85  357.95      408.50    485.00  306.70   \n",
       "2018-08-06 10:48:00+05:30      316.50  357.55      408.10    484.95  306.70   \n",
       "2018-08-06 10:49:00+05:30      316.85  357.20      408.85    485.00  306.75   \n",
       "2018-08-06 10:50:00+05:30      316.80  357.65      408.90    484.10  306.75   \n",
       "2018-08-06 10:51:00+05:30      316.80  357.75      408.85    484.20  306.85   \n",
       "...                               ...     ...         ...       ...     ...   \n",
       "2022-10-25 12:27:00+05:30         NaN     NaN         NaN       NaN     NaN   \n",
       "2022-10-25 12:28:00+05:30         NaN     NaN         NaN       NaN     NaN   \n",
       "2022-10-25 12:29:00+05:30         NaN     NaN         NaN       NaN     NaN   \n",
       "2022-10-25 12:30:00+05:30         NaN     NaN         NaN       NaN     NaN   \n",
       "2022-10-25 12:31:00+05:30         NaN     NaN         NaN       NaN     NaN   \n",
       "\n",
       "                           SUNPHARMA  ...  SBICARD   DABUR   SAIL  BRITANNIA  \\\n",
       "date                                  ...                                      \n",
       "2018-08-06 10:47:00+05:30     579.65  ...      NaN  435.30  79.35    3194.92   \n",
       "2018-08-06 10:48:00+05:30     579.30  ...      NaN  435.15  79.30    3193.02   \n",
       "2018-08-06 10:49:00+05:30     579.45  ...      NaN  435.70  79.30    3192.50   \n",
       "2018-08-06 10:50:00+05:30     578.10  ...      NaN  435.65  79.30    3192.67   \n",
       "2018-08-06 10:51:00+05:30     578.50  ...      NaN  436.00  79.30    3194.02   \n",
       "...                              ...  ...      ...     ...    ...        ...   \n",
       "2022-10-25 12:27:00+05:30        NaN  ...      NaN     NaN    NaN        NaN   \n",
       "2022-10-25 12:28:00+05:30        NaN  ...      NaN     NaN    NaN        NaN   \n",
       "2022-10-25 12:29:00+05:30        NaN  ...      NaN     NaN    NaN        NaN   \n",
       "2022-10-25 12:30:00+05:30        NaN  ...      NaN     NaN    NaN        NaN   \n",
       "2022-10-25 12:31:00+05:30        NaN  ...      NaN     NaN    NaN        NaN   \n",
       "\n",
       "                           ADANIPORTS    ONGC  EICHERMOT   TECHM  JINDALSTEL  \\\n",
       "date                                                                           \n",
       "2018-08-06 10:47:00+05:30      400.70  167.75    2711.30  670.45      207.65   \n",
       "2018-08-06 10:48:00+05:30      400.55  167.55    2710.60  670.55      207.80   \n",
       "2018-08-06 10:49:00+05:30      400.30  167.55    2709.80  670.65      207.55   \n",
       "2018-08-06 10:50:00+05:30      399.80  167.50    2708.05  670.90      207.90   \n",
       "2018-08-06 10:51:00+05:30      400.30  167.40    2709.95  671.20      207.80   \n",
       "...                               ...     ...        ...     ...         ...   \n",
       "2022-10-25 12:27:00+05:30         NaN     NaN        NaN     NaN         NaN   \n",
       "2022-10-25 12:28:00+05:30         NaN     NaN        NaN     NaN         NaN   \n",
       "2022-10-25 12:29:00+05:30         NaN     NaN        NaN     NaN         NaN   \n",
       "2022-10-25 12:30:00+05:30         NaN     NaN        NaN     NaN         NaN   \n",
       "2022-10-25 12:31:00+05:30         NaN     NaN        NaN     NaN         NaN   \n",
       "\n",
       "                           HEROMOTOCO  \n",
       "date                                   \n",
       "2018-08-06 10:47:00+05:30     3259.00  \n",
       "2018-08-06 10:48:00+05:30     3260.60  \n",
       "2018-08-06 10:49:00+05:30     3259.50  \n",
       "2018-08-06 10:50:00+05:30     3258.50  \n",
       "2018-08-06 10:51:00+05:30     3258.00  \n",
       "...                               ...  \n",
       "2022-10-25 12:27:00+05:30     2589.95  \n",
       "2022-10-25 12:28:00+05:30     2589.30  \n",
       "2022-10-25 12:29:00+05:30     2590.20  \n",
       "2022-10-25 12:30:00+05:30     2591.00  \n",
       "2022-10-25 12:31:00+05:30     2591.00  \n",
       "\n",
       "[677188 rows x 101 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa255c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:38:31.343412Z",
     "iopub.status.busy": "2024-12-30T11:38:31.342966Z",
     "iopub.status.idle": "2024-12-30T11:38:31.512364Z",
     "shell.execute_reply": "2024-12-30T11:38:31.511350Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.183634,
     "end_time": "2024-12-30T11:38:31.514667",
     "exception": false,
     "start_time": "2024-12-30T11:38:31.331033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['NIFTY 50', 'NIFTY BANK'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242b98d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:38:31.537780Z",
     "iopub.status.busy": "2024-12-30T11:38:31.536734Z",
     "iopub.status.idle": "2024-12-30T11:38:31.960592Z",
     "shell.execute_reply": "2024-12-30T11:38:31.959543Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.438101,
     "end_time": "2024-12-30T11:38:31.963273",
     "exception": false,
     "start_time": "2024-12-30T11:38:31.525172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing rows with more than 25 stocks without data\n",
    "nan_counts = df.isna().sum(axis=1)\n",
    "df = df[nan_counts <= 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f45d4239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:38:31.986278Z",
     "iopub.status.busy": "2024-12-30T11:38:31.985888Z",
     "iopub.status.idle": "2024-12-30T11:38:59.876950Z",
     "shell.execute_reply": "2024-12-30T11:38:59.875715Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 27.905586,
     "end_time": "2024-12-30T11:38:59.879588",
     "exception": false,
     "start_time": "2024-12-30T11:38:31.974002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest period of NaN values in column HDFCAMC: 305198 periods\n",
      "Longest period of NaN values in column APOLLOHOSP: 1 periods\n",
      "Longest period of NaN values in column BOSCHLTD: 3 periods\n",
      "Longest period of NaN values in column INDUSTOWER: 1 periods\n",
      "Longest period of NaN values in column BERGEPAINT: 1 periods\n",
      "Longest period of NaN values in column MARICO: 0 periods\n",
      "Longest period of NaN values in column ICICIPRULI: 144000 periods\n",
      "Longest period of NaN values in column HDFCLIFE: 242175 periods\n",
      "Longest period of NaN values in column SBIN: 0 periods\n",
      "Longest period of NaN values in column SUNPHARMA: 0 periods\n",
      "Longest period of NaN values in column COALINDIA: 0 periods\n",
      "Longest period of NaN values in column WIPRO: 1 periods\n",
      "Longest period of NaN values in column BAJAJ-AUTO: 1 periods\n",
      "Longest period of NaN values in column GODREJCP: 1 periods\n",
      "Longest period of NaN values in column UPL: 0 periods\n",
      "Longest period of NaN values in column SIEMENS: 0 periods\n",
      "Longest period of NaN values in column BPCL: 0 periods\n",
      "Longest period of NaN values in column TATAMOTORS: 0 periods\n",
      "Longest period of NaN values in column BANKBARODA: 0 periods\n",
      "Longest period of NaN values in column GAIL: 0 periods\n",
      "Longest period of NaN values in column HDFCBANK: 0 periods\n",
      "Longest period of NaN values in column BHARTIARTL: 1 periods\n",
      "Longest period of NaN values in column NMDC: 1 periods\n",
      "Longest period of NaN values in column INDUSINDBK: 1 periods\n",
      "Longest period of NaN values in column NTPC: 0 periods\n",
      "Longest period of NaN values in column JSWSTEEL: 375 periods\n",
      "Longest period of NaN values in column HCLTECH: 0 periods\n",
      "Longest period of NaN values in column AMBUJACEM: 375 periods\n",
      "Longest period of NaN values in column YESBANK: 7 periods\n",
      "Longest period of NaN values in column PNB: 0 periods\n",
      "Longest period of NaN values in column NESTLEIND: 2 periods\n",
      "Longest period of NaN values in column TORNTPHARM: 2 periods\n",
      "Longest period of NaN values in column NAUKRI: 11 periods\n",
      "Longest period of NaN values in column ULTRACEMCO: 0 periods\n",
      "Longest period of NaN values in column PIDILITIND: 1 periods\n",
      "Longest period of NaN values in column BAJAJHLDNG: 9 periods\n",
      "Longest period of NaN values in column HINDALCO: 0 periods\n",
      "Longest period of NaN values in column IOC: 375 periods\n",
      "Longest period of NaN values in column ADANIENT: 375 periods\n",
      "Longest period of NaN values in column MUTHOOTFIN: 6 periods\n",
      "Longest period of NaN values in column TATACONSUM: 1 periods\n",
      "Longest period of NaN values in column ICICIBANK: 1 periods\n",
      "Longest period of NaN values in column POWERGRID: 0 periods\n",
      "Longest period of NaN values in column SHREECEM: 5 periods\n",
      "Longest period of NaN values in column HAVELLS: 0 periods\n",
      "Longest period of NaN values in column GLAND: 305198 periods\n",
      "Longest period of NaN values in column DRREDDY: 0 periods\n",
      "Longest period of NaN values in column MCDOWELL-N: 1 periods\n",
      "Longest period of NaN values in column ITC: 375 periods\n",
      "Longest period of NaN values in column LUPIN: 1 periods\n",
      "Longest period of NaN values in column IGL: 375 periods\n",
      "Longest period of NaN values in column COLPAL: 1 periods\n",
      "Longest period of NaN values in column VEDL: 1 periods\n",
      "Longest period of NaN values in column BAJFINANCE: 2 periods\n",
      "Longest period of NaN values in column AUROPHARMA: 1 periods\n",
      "Longest period of NaN values in column ADANIGREEN: 292800 periods\n",
      "Longest period of NaN values in column BAJAJFINSV: 2 periods\n",
      "Longest period of NaN values in column LT: 375 periods\n",
      "Longest period of NaN values in column RELIANCE: 0 periods\n",
      "Longest period of NaN values in column PIIND: 7 periods\n",
      "Longest period of NaN values in column HINDPETRO: 375 periods\n",
      "Longest period of NaN values in column CIPLA: 1 periods\n",
      "Longest period of NaN values in column INFY: 0 periods\n",
      "Longest period of NaN values in column CHOLAFIN: 18 periods\n",
      "Longest period of NaN values in column PGHH: 43 periods\n",
      "Longest period of NaN values in column TITAN: 0 periods\n",
      "Longest period of NaN values in column ACC: 375 periods\n",
      "Longest period of NaN values in column PEL: 28 periods\n",
      "Longest period of NaN values in column KOTAKBANK: 375 periods\n",
      "Longest period of NaN values in column DMART: 184560 periods\n",
      "Longest period of NaN values in column TATASTEEL: 0 periods\n",
      "Longest period of NaN values in column HINDUNILVR: 0 periods\n",
      "Longest period of NaN values in column DIVISLAB: 1 periods\n",
      "Longest period of NaN values in column TCS: 0 periods\n",
      "Longest period of NaN values in column DLF: 0 periods\n",
      "Longest period of NaN values in column ICICIGI: 230115 periods\n",
      "Longest period of NaN values in column MM: 375 periods\n",
      "Longest period of NaN values in column HDFC: 0 periods\n",
      "Longest period of NaN values in column INDIGO: 68212 periods\n",
      "Longest period of NaN values in column BANDHANBNK: 272925 periods\n",
      "Longest period of NaN values in column AXISBANK: 1 periods\n",
      "Longest period of NaN values in column LTI: 127125 periods\n",
      "Longest period of NaN values in column GRASIM: 29 periods\n",
      "Longest period of NaN values in column JUBLFOOD: 1 periods\n",
      "Longest period of NaN values in column SBILIFE: 231240 periods\n",
      "Longest period of NaN values in column MARUTI: 0 periods\n",
      "Longest period of NaN values in column ASIANPAINT: 0 periods\n",
      "Longest period of NaN values in column BIOCON: 0 periods\n",
      "Longest period of NaN values in column LICI: 327223 periods\n",
      "Longest period of NaN values in column SBICARD: 305197 periods\n",
      "Longest period of NaN values in column DABUR: 1 periods\n",
      "Longest period of NaN values in column SAIL: 1 periods\n",
      "Longest period of NaN values in column BRITANNIA: 1 periods\n",
      "Longest period of NaN values in column ADANIPORTS: 0 periods\n",
      "Longest period of NaN values in column ONGC: 0 periods\n",
      "Longest period of NaN values in column EICHERMOT: 0 periods\n",
      "Longest period of NaN values in column TECHM: 0 periods\n",
      "Longest period of NaN values in column JINDALSTEL: 0 periods\n",
      "Longest period of NaN values in column HEROMOTOCO: 1 periods\n"
     ]
    }
   ],
   "source": [
    "# Now we observe per column, the longest period with NaN values\n",
    "longest_nan_periods = {column: longest_nan_period(df[column]) for column in df.columns}\n",
    "\n",
    "# Print the longest NaN period for each column\n",
    "for column, period in longest_nan_periods.items():\n",
    "    print(f\"Longest period of NaN values in column {column}: {period} periods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485780c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:38:59.903115Z",
     "iopub.status.busy": "2024-12-30T11:38:59.902702Z",
     "iopub.status.idle": "2024-12-30T11:39:00.115705Z",
     "shell.execute_reply": "2024-12-30T11:39:00.114481Z"
    },
    "papermill": {
     "duration": 0.227973,
     "end_time": "2024-12-30T11:39:00.118341",
     "exception": false,
     "start_time": "2024-12-30T11:38:59.890368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['HDFCAMC', 'ICICIPRULI', 'HDFCLIFE', 'GLAND', 'ADANIGREEN', 'LTI', 'SBICARD',\n",
    "                 'DMART','ICICIGI','BANDHANBNK','INDIGO','SBILIFE','LICI'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74e62121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:00.141841Z",
     "iopub.status.busy": "2024-12-30T11:39:00.141401Z",
     "iopub.status.idle": "2024-12-30T11:39:00.213906Z",
     "shell.execute_reply": "2024-12-30T11:39:00.212700Z"
    },
    "papermill": {
     "duration": 0.086831,
     "end_time": "2024-12-30T11:39:00.216180",
     "exception": false,
     "start_time": "2024-12-30T11:39:00.129349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values: 30836\n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows with NaN values\n",
    "nan_rows = df.isna().any(axis=1)\n",
    "num_nan_rows = nan_rows.sum()\n",
    "print(f\"Number of rows with NaN values: {num_nan_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b4c883b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:00.239516Z",
     "iopub.status.busy": "2024-12-30T11:39:00.239135Z",
     "iopub.status.idle": "2024-12-30T11:39:00.518505Z",
     "shell.execute_reply": "2024-12-30T11:39:00.517379Z"
    },
    "papermill": {
     "duration": 0.293839,
     "end_time": "2024-12-30T11:39:00.521048",
     "exception": false,
     "start_time": "2024-12-30T11:39:00.227209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows that still have NaNs in place\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "664c3167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:00.544245Z",
     "iopub.status.busy": "2024-12-30T11:39:00.543848Z",
     "iopub.status.idle": "2024-12-30T11:39:00.895166Z",
     "shell.execute_reply": "2024-12-30T11:39:00.894040Z"
    },
    "papermill": {
     "duration": 0.365874,
     "end_time": "2024-12-30T11:39:00.897737",
     "exception": false,
     "start_time": "2024-12-30T11:39:00.531863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.index = df.index.tz_localize(None)\n",
    "df_total = df.join(nifty_100_closing, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2390c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:00.921160Z",
     "iopub.status.busy": "2024-12-30T11:39:00.920762Z",
     "iopub.status.idle": "2024-12-30T11:39:01.231927Z",
     "shell.execute_reply": "2024-12-30T11:39:01.230812Z"
    },
    "papermill": {
     "duration": 0.325732,
     "end_time": "2024-12-30T11:39:01.234283",
     "exception": false,
     "start_time": "2024-12-30T11:39:00.908551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_total.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf8798c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:01.258302Z",
     "iopub.status.busy": "2024-12-30T11:39:01.257923Z",
     "iopub.status.idle": "2024-12-30T11:39:02.457449Z",
     "shell.execute_reply": "2024-12-30T11:39:02.456257Z"
    },
    "papermill": {
     "duration": 1.214862,
     "end_time": "2024-12-30T11:39:02.460125",
     "exception": false,
     "start_time": "2024-12-30T11:39:01.245263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resample to Hourly Period\n",
    "hourly_df = df_total.resample('H').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4567a329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:02.483532Z",
     "iopub.status.busy": "2024-12-30T11:39:02.483130Z",
     "iopub.status.idle": "2024-12-30T11:39:04.397173Z",
     "shell.execute_reply": "2024-12-30T11:39:04.395265Z"
    },
    "papermill": {
     "duration": 1.928511,
     "end_time": "2024-12-30T11:39:04.399693",
     "exception": false,
     "start_time": "2024-12-30T11:39:02.471182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with large changes (>100.0%): 42\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage change\n",
    "X_pct_change = df_total.pct_change()\n",
    "\n",
    "# Define the threshold for large changes (e.g., 100% change)\n",
    "threshold = 1.0  # 1.0 corresponds to 100% when working with pct_change()\n",
    "\n",
    "# Find rows with any column having a percentage change greater than the threshold\n",
    "large_change_rows = X_pct_change[(X_pct_change.abs() > threshold).any(axis=1)]\n",
    "\n",
    "# Get the number of rows where there are large percentage changes\n",
    "num_large_changes = large_change_rows.shape[0]\n",
    "\n",
    "print(f\"Number of rows with large changes (>{threshold * 100}%): {num_large_changes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88bc91a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:04.434348Z",
     "iopub.status.busy": "2024-12-30T11:39:04.433764Z",
     "iopub.status.idle": "2024-12-30T11:39:04.441457Z",
     "shell.execute_reply": "2024-12-30T11:39:04.439850Z"
    },
    "papermill": {
     "duration": 0.029075,
     "end_time": "2024-12-30T11:39:04.445059",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.415984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629791\n"
     ]
    }
   ],
   "source": [
    "print(len(df_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e40fc60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:04.470952Z",
     "iopub.status.busy": "2024-12-30T11:39:04.470504Z",
     "iopub.status.idle": "2024-12-30T11:39:04.695407Z",
     "shell.execute_reply": "2024-12-30T11:39:04.694074Z"
    },
    "papermill": {
     "duration": 0.241156,
     "end_time": "2024-12-30T11:39:04.698019",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.456863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_total.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2124d519",
   "metadata": {
    "papermill": {
     "duration": 0.010495,
     "end_time": "2024-12-30T11:39:04.719646",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.709151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Models\n",
    "\n",
    "Firstly, let's explain the general architecture of the model, firstly, we must understand the pitfalls of using time series data, and how it affects our choice of cross validation strategy. Since the order of data must be preserved, we cannot use a conventional cross validation, and rather opt for a TimeSeriesSplit, specifically, we set the training and test sizes of the window, to get the desired sliding window result, where the training window is roughly 1 year, and the testing widnow around 3 months, this yields a total of 18 windows.\n",
    "\n",
    "Knowing this, we propose the following general architecture, for a given model and threshold:\n",
    "\n",
    "For each fold:\n",
    "\n",
    "1. Perform feature selection (ex: Lasso with high alpha, Dropping Correlated variables, Mutual Information) using the threshold.\n",
    "2. Train model on reduced feature space, with hyperparameter tuning to choose best parameters.\n",
    "3. Save the tracking error and number of features (stocks) used in training\n",
    "\n",
    "Finally, we return the following averages over the folds\n",
    "* Average number of stocks.\n",
    "* Average tracking error.\n",
    "\n",
    "We will observe how different models (LinearRegression, ElasticNet, XGBRegressor and RandomForestRegressor) produce predictions and produce a plot for the average tracking error against the average number of stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa39fb",
   "metadata": {
    "papermill": {
     "duration": 0.010894,
     "end_time": "2024-12-30T11:39:04.742193",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.731299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Correlation - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d68381",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:04.766596Z",
     "iopub.status.busy": "2024-12-30T11:39:04.766184Z",
     "iopub.status.idle": "2024-12-30T11:39:04.773601Z",
     "shell.execute_reply": "2024-12-30T11:39:04.772463Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.022261,
     "end_time": "2024-12-30T11:39:04.776056",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.753795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_correlated_features(X_train, threshold):\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    \n",
    "    # Compute the correlation matrix (absolute values)\n",
    "    corr_matrix = X_train.corr().abs()\n",
    "    \n",
    "    # Create a mask to select only the upper triangle of the correlation matrix\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features with correlation greater than the threshold\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "    print(f\"Features to drop (correlation > {threshold}): {to_drop}\")\n",
    "    selected_features = [i for i in range(X_train.shape[1]) if i not in to_drop]\n",
    "    \n",
    "    return selected_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4b82823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:04.800978Z",
     "iopub.status.busy": "2024-12-30T11:39:04.800160Z",
     "iopub.status.idle": "2024-12-30T11:39:04.811237Z",
     "shell.execute_reply": "2024-12-30T11:39:04.810023Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.02599,
     "end_time": "2024-12-30T11:39:04.813489",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.787499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def Corr_Lasso(threshold, X, y, debug=False):\n",
    "    tscv = TimeSeriesSplit(n_splits=13, max_train_size=85000, test_size=45000)\n",
    "    stocks = []\n",
    "    num_stocks = []\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_train_cv)\n",
    "        X_test_scaled = scaler.transform(X_test_cv)\n",
    "        \n",
    "        # Feature selection using correlation matrix\n",
    "        selected_indices = remove_correlated_features(X_scaled, threshold)\n",
    "\n",
    "        X_train_selected = X_scaled[:, selected_indices]\n",
    "        X_test_selected = X_test_scaled[:, selected_indices]\n",
    "        if debug == True:\n",
    "            print(f\"Selected {len(selected_indices)} stocks\")\n",
    "        num_stocks.append(len(selected_indices))\n",
    "        stocks.append(selected_indices)\n",
    "\n",
    "        best_alpha = None\n",
    "        best_score = float('inf')  # Assuming lower is better for your metric\n",
    "        for alph in [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025]:\n",
    "            lasso = Lasso(alpha=alph, max_iter=15000)\n",
    "            lasso.fit(X_train_selected, y_train_cv)\n",
    "            y_pred = lasso.predict(X_test_selected)\n",
    "            fold_score = tracking_error(y_test_cv, y_pred)\n",
    "            \n",
    "            if fold_score < best_score:\n",
    "                best_score = fold_score\n",
    "                best_alpha = alph\n",
    "\n",
    "        print(f\"Best Alpha for this fold: {best_alpha}, Best Fold Score: {best_score}\")\n",
    "        fold_scores.append(best_score)\n",
    "\n",
    "    # Calculate the average score across all folds\n",
    "    average_score = np.mean(fold_scores)\n",
    "\n",
    "    # Get average stocks chosen over all folds\n",
    "    average_selected_features = np.mean(num_stocks)\n",
    "\n",
    "    # Print the tracking error for this alpha\n",
    "    print(f\"Threshold: {threshold}, Tracking Error: {average_score}\")\n",
    "    return stocks, average_selected_features, average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee4f3ca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:04.838858Z",
     "iopub.status.busy": "2024-12-30T11:39:04.838385Z",
     "iopub.status.idle": "2024-12-30T11:39:04.848155Z",
     "shell.execute_reply": "2024-12-30T11:39:04.846584Z"
    },
    "papermill": {
     "duration": 0.024975,
     "end_time": "2024-12-30T11:39:04.850483",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.825508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Corr_Lasso_plot(X,y):\n",
    "    thresholds = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "    tracking_errors = []\n",
    "    num_stocks_list = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        print(f\"----------Starting threshold: {threshold}-------------\")\n",
    "        _, num_stocks, tracking_err = Corr_Lasso(threshold, X, y, debug=False)\n",
    "        num_stocks_list.append(num_stocks)\n",
    "        tracking_errors.append(tracking_err)\n",
    "    \n",
    "    # Plotting the results\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    # Plotting number of stocks on the primary y-axis\n",
    "    color = 'tab:blue'    \n",
    "    ax1.set_xlabel('Alpha')\n",
    "    ax1.set_ylabel('Number of Stocks', color=color)    \n",
    "    ax1.plot(thresholds, num_stocks_list, color=color, marker='o', label='Number of Stocks')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    # Creating a second y-axis for tracking error\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Tracking Error', color=color)\n",
    "    #ax2.plot(alphas, tracking_errors, color='tab:green', marker='o', linestyle='--', label='Tracking Error')\n",
    "    ax2.plot(thresholds, tracking_errors, color=color, marker='x', linestyle='--', label='Adjusted Tracking Error')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    # Adding a title and legend\n",
    "    plt.title('Number of Stocks and Tracking Error vs. Alpha')    \n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.savefig('Corr-Lasso.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2ff26",
   "metadata": {
    "papermill": {
     "duration": 0.010991,
     "end_time": "2024-12-30T11:39:04.873842",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.862851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lasso - Lasso\n",
    "\n",
    "\n",
    "\n",
    "We will utilize a Lasso model with a high alpha value to perform feature selection, this will drop the coefficients for less important features to 0. After that we retrain a Lasso model with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e97241db",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:04.898894Z",
     "iopub.status.busy": "2024-12-30T11:39:04.897722Z",
     "iopub.status.idle": "2024-12-30T11:39:04.911482Z",
     "shell.execute_reply": "2024-12-30T11:39:04.910294Z"
    },
    "papermill": {
     "duration": 0.02857,
     "end_time": "2024-12-30T11:39:04.913884",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.885314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def Lasso_Lasso(alpha, X, y):    \n",
    "    tscv = TimeSeriesSplit(n_splits=15, max_train_size=15000, test_size=4500)\n",
    "    stocks = []\n",
    "    num_stocks = []\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "        train_period = X.iloc[train_index].index\n",
    "        test_period = X.iloc[test_index].index\n",
    "\n",
    "        # Convert to date format\n",
    "        train_start_date, train_end_date = train_period.min(), train_period.max()\n",
    "        test_start_date, test_end_date = test_period.min(), test_period.max()\n",
    "\n",
    "        print(f\"Train period: {train_start_date} to {train_end_date}\")\n",
    "        print(f\"Test period: {test_start_date} to {test_end_date}\")\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_train_cv)\n",
    "        X_test_scaled = scaler.transform(X_test_cv)\n",
    "        \n",
    "        # Feature selection with alpha from list\n",
    "        lasso_fs = Lasso(alpha=alpha, max_iter=200)\n",
    "        lasso_fs.fit(X_scaled, y_train_cv)\n",
    "\n",
    "        # for feature, coef in zip(X_train_cv.columns, lasso_fs.coef_):\n",
    "        #    print(f\"{feature}: {coef}\")\n",
    "\n",
    "        selector = SelectFromModel(lasso_fs, threshold=10e-10, max_features=None)  # Adjust threshold as needed\n",
    "        X_selected_train = selector.transform(X_scaled)\n",
    "        X_selected_test = selector.transform(X_test_scaled)\n",
    "\n",
    "        selected_features = np.where(selector.get_support())[0] \n",
    "        print(f\"Selected {len(selected_features)} stocks\")\n",
    "        num_stocks.append(len(selected_features))\n",
    "        stocks.append(selected_features)\n",
    "\n",
    "        best_alpha = None\n",
    "        best_score = float('inf')  # Assuming lower is better for your metric\n",
    "        for alph in [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025]:\n",
    "            lasso = Lasso(alpha=alph, max_iter=15000)\n",
    "            lasso.fit(X_selected_train, y_train_cv)\n",
    "            y_pred = lasso.predict(X_selected_test)\n",
    "            \n",
    "            fold_score = tracking_error(y_test_cv, y_pred)\n",
    "            \n",
    "            if fold_score < best_score:\n",
    "                best_score = fold_score\n",
    "                best_alpha = alph\n",
    "\n",
    "        #print(f\"Best Alpha for this fold: {best_alpha}, Best Fold Score: {best_score}\")\n",
    "        fold_scores.append(best_score)\n",
    "\n",
    "    # Calculate the average score across all folds\n",
    "    average_score = np.mean(fold_scores)\n",
    "\n",
    "    # Get average stocks chosen over all folds\n",
    "    average_selected_features = np.mean(num_stocks)\n",
    "\n",
    "    # Print the tracking error for this alpha\n",
    "    print(f\"Alpha: {alpha}, Number of Stocks: {average_selected_features}, Average Tracking Error (bps): {average_score}\")\n",
    "    return stocks, average_selected_features, average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89a466b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:04.939324Z",
     "iopub.status.busy": "2024-12-30T11:39:04.938911Z",
     "iopub.status.idle": "2024-12-30T11:39:04.947439Z",
     "shell.execute_reply": "2024-12-30T11:39:04.946403Z"
    },
    "papermill": {
     "duration": 0.023952,
     "end_time": "2024-12-30T11:39:04.949431",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.925479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Lasso_Lasso_plot(X,y):\n",
    "    alphas = np.logspace(-3, -4, 10)\n",
    "    tracking_errors = []\n",
    "    num_stocks_list = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        print(f\"----------Starting alpha {alpha}-------------\")\n",
    "        _, num_stocks, tracking_err = Lasso_Lasso(alpha, X, y)\n",
    "        num_stocks_list.append(num_stocks)\n",
    "        tracking_errors.append(tracking_err)\n",
    "    \n",
    "    # Plotting the results\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    # Plotting number of stocks on the primary y-axis\n",
    "    color = 'tab:blue'    \n",
    "    ax1.set_xlabel('Alpha')\n",
    "    ax1.set_ylabel('Number of Stocks', color=color)    \n",
    "    ax1.plot(alphas, num_stocks_list, color=color, marker='o', label='Number of Stocks')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    # Creating a second y-axis for tracking error\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Tracking Error', color=color)\n",
    "    #ax2.plot(alphas, tracking_errors, color='tab:green', marker='o', linestyle='--', label='Tracking Error')\n",
    "    ax2.plot(alphas, tracking_errors, color=color, marker='x', linestyle='--', label='Adjusted Tracking Error')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    # Adding a title and legend\n",
    "    plt.title('Number of Stocks and Tracking Error vs. Alpha')    \n",
    "    fig.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.savefig('Lasso-Lasso.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f69bb8",
   "metadata": {
    "papermill": {
     "duration": 0.010637,
     "end_time": "2024-12-30T11:39:04.971064",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.960427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Mutual Info - Lasso\n",
    "\n",
    "Using mutual information between variables, which measures the dependency between them, 0 value being independence. By getting the mutual information between the features (stocks) and the target (NIFTY 100) we can \"discard\" the variables with lower dependencies. After feature selection, we traing a Lasso model with CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "311320a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:04.995486Z",
     "iopub.status.busy": "2024-12-30T11:39:04.995058Z",
     "iopub.status.idle": "2024-12-30T11:39:05.007855Z",
     "shell.execute_reply": "2024-12-30T11:39:05.006688Z"
    },
    "papermill": {
     "duration": 0.027805,
     "end_time": "2024-12-30T11:39:05.010370",
     "exception": false,
     "start_time": "2024-12-30T11:39:04.982565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def MI_Experiment(percentile, X, y, model_class, params, debug=False):\n",
    "    tscv = TimeSeriesSplit(n_splits=15, max_train_size=15000, test_size=4500)\n",
    "    stocks = []\n",
    "    num_stocks = []\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_train_cv)\n",
    "        X_test_scaled = scaler.transform(X_test_cv)\n",
    "\n",
    "        # Feature selection with alpha from list\n",
    "        mi_scores = mutual_info_regression(X_train_cv, y_train_cv)\n",
    "        mi_scores_series = pd.Series(mi_scores, index=X_train_cv.columns)\n",
    "\n",
    "        threshold_value = mi_scores_series.quantile(percentile)        \n",
    "\n",
    "        selected_features = mi_scores_series[mi_scores_series > threshold_value].index\n",
    "        selected_indices = [X_train_cv.columns.get_loc(feature) for feature in selected_features]\n",
    "\n",
    "        # Filter the NumPy arrays based on the selected feature indices\n",
    "        X_train_selected = X_scaled[:, selected_indices]\n",
    "        X_test_selected = X_test_scaled[:, selected_indices]\n",
    "        # Initial index value\n",
    "        \n",
    "\n",
    "        if debug == True:\n",
    "            print(f\"Selected {len(selected_features)} stocks\")\n",
    "        num_stocks.append(len(selected_features))\n",
    "        stocks.append(selected_features)\n",
    "\n",
    "        best_params = None\n",
    "        best_score = float('inf')  # Assuming lower is better for your metric\n",
    "        for param in ParameterGrid(params):\n",
    "            if debug == True:\n",
    "                print(f\"Trying combination {param}\")\n",
    "            model = model_class(**param)\n",
    "            model.fit(X_train_selected, y_train_cv)\n",
    "            y_pred = model.predict(X_test_selected)\n",
    "\n",
    "            train_fold_score = tracking_error(y_train_cv, model.predict(X_train_selected))\n",
    "            fold_score = tracking_error(y_test_cv, y_pred)\n",
    "            print(f\"Train Error: {train_fold_score}, Test Error: {fold_score}\")\n",
    "\n",
    "            if fold_score < best_score:\n",
    "                best_score = fold_score\n",
    "                best_params = param\n",
    "        if debug == True:\n",
    "            print(f\"Best Params for this fold: {best_params}, Best Test Score: {best_score}\")\n",
    "        fold_scores.append(best_score)\n",
    "\n",
    "    # Calculate the average score across all folds\n",
    "    average_score = np.mean(fold_scores)\n",
    "\n",
    "    # Get average stocks chosen over all folds\n",
    "    average_selected_features = np.mean(num_stocks)\n",
    "\n",
    "    # Print the tracking error for this alpha\n",
    "    print(f\"Percentile: {percentile}, Tracking Error: {average_score}\")\n",
    "    return stocks, average_selected_features, average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75d7d8",
   "metadata": {
    "papermill": {
     "duration": 0.010619,
     "end_time": "2024-12-30T11:39:05.031874",
     "exception": false,
     "start_time": "2024-12-30T11:39:05.021255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### MI - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7bfcea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:05.055432Z",
     "iopub.status.busy": "2024-12-30T11:39:05.055044Z",
     "iopub.status.idle": "2024-12-30T11:39:05.062920Z",
     "shell.execute_reply": "2024-12-30T11:39:05.061480Z"
    },
    "papermill": {
     "duration": 0.0224,
     "end_time": "2024-12-30T11:39:05.065291",
     "exception": false,
     "start_time": "2024-12-30T11:39:05.042891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MI_Lasso_plot(X,y):\n",
    "    tracking_errors = []\n",
    "    num_stocks_list = []\n",
    "    \n",
    "    percentiles = [0.6]\n",
    "    param_grid = {\n",
    "        'alpha': [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025],\n",
    "        'max_iter': [15000]\n",
    "    }\n",
    "    \n",
    "    for percentile in percentiles:\n",
    "        print(f\"Starting percentile: {percentile}\")\n",
    "        _, num_stocks, tracking_err = MI_Experiment(percentile, X, y, Lasso, param_grid)\n",
    "        num_stocks_list.append(num_stocks)\n",
    "        tracking_errors.append(tracking_err)\n",
    "    \n",
    "    # Plotting the results\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    # Plotting number of stocks on the primary y-axis\n",
    "    plt.plot(num_stocks_list, tracking_errors, marker='x')\n",
    "    plt.xlabel('Number of Stocks')\n",
    "    plt.ylabel('Tracking Error')\n",
    "    plt.title('Lasso - Tracking Errors vs Number of Stocks')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"MI-Lasso.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65937f",
   "metadata": {
    "papermill": {
     "duration": 0.010569,
     "end_time": "2024-12-30T11:39:05.086655",
     "exception": false,
     "start_time": "2024-12-30T11:39:05.076086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### MI - ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f56ba4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:05.110739Z",
     "iopub.status.busy": "2024-12-30T11:39:05.110312Z",
     "iopub.status.idle": "2024-12-30T11:39:05.118338Z",
     "shell.execute_reply": "2024-12-30T11:39:05.117203Z"
    },
    "papermill": {
     "duration": 0.022786,
     "end_time": "2024-12-30T11:39:05.120791",
     "exception": false,
     "start_time": "2024-12-30T11:39:05.098005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MI_ElasticNet_plot(X,y):\n",
    "    tracking_errors = []\n",
    "    num_stocks_list = []\n",
    "    \n",
    "    percentiles = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "    param_grid = {\n",
    "        'alpha': [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025],\n",
    "        'l1_ratio': [0.1, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    \n",
    "    for percentile in percentiles:\n",
    "        print(f\"Starting percentile: {percentile}\")\n",
    "        _, num_stocks, tracking_err = MI_Experiment(percentile, X, y, ElasticNet, param_grid)\n",
    "        num_stocks_list.append(num_stocks)\n",
    "        tracking_errors.append(tracking_err)\n",
    "    \n",
    "    # Plotting the results\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    # Plotting number of stocks on the primary y-axis\n",
    "    plt.plot(num_stocks_list, tracking_errors, marker='x')\n",
    "    plt.xlabel('Number of Stocks')\n",
    "    plt.ylabel('Tracking Error')\n",
    "    plt.title('ElasticNet - Tracking Errors vs Number of Stocks ')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"MI-ElasticNet.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14fba3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T21:50:37.572889Z",
     "iopub.status.busy": "2024-11-12T21:50:37.570440Z"
    },
    "papermill": {
     "duration": 0.010751,
     "end_time": "2024-12-30T11:39:05.143262",
     "exception": false,
     "start_time": "2024-12-30T11:39:05.132511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TODO: Theil-Sein Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e1039e",
   "metadata": {
    "papermill": {
     "duration": 0.01075,
     "end_time": "2024-12-30T11:39:05.165026",
     "exception": false,
     "start_time": "2024-12-30T11:39:05.154276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### MI-RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f279720d",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:05.189500Z",
     "iopub.status.busy": "2024-12-30T11:39:05.189084Z",
     "iopub.status.idle": "2024-12-30T11:39:05.197543Z",
     "shell.execute_reply": "2024-12-30T11:39:05.196483Z"
    },
    "papermill": {
     "duration": 0.023291,
     "end_time": "2024-12-30T11:39:05.199653",
     "exception": false,
     "start_time": "2024-12-30T11:39:05.176362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MI_RF_plot(X,y):\n",
    "    tracking_errors = []\n",
    "    num_stocks_list = []\n",
    "    \n",
    "    percentiles = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 20, 30, 40, 50],       # Number of trees in the forest\n",
    "        'max_depth': [5, 10, 15],      # Maximum depth of the tree\n",
    "        'max_samples': [0.5],\n",
    "        'min_samples_split': [0.05],\n",
    "        'bootstrap': [True],\n",
    "        'n_jobs': [-1],\n",
    "        'max_features': ['sqrt']\n",
    "    }\n",
    "    \n",
    "    for percentile in percentiles:\n",
    "        print(f\"Starting percentile: {percentile}\")\n",
    "        _, num_stocks, tracking_err = MI_Experiment(percentile, X, y, RandomForestRegressor, param_grid)\n",
    "        num_stocks_list.append(num_stocks)\n",
    "        tracking_errors.append(tracking_err)\n",
    "    \n",
    "    # Plotting the results\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    # Plotting number of stocks on the primary y-axis\n",
    "    plt.plot(num_stocks_list, tracking_errors, marker='x')\n",
    "    plt.xlabel('Number of Stocks')\n",
    "    plt.ylabel('Tracking Error')\n",
    "    plt.title('RandomForestRegressor - Tracking Errors vs Number of Stocks ')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"MI-RandomForestRegressor.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3f4b1",
   "metadata": {
    "papermill": {
     "duration": 0.010548,
     "end_time": "2024-12-30T11:39:05.221675",
     "exception": false,
     "start_time": "2024-12-30T11:39:05.211127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### MI - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fa2146c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:05.245484Z",
     "iopub.status.busy": "2024-12-30T11:39:05.245072Z",
     "iopub.status.idle": "2024-12-30T11:39:05.253274Z",
     "shell.execute_reply": "2024-12-30T11:39:05.252024Z"
    },
    "papermill": {
     "duration": 0.022751,
     "end_time": "2024-12-30T11:39:05.255394",
     "exception": false,
     "start_time": "2024-12-30T11:39:05.232643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MI_XGBoost_plot(X,y):\n",
    "    tracking_errors = []\n",
    "    num_stocks_list = []\n",
    "    \n",
    "    percentiles = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    param_grid = {\n",
    "        'n_estimators': [300, 350, 400],  # Number of boosting rounds\n",
    "        'learning_rate': [0.075, 0.1, 0.085],  # Step size shrinkage\n",
    "        'colsample_bytree': [0.5, 0.7],\n",
    "        'max_depth': [6, 7, 9],  # Maximum depth of a tree\n",
    "        'n_jobs': [-1],\n",
    "        'booster': ['gblinear'],\n",
    "        'tree_method': ['gpu_hist'],\n",
    "    }\n",
    "    \n",
    "    for percentile in percentiles:\n",
    "        print(f\"Starting percentile: {percentile}\")\n",
    "        _, num_stocks, tracking_err = MI_Experiment(percentile, X, y, xgb.XGBRegressor, param_grid)\n",
    "        num_stocks_list.append(num_stocks)\n",
    "        tracking_errors.append(tracking_err)\n",
    "    \n",
    "    # Plotting the results\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    # Plotting number of stocks on the primary y-axis\n",
    "    plt.plot(num_stocks_list, tracking_errors, marker='x')\n",
    "    plt.xlabel('Number of Stocks')\n",
    "    plt.ylabel('Tracking Error')\n",
    "    plt.title('XGBRegressor - Tracking Errors vs Number of Stocks ')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"MI-XGBRegressor.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3d27413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:05.279142Z",
     "iopub.status.busy": "2024-12-30T11:39:05.278222Z",
     "iopub.status.idle": "2024-12-30T11:39:08.262032Z",
     "shell.execute_reply": "2024-12-30T11:39:08.260863Z"
    },
    "papermill": {
     "duration": 2.99831,
     "end_time": "2024-12-30T11:39:08.264540",
     "exception": false,
     "start_time": "2024-12-30T11:39:05.266230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_total_z = df_total.replace(0, np.nan)  # Convert zeroes back to NaNs\n",
    "df_total_returns = df_total_z.pct_change().dropna()\n",
    "df_total_returns = df_total_returns*100\n",
    "X = df_total_returns.drop(columns=['NIFTY_100'])\n",
    "y = df_total_returns['NIFTY_100']\n",
    "\n",
    "hourly_df_total_z = hourly_df.replace(0, np.nan)  # Convert zeroes back to NaNs\n",
    "hourly_df_total_returns = hourly_df_total_z.pct_change().dropna()\n",
    "hourly_df_total_returns = hourly_df_total_returns*100\n",
    "hourly_X = hourly_df_total_returns.drop(columns=['NIFTY_100'])\n",
    "hourly_y = hourly_df_total_returns['NIFTY_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59604208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:08.288138Z",
     "iopub.status.busy": "2024-12-30T11:39:08.287732Z",
     "iopub.status.idle": "2024-12-30T11:39:08.292535Z",
     "shell.execute_reply": "2024-12-30T11:39:08.291541Z"
    },
    "papermill": {
     "duration": 0.019218,
     "end_time": "2024-12-30T11:39:08.294763",
     "exception": false,
     "start_time": "2024-12-30T11:39:08.275545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Corr_Lasso_plot(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f682600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:39:08.318675Z",
     "iopub.status.busy": "2024-12-30T11:39:08.318273Z",
     "iopub.status.idle": "2024-12-30T11:45:43.833300Z",
     "shell.execute_reply": "2024-12-30T11:45:43.831896Z"
    },
    "papermill": {
     "duration": 395.530045,
     "end_time": "2024-12-30T11:45:43.836138",
     "exception": false,
     "start_time": "2024-12-30T11:39:08.306093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Starting alpha 0.001-------------\n",
      "Train period: 2015-02-02 11:00:00 to 2015-02-11 07:00:00\n",
      "Test period: 2015-02-11 08:00:00 to 2015-08-17 19:00:00\n",
      "Selected 41 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.299e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2015-02-02 11:00:00 to 2015-08-17 19:00:00\n",
      "Test period: 2015-08-17 20:00:00 to 2016-02-21 07:00:00\n",
      "Selected 72 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-02-21 07:00:00\n",
      "Test period: 2016-02-21 08:00:00 to 2016-08-26 19:00:00\n",
      "Selected 74 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-08-26 19:00:00\n",
      "Test period: 2016-08-26 20:00:00 to 2017-03-02 07:00:00\n",
      "Selected 77 stocks\n",
      "Train period: 2015-06-16 08:00:00 to 2017-03-02 07:00:00\n",
      "Test period: 2017-03-02 08:00:00 to 2017-09-05 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.931e-02, tolerance: 3.239e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 82 stocks\n",
      "Train period: 2015-12-20 20:00:00 to 2017-09-05 19:00:00\n",
      "Test period: 2017-09-05 20:00:00 to 2018-03-12 07:00:00\n",
      "Selected 82 stocks\n",
      "Train period: 2016-06-25 08:00:00 to 2018-03-12 07:00:00\n",
      "Test period: 2018-03-12 08:00:00 to 2018-09-15 19:00:00\n",
      "Selected 81 stocks\n",
      "Train period: 2016-12-29 20:00:00 to 2018-09-15 19:00:00\n",
      "Test period: 2018-09-15 20:00:00 to 2019-03-22 07:00:00\n",
      "Selected 78 stocks\n",
      "Train period: 2017-07-05 08:00:00 to 2019-03-22 07:00:00\n",
      "Test period: 2019-03-22 08:00:00 to 2019-09-25 19:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2018-01-08 20:00:00 to 2019-09-25 19:00:00\n",
      "Test period: 2019-09-25 20:00:00 to 2020-03-31 07:00:00\n",
      "Selected 82 stocks\n",
      "Train period: 2018-07-15 08:00:00 to 2020-03-31 07:00:00\n",
      "Test period: 2020-03-31 08:00:00 to 2020-10-04 19:00:00\n",
      "Selected 83 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.961e-01, tolerance: 8.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2019-01-18 20:00:00 to 2020-10-04 19:00:00\n",
      "Test period: 2020-10-04 20:00:00 to 2021-04-10 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.249e+00, tolerance: 9.858e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2019-07-25 08:00:00 to 2021-04-10 07:00:00\n",
      "Test period: 2021-04-10 08:00:00 to 2021-10-14 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.287e+00, tolerance: 1.046e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 84 stocks\n",
      "Train period: 2020-01-28 20:00:00 to 2021-10-14 19:00:00\n",
      "Test period: 2021-10-14 20:00:00 to 2022-04-20 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.380e+00, tolerance: 1.037e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 84 stocks\n",
      "Train period: 2020-08-03 08:00:00 to 2022-04-20 07:00:00\n",
      "Test period: 2022-04-20 08:00:00 to 2022-10-24 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.991e-02, tolerance: 3.832e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Alpha: 0.001, Number of Stocks: 78.06666666666666, Average Tracking Error (bps): 1.3503236042378859\n",
      "----------Starting alpha 0.000774263682681127-------------\n",
      "Train period: 2015-02-02 11:00:00 to 2015-02-11 07:00:00\n",
      "Test period: 2015-02-11 08:00:00 to 2015-08-17 19:00:00\n",
      "Selected 46 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.909e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2015-02-02 11:00:00 to 2015-08-17 19:00:00\n",
      "Test period: 2015-08-17 20:00:00 to 2016-02-21 07:00:00\n",
      "Selected 75 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-02-21 07:00:00\n",
      "Test period: 2016-02-21 08:00:00 to 2016-08-26 19:00:00\n",
      "Selected 77 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-08-26 19:00:00\n",
      "Test period: 2016-08-26 20:00:00 to 2017-03-02 07:00:00\n",
      "Selected 79 stocks\n",
      "Train period: 2015-06-16 08:00:00 to 2017-03-02 07:00:00\n",
      "Test period: 2017-03-02 08:00:00 to 2017-09-05 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.345e-02, tolerance: 3.239e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 82 stocks\n",
      "Train period: 2015-12-20 20:00:00 to 2017-09-05 19:00:00\n",
      "Test period: 2017-09-05 20:00:00 to 2018-03-12 07:00:00\n",
      "Selected 82 stocks\n",
      "Train period: 2016-06-25 08:00:00 to 2018-03-12 07:00:00\n",
      "Test period: 2018-03-12 08:00:00 to 2018-09-15 19:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2016-12-29 20:00:00 to 2018-09-15 19:00:00\n",
      "Test period: 2018-09-15 20:00:00 to 2019-03-22 07:00:00\n",
      "Selected 80 stocks\n",
      "Train period: 2017-07-05 08:00:00 to 2019-03-22 07:00:00\n",
      "Test period: 2019-03-22 08:00:00 to 2019-09-25 19:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2018-01-08 20:00:00 to 2019-09-25 19:00:00\n",
      "Test period: 2019-09-25 20:00:00 to 2020-03-31 07:00:00\n",
      "Selected 82 stocks\n",
      "Train period: 2018-07-15 08:00:00 to 2020-03-31 07:00:00\n",
      "Test period: 2020-03-31 08:00:00 to 2020-10-04 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.024e+00, tolerance: 8.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-01-18 20:00:00 to 2020-10-04 19:00:00\n",
      "Test period: 2020-10-04 20:00:00 to 2021-04-10 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.301e+00, tolerance: 9.858e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2019-07-25 08:00:00 to 2021-04-10 07:00:00\n",
      "Test period: 2021-04-10 08:00:00 to 2021-10-14 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.315e+00, tolerance: 1.046e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 84 stocks\n",
      "Train period: 2020-01-28 20:00:00 to 2021-10-14 19:00:00\n",
      "Test period: 2021-10-14 20:00:00 to 2022-04-20 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.404e+00, tolerance: 1.037e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2020-08-03 08:00:00 to 2022-04-20 07:00:00\n",
      "Test period: 2022-04-20 08:00:00 to 2022-10-24 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.195e-02, tolerance: 3.832e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Alpha: 0.000774263682681127, Number of Stocks: 79.46666666666667, Average Tracking Error (bps): 1.3475875107297093\n",
      "----------Starting alpha 0.0005994842503189409-------------\n",
      "Train period: 2015-02-02 11:00:00 to 2015-02-11 07:00:00\n",
      "Test period: 2015-02-11 08:00:00 to 2015-08-17 19:00:00\n",
      "Selected 47 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.751e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2015-02-02 11:00:00 to 2015-08-17 19:00:00\n",
      "Test period: 2015-08-17 20:00:00 to 2016-02-21 07:00:00\n",
      "Selected 76 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-02-21 07:00:00\n",
      "Test period: 2016-02-21 08:00:00 to 2016-08-26 19:00:00\n",
      "Selected 78 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-08-26 19:00:00\n",
      "Test period: 2016-08-26 20:00:00 to 2017-03-02 07:00:00\n",
      "Selected 79 stocks\n",
      "Train period: 2015-06-16 08:00:00 to 2017-03-02 07:00:00\n",
      "Test period: 2017-03-02 08:00:00 to 2017-09-05 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.776e-02, tolerance: 3.239e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2015-12-20 20:00:00 to 2017-09-05 19:00:00\n",
      "Test period: 2017-09-05 20:00:00 to 2018-03-12 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2016-06-25 08:00:00 to 2018-03-12 07:00:00\n",
      "Test period: 2018-03-12 08:00:00 to 2018-09-15 19:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2016-12-29 20:00:00 to 2018-09-15 19:00:00\n",
      "Test period: 2018-09-15 20:00:00 to 2019-03-22 07:00:00\n",
      "Selected 82 stocks\n",
      "Train period: 2017-07-05 08:00:00 to 2019-03-22 07:00:00\n",
      "Test period: 2019-03-22 08:00:00 to 2019-09-25 19:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2018-01-08 20:00:00 to 2019-09-25 19:00:00\n",
      "Test period: 2019-09-25 20:00:00 to 2020-03-31 07:00:00\n",
      "Selected 82 stocks\n",
      "Train period: 2018-07-15 08:00:00 to 2020-03-31 07:00:00\n",
      "Test period: 2020-03-31 08:00:00 to 2020-10-04 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.071e+00, tolerance: 8.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-01-18 20:00:00 to 2020-10-04 19:00:00\n",
      "Test period: 2020-10-04 20:00:00 to 2021-04-10 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.361e+00, tolerance: 9.858e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 84 stocks\n",
      "Train period: 2019-07-25 08:00:00 to 2021-04-10 07:00:00\n",
      "Test period: 2021-04-10 08:00:00 to 2021-10-14 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.403e+00, tolerance: 1.046e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 84 stocks\n",
      "Train period: 2020-01-28 20:00:00 to 2021-10-14 19:00:00\n",
      "Test period: 2021-10-14 20:00:00 to 2022-04-20 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.498e+00, tolerance: 1.037e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2020-08-03 08:00:00 to 2022-04-20 07:00:00\n",
      "Test period: 2022-04-20 08:00:00 to 2022-10-24 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.546e-02, tolerance: 3.832e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Alpha: 0.0005994842503189409, Number of Stocks: 80.0, Average Tracking Error (bps): 1.346114748339023\n",
      "----------Starting alpha 0.00046415888336127773-------------\n",
      "Train period: 2015-02-02 11:00:00 to 2015-02-11 07:00:00\n",
      "Test period: 2015-02-11 08:00:00 to 2015-08-17 19:00:00\n",
      "Selected 49 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.259e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2015-02-02 11:00:00 to 2015-08-17 19:00:00\n",
      "Test period: 2015-08-17 20:00:00 to 2016-02-21 07:00:00\n",
      "Selected 78 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-02-21 07:00:00\n",
      "Test period: 2016-02-21 08:00:00 to 2016-08-26 19:00:00\n",
      "Selected 79 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-08-26 19:00:00\n",
      "Test period: 2016-08-26 20:00:00 to 2017-03-02 07:00:00\n",
      "Selected 79 stocks\n",
      "Train period: 2015-06-16 08:00:00 to 2017-03-02 07:00:00\n",
      "Test period: 2017-03-02 08:00:00 to 2017-09-05 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.133e-02, tolerance: 3.239e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2015-12-20 20:00:00 to 2017-09-05 19:00:00\n",
      "Test period: 2017-09-05 20:00:00 to 2018-03-12 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2016-06-25 08:00:00 to 2018-03-12 07:00:00\n",
      "Test period: 2018-03-12 08:00:00 to 2018-09-15 19:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2016-12-29 20:00:00 to 2018-09-15 19:00:00\n",
      "Test period: 2018-09-15 20:00:00 to 2019-03-22 07:00:00\n",
      "Selected 82 stocks\n",
      "Train period: 2017-07-05 08:00:00 to 2019-03-22 07:00:00\n",
      "Test period: 2019-03-22 08:00:00 to 2019-09-25 19:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2018-01-08 20:00:00 to 2019-09-25 19:00:00\n",
      "Test period: 2019-09-25 20:00:00 to 2020-03-31 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2018-07-15 08:00:00 to 2020-03-31 07:00:00\n",
      "Test period: 2020-03-31 08:00:00 to 2020-10-04 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.119e+00, tolerance: 8.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-01-18 20:00:00 to 2020-10-04 19:00:00\n",
      "Test period: 2020-10-04 20:00:00 to 2021-04-10 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.444e+00, tolerance: 9.858e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 84 stocks\n",
      "Train period: 2019-07-25 08:00:00 to 2021-04-10 07:00:00\n",
      "Test period: 2021-04-10 08:00:00 to 2021-10-14 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.515e+00, tolerance: 1.046e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 84 stocks\n",
      "Train period: 2020-01-28 20:00:00 to 2021-10-14 19:00:00\n",
      "Test period: 2021-10-14 20:00:00 to 2022-04-20 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.585e+00, tolerance: 1.037e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 84 stocks\n",
      "Train period: 2020-08-03 08:00:00 to 2022-04-20 07:00:00\n",
      "Test period: 2022-04-20 08:00:00 to 2022-10-24 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.946e-02, tolerance: 3.832e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Alpha: 0.00046415888336127773, Number of Stocks: 80.4, Average Tracking Error (bps): 1.3472185248780058\n",
      "----------Starting alpha 0.00035938136638046257-------------\n",
      "Train period: 2015-02-02 11:00:00 to 2015-02-11 07:00:00\n",
      "Test period: 2015-02-11 08:00:00 to 2015-08-17 19:00:00\n",
      "Selected 49 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.773e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2015-02-02 11:00:00 to 2015-08-17 19:00:00\n",
      "Test period: 2015-08-17 20:00:00 to 2016-02-21 07:00:00\n",
      "Selected 78 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-02-21 07:00:00\n",
      "Test period: 2016-02-21 08:00:00 to 2016-08-26 19:00:00\n",
      "Selected 79 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-08-26 19:00:00\n",
      "Test period: 2016-08-26 20:00:00 to 2017-03-02 07:00:00\n",
      "Selected 80 stocks\n",
      "Train period: 2015-06-16 08:00:00 to 2017-03-02 07:00:00\n",
      "Test period: 2017-03-02 08:00:00 to 2017-09-05 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.437e-02, tolerance: 3.239e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2015-12-20 20:00:00 to 2017-09-05 19:00:00\n",
      "Test period: 2017-09-05 20:00:00 to 2018-03-12 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2016-06-25 08:00:00 to 2018-03-12 07:00:00\n",
      "Test period: 2018-03-12 08:00:00 to 2018-09-15 19:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2016-12-29 20:00:00 to 2018-09-15 19:00:00\n",
      "Test period: 2018-09-15 20:00:00 to 2019-03-22 07:00:00\n",
      "Selected 82 stocks\n",
      "Train period: 2017-07-05 08:00:00 to 2019-03-22 07:00:00\n",
      "Test period: 2019-03-22 08:00:00 to 2019-09-25 19:00:00\n",
      "Selected 85 stocks\n",
      "Train period: 2018-01-08 20:00:00 to 2019-09-25 19:00:00\n",
      "Test period: 2019-09-25 20:00:00 to 2020-03-31 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2018-07-15 08:00:00 to 2020-03-31 07:00:00\n",
      "Test period: 2020-03-31 08:00:00 to 2020-10-04 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.135e+00, tolerance: 8.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-01-18 20:00:00 to 2020-10-04 19:00:00\n",
      "Test period: 2020-10-04 20:00:00 to 2021-04-10 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.559e+00, tolerance: 9.858e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 82 stocks\n",
      "Train period: 2019-07-25 08:00:00 to 2021-04-10 07:00:00\n",
      "Test period: 2021-04-10 08:00:00 to 2021-10-14 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.582e+00, tolerance: 1.046e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2020-01-28 20:00:00 to 2021-10-14 19:00:00\n",
      "Test period: 2021-10-14 20:00:00 to 2022-04-20 07:00:00\n",
      "Selected 83 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.640e+00, tolerance: 1.037e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2020-08-03 08:00:00 to 2022-04-20 07:00:00\n",
      "Test period: 2022-04-20 08:00:00 to 2022-10-24 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.841e-02, tolerance: 3.832e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Alpha: 0.00035938136638046257, Number of Stocks: 80.6, Average Tracking Error (bps): 1.346246395117748\n",
      "----------Starting alpha 0.0002782559402207126-------------\n",
      "Train period: 2015-02-02 11:00:00 to 2015-02-11 07:00:00\n",
      "Test period: 2015-02-11 08:00:00 to 2015-08-17 19:00:00\n",
      "Selected 52 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.038e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2015-02-02 11:00:00 to 2015-08-17 19:00:00\n",
      "Test period: 2015-08-17 20:00:00 to 2016-02-21 07:00:00\n",
      "Selected 79 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-02-21 07:00:00\n",
      "Test period: 2016-02-21 08:00:00 to 2016-08-26 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.365e-02, tolerance: 2.337e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 81 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-08-26 19:00:00\n",
      "Test period: 2016-08-26 20:00:00 to 2017-03-02 07:00:00\n",
      "Selected 82 stocks\n",
      "Train period: 2015-06-16 08:00:00 to 2017-03-02 07:00:00\n",
      "Test period: 2017-03-02 08:00:00 to 2017-09-05 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.895e-02, tolerance: 3.239e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2015-12-20 20:00:00 to 2017-09-05 19:00:00\n",
      "Test period: 2017-09-05 20:00:00 to 2018-03-12 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2016-06-25 08:00:00 to 2018-03-12 07:00:00\n",
      "Test period: 2018-03-12 08:00:00 to 2018-09-15 19:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2016-12-29 20:00:00 to 2018-09-15 19:00:00\n",
      "Test period: 2018-09-15 20:00:00 to 2019-03-22 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2017-07-05 08:00:00 to 2019-03-22 07:00:00\n",
      "Test period: 2019-03-22 08:00:00 to 2019-09-25 19:00:00\n",
      "Selected 85 stocks\n",
      "Train period: 2018-01-08 20:00:00 to 2019-09-25 19:00:00\n",
      "Test period: 2019-09-25 20:00:00 to 2020-03-31 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2018-07-15 08:00:00 to 2020-03-31 07:00:00\n",
      "Test period: 2020-03-31 08:00:00 to 2020-10-04 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.142e+00, tolerance: 8.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-01-18 20:00:00 to 2020-10-04 19:00:00\n",
      "Test period: 2020-10-04 20:00:00 to 2021-04-10 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.628e+00, tolerance: 9.858e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 82 stocks\n",
      "Train period: 2019-07-25 08:00:00 to 2021-04-10 07:00:00\n",
      "Test period: 2021-04-10 08:00:00 to 2021-10-14 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.613e+00, tolerance: 1.046e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2020-01-28 20:00:00 to 2021-10-14 19:00:00\n",
      "Test period: 2021-10-14 20:00:00 to 2022-04-20 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.686e+00, tolerance: 1.037e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 82 stocks\n",
      "Train period: 2020-08-03 08:00:00 to 2022-04-20 07:00:00\n",
      "Test period: 2022-04-20 08:00:00 to 2022-10-24 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.995e-02, tolerance: 3.832e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Alpha: 0.0002782559402207126, Number of Stocks: 81.13333333333334, Average Tracking Error (bps): 1.3516995357692354\n",
      "----------Starting alpha 0.00021544346900318845-------------\n",
      "Train period: 2015-02-02 11:00:00 to 2015-02-11 07:00:00\n",
      "Test period: 2015-02-11 08:00:00 to 2015-08-17 19:00:00\n",
      "Selected 58 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e-02, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2015-02-02 11:00:00 to 2015-08-17 19:00:00\n",
      "Test period: 2015-08-17 20:00:00 to 2016-02-21 07:00:00\n",
      "Selected 80 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-02-21 07:00:00\n",
      "Test period: 2016-02-21 08:00:00 to 2016-08-26 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.231e-02, tolerance: 2.337e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 81 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-08-26 19:00:00\n",
      "Test period: 2016-08-26 20:00:00 to 2017-03-02 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2015-06-16 08:00:00 to 2017-03-02 07:00:00\n",
      "Test period: 2017-03-02 08:00:00 to 2017-09-05 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.757e-02, tolerance: 3.239e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2015-12-20 20:00:00 to 2017-09-05 19:00:00\n",
      "Test period: 2017-09-05 20:00:00 to 2018-03-12 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2016-06-25 08:00:00 to 2018-03-12 07:00:00\n",
      "Test period: 2018-03-12 08:00:00 to 2018-09-15 19:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2016-12-29 20:00:00 to 2018-09-15 19:00:00\n",
      "Test period: 2018-09-15 20:00:00 to 2019-03-22 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2017-07-05 08:00:00 to 2019-03-22 07:00:00\n",
      "Test period: 2019-03-22 08:00:00 to 2019-09-25 19:00:00\n",
      "Selected 85 stocks\n",
      "Train period: 2018-01-08 20:00:00 to 2019-09-25 19:00:00\n",
      "Test period: 2019-09-25 20:00:00 to 2020-03-31 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2018-07-15 08:00:00 to 2020-03-31 07:00:00\n",
      "Test period: 2020-03-31 08:00:00 to 2020-10-04 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.176e+00, tolerance: 8.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-01-18 20:00:00 to 2020-10-04 19:00:00\n",
      "Test period: 2020-10-04 20:00:00 to 2021-04-10 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.651e+00, tolerance: 9.858e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 82 stocks\n",
      "Train period: 2019-07-25 08:00:00 to 2021-04-10 07:00:00\n",
      "Test period: 2021-04-10 08:00:00 to 2021-10-14 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.622e+00, tolerance: 1.046e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2020-01-28 20:00:00 to 2021-10-14 19:00:00\n",
      "Test period: 2021-10-14 20:00:00 to 2022-04-20 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.807e+00, tolerance: 1.037e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 82 stocks\n",
      "Train period: 2020-08-03 08:00:00 to 2022-04-20 07:00:00\n",
      "Test period: 2022-04-20 08:00:00 to 2022-10-24 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.710e-02, tolerance: 3.832e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Alpha: 0.00021544346900318845, Number of Stocks: 81.66666666666667, Average Tracking Error (bps): 1.3557220206897114\n",
      "----------Starting alpha 0.0001668100537200059-------------\n",
      "Train period: 2015-02-02 11:00:00 to 2015-02-11 07:00:00\n",
      "Test period: 2015-02-11 08:00:00 to 2015-08-17 19:00:00\n",
      "Selected 60 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.196e-02, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2015-02-02 11:00:00 to 2015-08-17 19:00:00\n",
      "Test period: 2015-08-17 20:00:00 to 2016-02-21 07:00:00\n",
      "Selected 81 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-02-21 07:00:00\n",
      "Test period: 2016-02-21 08:00:00 to 2016-08-26 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.042e-02, tolerance: 2.337e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-08-26 19:00:00\n",
      "Test period: 2016-08-26 20:00:00 to 2017-03-02 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2015-06-16 08:00:00 to 2017-03-02 07:00:00\n",
      "Test period: 2017-03-02 08:00:00 to 2017-09-05 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.212e-02, tolerance: 3.239e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2015-12-20 20:00:00 to 2017-09-05 19:00:00\n",
      "Test period: 2017-09-05 20:00:00 to 2018-03-12 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2016-06-25 08:00:00 to 2018-03-12 07:00:00\n",
      "Test period: 2018-03-12 08:00:00 to 2018-09-15 19:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2016-12-29 20:00:00 to 2018-09-15 19:00:00\n",
      "Test period: 2018-09-15 20:00:00 to 2019-03-22 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2017-07-05 08:00:00 to 2019-03-22 07:00:00\n",
      "Test period: 2019-03-22 08:00:00 to 2019-09-25 19:00:00\n",
      "Selected 85 stocks\n",
      "Train period: 2018-01-08 20:00:00 to 2019-09-25 19:00:00\n",
      "Test period: 2019-09-25 20:00:00 to 2020-03-31 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2018-07-15 08:00:00 to 2020-03-31 07:00:00\n",
      "Test period: 2020-03-31 08:00:00 to 2020-10-04 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.182e+00, tolerance: 8.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-01-18 20:00:00 to 2020-10-04 19:00:00\n",
      "Test period: 2020-10-04 20:00:00 to 2021-04-10 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+00, tolerance: 9.858e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2019-07-25 08:00:00 to 2021-04-10 07:00:00\n",
      "Test period: 2021-04-10 08:00:00 to 2021-10-14 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e+00, tolerance: 1.046e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 84 stocks\n",
      "Train period: 2020-01-28 20:00:00 to 2021-10-14 19:00:00\n",
      "Test period: 2021-10-14 20:00:00 to 2022-04-20 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.847e+00, tolerance: 1.037e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 82 stocks\n",
      "Train period: 2020-08-03 08:00:00 to 2022-04-20 07:00:00\n",
      "Test period: 2022-04-20 08:00:00 to 2022-10-24 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.008e-01, tolerance: 3.832e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Alpha: 0.0001668100537200059, Number of Stocks: 82.13333333333334, Average Tracking Error (bps): 1.3545237498827485\n",
      "----------Starting alpha 0.0001291549665014884-------------\n",
      "Train period: 2015-02-02 11:00:00 to 2015-02-11 07:00:00\n",
      "Test period: 2015-02-11 08:00:00 to 2015-08-17 19:00:00\n",
      "Selected 64 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.223e-02, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2015-02-02 11:00:00 to 2015-08-17 19:00:00\n",
      "Test period: 2015-08-17 20:00:00 to 2016-02-21 07:00:00\n",
      "Selected 82 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-02-21 07:00:00\n",
      "Test period: 2016-02-21 08:00:00 to 2016-08-26 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.046e-02, tolerance: 2.337e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-08-26 19:00:00\n",
      "Test period: 2016-08-26 20:00:00 to 2017-03-02 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2015-06-16 08:00:00 to 2017-03-02 07:00:00\n",
      "Test period: 2017-03-02 08:00:00 to 2017-09-05 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.645e-02, tolerance: 3.239e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Train period: 2015-12-20 20:00:00 to 2017-09-05 19:00:00\n",
      "Test period: 2017-09-05 20:00:00 to 2018-03-12 07:00:00\n",
      "Selected 85 stocks\n",
      "Train period: 2016-06-25 08:00:00 to 2018-03-12 07:00:00\n",
      "Test period: 2018-03-12 08:00:00 to 2018-09-15 19:00:00\n",
      "Selected 85 stocks\n",
      "Train period: 2016-12-29 20:00:00 to 2018-09-15 19:00:00\n",
      "Test period: 2018-09-15 20:00:00 to 2019-03-22 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2017-07-05 08:00:00 to 2019-03-22 07:00:00\n",
      "Test period: 2019-03-22 08:00:00 to 2019-09-25 19:00:00\n",
      "Selected 85 stocks\n",
      "Train period: 2018-01-08 20:00:00 to 2019-09-25 19:00:00\n",
      "Test period: 2019-09-25 20:00:00 to 2020-03-31 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2018-07-15 08:00:00 to 2020-03-31 07:00:00\n",
      "Test period: 2020-03-31 08:00:00 to 2020-10-04 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.193e+00, tolerance: 8.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-01-18 20:00:00 to 2020-10-04 19:00:00\n",
      "Test period: 2020-10-04 20:00:00 to 2021-04-10 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.753e+00, tolerance: 9.858e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-07-25 08:00:00 to 2021-04-10 07:00:00\n",
      "Test period: 2021-04-10 08:00:00 to 2021-10-14 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.679e+00, tolerance: 1.046e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 84 stocks\n",
      "Train period: 2020-01-28 20:00:00 to 2021-10-14 19:00:00\n",
      "Test period: 2021-10-14 20:00:00 to 2022-04-20 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.906e+00, tolerance: 1.037e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2020-08-03 08:00:00 to 2022-04-20 07:00:00\n",
      "Test period: 2022-04-20 08:00:00 to 2022-10-24 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.026e-01, tolerance: 3.832e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Alpha: 0.0001291549665014884, Number of Stocks: 82.93333333333334, Average Tracking Error (bps): 1.3496517597580975\n",
      "----------Starting alpha 0.0001-------------\n",
      "Train period: 2015-02-02 11:00:00 to 2015-02-11 07:00:00\n",
      "Test period: 2015-02-11 08:00:00 to 2015-08-17 19:00:00\n",
      "Selected 68 stocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.234e-02, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2015-02-02 11:00:00 to 2015-08-17 19:00:00\n",
      "Test period: 2015-08-17 20:00:00 to 2016-02-21 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-02-21 07:00:00\n",
      "Test period: 2016-02-21 08:00:00 to 2016-08-26 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.014e-02, tolerance: 2.337e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2015-02-02 11:00:00 to 2016-08-26 19:00:00\n",
      "Test period: 2016-08-26 20:00:00 to 2017-03-02 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2015-06-16 08:00:00 to 2017-03-02 07:00:00\n",
      "Test period: 2017-03-02 08:00:00 to 2017-09-05 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.090e-02, tolerance: 3.239e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Train period: 2015-12-20 20:00:00 to 2017-09-05 19:00:00\n",
      "Test period: 2017-09-05 20:00:00 to 2018-03-12 07:00:00\n",
      "Selected 86 stocks\n",
      "Train period: 2016-06-25 08:00:00 to 2018-03-12 07:00:00\n",
      "Test period: 2018-03-12 08:00:00 to 2018-09-15 19:00:00\n",
      "Selected 86 stocks\n",
      "Train period: 2016-12-29 20:00:00 to 2018-09-15 19:00:00\n",
      "Test period: 2018-09-15 20:00:00 to 2019-03-22 07:00:00\n",
      "Selected 83 stocks\n",
      "Train period: 2017-07-05 08:00:00 to 2019-03-22 07:00:00\n",
      "Test period: 2019-03-22 08:00:00 to 2019-09-25 19:00:00\n",
      "Selected 85 stocks\n",
      "Train period: 2018-01-08 20:00:00 to 2019-09-25 19:00:00\n",
      "Test period: 2019-09-25 20:00:00 to 2020-03-31 07:00:00\n",
      "Selected 84 stocks\n",
      "Train period: 2018-07-15 08:00:00 to 2020-03-31 07:00:00\n",
      "Test period: 2020-03-31 08:00:00 to 2020-10-04 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.202e+00, tolerance: 8.498e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-01-18 20:00:00 to 2020-10-04 19:00:00\n",
      "Test period: 2020-10-04 20:00:00 to 2021-04-10 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e+00, tolerance: 9.858e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2019-07-25 08:00:00 to 2021-04-10 07:00:00\n",
      "Test period: 2021-04-10 08:00:00 to 2021-10-14 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.680e+00, tolerance: 1.046e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 83 stocks\n",
      "Train period: 2020-01-28 20:00:00 to 2021-10-14 19:00:00\n",
      "Test period: 2021-10-14 20:00:00 to 2022-04-20 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.901e+00, tolerance: 1.037e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 85 stocks\n",
      "Train period: 2020-08-03 08:00:00 to 2022-04-20 07:00:00\n",
      "Test period: 2022-04-20 08:00:00 to 2022-10-24 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.129e-01, tolerance: 3.832e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 stocks\n",
      "Alpha: 0.0001, Number of Stocks: 83.46666666666667, Average Tracking Error (bps): 1.3437665626777\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Lasso_Lasso_plot(hourly_X,hourly_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c2afeca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:45:43.896805Z",
     "iopub.status.busy": "2024-12-30T11:45:43.896378Z",
     "iopub.status.idle": "2024-12-30T11:45:43.900975Z",
     "shell.execute_reply": "2024-12-30T11:45:43.899910Z"
    },
    "papermill": {
     "duration": 0.038572,
     "end_time": "2024-12-30T11:45:43.903138",
     "exception": false,
     "start_time": "2024-12-30T11:45:43.864566",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MI_Lasso_plot(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "504e38cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T11:45:43.961445Z",
     "iopub.status.busy": "2024-12-30T11:45:43.961052Z",
     "iopub.status.idle": "2024-12-30T12:19:23.090553Z",
     "shell.execute_reply": "2024-12-30T12:19:23.089137Z"
    },
    "papermill": {
     "duration": 2019.161741,
     "end_time": "2024-12-30T12:19:23.093164",
     "exception": false,
     "start_time": "2024-12-30T11:45:43.931423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting percentile: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.618e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.646e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.00033256253066221285, Test Error: 1.4629415840818931\n",
      "Train Error: 0.001360313401799282, Test Error: 0.5286720548570398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.067e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0011140189554860722, Test Error: 0.40178222062170393\n",
      "Train Error: 0.0016865974966379894, Test Error: 0.41100534566729324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.837e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0010334711147001352, Test Error: 0.9521925912708091\n",
      "Train Error: 0.0024704286607037636, Test Error: 0.4151282084364441\n",
      "Train Error: 0.0034550525064757846, Test Error: 0.42705834999814846\n",
      "Train Error: 0.004390063861588344, Test Error: 0.44175058157097635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.714e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.001331281193174784, Test Error: 0.42514980526954277\n",
      "Train Error: 0.004797773344681374, Test Error: 0.4379646576982237\n",
      "Train Error: 0.006616116232697359, Test Error: 0.4580049475231932\n",
      "Train Error: 0.008304466118874865, Test Error: 0.46516080409265775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.723e-04, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.001797258642096902, Test Error: 0.40105908496996284\n",
      "Train Error: 0.007007654716458565, Test Error: 0.45468302839829716\n",
      "Train Error: 0.009303822509455825, Test Error: 0.4711809715907553\n",
      "Train Error: 0.011311555477373406, Test Error: 0.4923645102754304\n",
      "Train Error: 0.0021605961473387656, Test Error: 0.40844762560002557\n",
      "Train Error: 0.008976718058222085, Test Error: 0.46653331821110855\n",
      "Train Error: 0.011655233367618808, Test Error: 0.4947652275533895\n",
      "Train Error: 0.014367804196954348, Test Error: 0.5237491087780952\n",
      "Train Error: 0.004130633596181726, Test Error: 0.4162294809208582\n",
      "Train Error: 0.01572920600900917, Test Error: 0.5325935750460534\n",
      "Train Error: 0.021158578317537334, Test Error: 0.5841675786448789\n",
      "Train Error: 0.025920820542780543, Test Error: 0.6063867885013788\n",
      "Train Error: 0.005059824810996838, Test Error: 0.41891312536225994\n",
      "Train Error: 0.019242292699445662, Test Error: 0.5643350574700777\n",
      "Train Error: 0.025757202187475434, Test Error: 0.6021724808750383\n",
      "Train Error: 0.029204569632518447, Test Error: 0.5848317445312418\n",
      "Train Error: 0.13037444482991423, Test Error: 0.3438060360459244\n",
      "Train Error: 0.1304266516537742, Test Error: 0.34311058246439813\n",
      "Train Error: 0.1304540907004225, Test Error: 0.34271594427431895\n",
      "Train Error: 0.1304723119679586, Test Error: 0.34230143226990994\n",
      "Train Error: 0.13038588736885023, Test Error: 0.3434295402067622\n",
      "Train Error: 0.13050091463974223, Test Error: 0.3415570625293791\n",
      "Train Error: 0.13055038715128714, Test Error: 0.34050529413930164\n",
      "Train Error: 0.13063134451835726, Test Error: 0.33925398652047223\n",
      "Train Error: 0.13041735110357175, Test Error: 0.34304915316454354\n",
      "Train Error: 0.13067536281144362, Test Error: 0.3385646189527676\n",
      "Train Error: 0.13077910709094945, Test Error: 0.3364655327464815\n",
      "Train Error: 0.13099635971556473, Test Error: 0.33445424314734834\n",
      "Train Error: 0.1304463508583521, Test Error: 0.34247236969149586\n",
      "Train Error: 0.13082440025960815, Test Error: 0.33590391953464993\n",
      "Train Error: 0.13119392622474232, Test Error: 0.33292576074546837\n",
      "Train Error: 0.13192624423917543, Test Error: 0.3314163354011029\n",
      "Train Error: 0.1304653867867044, Test Error: 0.3418965858396477\n",
      "Train Error: 0.13112064614106034, Test Error: 0.3333746594462133\n",
      "Train Error: 0.13205698039300712, Test Error: 0.33115440745259384\n",
      "Train Error: 0.1332257305028734, Test Error: 0.3292780885608768\n",
      "Train Error: 0.13056430832124508, Test Error: 0.33953520291771644\n",
      "Train Error: 0.13384568817060108, Test Error: 0.3278941049030697\n",
      "Train Error: 0.1368050617905782, Test Error: 0.32284213232935244\n",
      "Train Error: 0.1400473905475208, Test Error: 0.31792392845922507\n",
      "Train Error: 0.13064896990917163, Test Error: 0.3381789338834569\n",
      "Train Error: 0.13558148528621153, Test Error: 0.32470419378714305\n",
      "Train Error: 0.13966148126855135, Test Error: 0.31843522667018515\n",
      "Train Error: 0.14377891656345673, Test Error: 0.3125291021429803\n",
      "Train Error: 0.142811950429769, Test Error: 0.12422367181140989\n",
      "Train Error: 0.14303601200457255, Test Error: 0.1239493948467529\n",
      "Train Error: 0.14316228136756784, Test Error: 0.12389181812653068\n",
      "Train Error: 0.14324990340874194, Test Error: 0.12388224963597928\n",
      "Train Error: 0.1428743715908146, Test Error: 0.12416418568278474\n",
      "Train Error: 0.14338484799925563, Test Error: 0.1239087229708593\n",
      "Train Error: 0.1436268185847176, Test Error: 0.12389418280876488\n",
      "Train Error: 0.14382333406214054, Test Error: 0.12414926582577367\n",
      "Train Error: 0.14302448013061542, Test Error: 0.12398120587633071\n",
      "Train Error: 0.143870179625621, Test Error: 0.12429204351937007\n",
      "Train Error: 0.14425453768434587, Test Error: 0.12510438287661507\n",
      "Train Error: 0.14470385575444425, Test Error: 0.12622223157808435\n",
      "Train Error: 0.1431728614159015, Test Error: 0.12395165597872604\n",
      "Train Error: 0.14435018465864288, Test Error: 0.12539644381854437\n",
      "Train Error: 0.14505941663471628, Test Error: 0.12714175030912486\n",
      "Train Error: 0.14583332001013247, Test Error: 0.129015839548309\n",
      "Train Error: 0.14325529675589777, Test Error: 0.12396313330891125\n",
      "Train Error: 0.14492195238440012, Test Error: 0.1268621354341066\n",
      "Train Error: 0.14595843722105054, Test Error: 0.1293575487530716\n",
      "Train Error: 0.14690150364493795, Test Error: 0.1323686533080518\n",
      "Train Error: 0.14368679968271486, Test Error: 0.12419571774285285\n",
      "Train Error: 0.14732147968791767, Test Error: 0.13403378695941737\n",
      "Train Error: 0.14939254416265138, Test Error: 0.14061354141249097\n",
      "Train Error: 0.15197037837230792, Test Error: 0.14757489288642003\n",
      "Train Error: 0.14377671359600558, Test Error: 0.12448944228452864\n",
      "Train Error: 0.14853264194414723, Test Error: 0.13812135918590035\n",
      "Train Error: 0.15159362700000337, Test Error: 0.14673793977123073\n",
      "Train Error: 0.15526782780273993, Test Error: 0.15600300258841068\n",
      "Train Error: 0.12665394744440298, Test Error: 0.15217634335093946\n",
      "Train Error: 0.12675584840383325, Test Error: 0.15224190663156503\n",
      "Train Error: 0.1268245295244775, Test Error: 0.152282999486482\n",
      "Train Error: 0.12689915375637992, Test Error: 0.15232674625250386\n",
      "Train Error: 0.12667823636844164, Test Error: 0.1521750753859173\n",
      "Train Error: 0.12704261332614503, Test Error: 0.15243735058620897\n",
      "Train Error: 0.12725753711917034, Test Error: 0.15293194474031122\n",
      "Train Error: 0.12737690538556887, Test Error: 0.15511322999054566\n",
      "Train Error: 0.12675666390626825, Test Error: 0.15217152413467686\n",
      "Train Error: 0.1274435134971855, Test Error: 0.15619938231412597\n",
      "Train Error: 0.12775815320824602, Test Error: 0.16055834081462475\n",
      "Train Error: 0.1281871060463771, Test Error: 0.16459346020534427\n",
      "Train Error: 0.1268431005189777, Test Error: 0.15222173581234844\n",
      "Train Error: 0.12785903593136758, Test Error: 0.16156295057620337\n",
      "Train Error: 0.1285159099849529, Test Error: 0.16764966049791644\n",
      "Train Error: 0.12923668358600796, Test Error: 0.17378374586806886\n",
      "Train Error: 0.12693736983587484, Test Error: 0.15231894945079183\n",
      "Train Error: 0.12840457299652855, Test Error: 0.16664394381735748\n",
      "Train Error: 0.12936679884247063, Test Error: 0.17480922848721175\n",
      "Train Error: 0.13048075059822412, Test Error: 0.18262923305319254\n",
      "Train Error: 0.12730888172960603, Test Error: 0.15403182951850017\n",
      "Train Error: 0.13107867317248736, Test Error: 0.18615316157533326\n",
      "Train Error: 0.13393654620430126, Test Error: 0.20011304545195735\n",
      "Train Error: 0.13732413357948356, Test Error: 0.2131133958898397\n",
      "Train Error: 0.12743733199109725, Test Error: 0.15623078506300897\n",
      "Train Error: 0.1327705805007345, Test Error: 0.19489342101472798\n",
      "Train Error: 0.13688561362027538, Test Error: 0.21152005703460106\n",
      "Train Error: 0.14172388596953803, Test Error: 0.22739260297569533\n",
      "Train Error: 0.08924692082953956, Test Error: 0.09521898458271644\n",
      "Train Error: 0.08922239440875433, Test Error: 0.09498215154201889\n",
      "Train Error: 0.08920813224825913, Test Error: 0.09489677088364473\n",
      "Train Error: 0.08919181432664887, Test Error: 0.09480965127856443\n",
      "Train Error: 0.08926537288260535, Test Error: 0.09514603152910212\n",
      "Train Error: 0.08918537061957706, Test Error: 0.09467576281972381\n",
      "Train Error: 0.08921088932994721, Test Error: 0.09446715032234258\n",
      "Train Error: 0.08967199965755715, Test Error: 0.09432722635843088\n",
      "Train Error: 0.0892190948700068, Test Error: 0.09498427749026489\n",
      "Train Error: 0.08995988145319166, Test Error: 0.09428830418392357\n",
      "Train Error: 0.0911097392553049, Test Error: 0.09411197652449721\n",
      "Train Error: 0.09233826448075781, Test Error: 0.09395412993766673\n",
      "Train Error: 0.08919541448804158, Test Error: 0.09487467768389542\n",
      "Train Error: 0.09142075092525777, Test Error: 0.09407646481792852\n",
      "Train Error: 0.09332035867613193, Test Error: 0.09385327945549309\n",
      "Train Error: 0.09530558530546747, Test Error: 0.09360911730319756\n",
      "Train Error: 0.08918425245576497, Test Error: 0.09477388061300057\n",
      "Train Error: 0.09300124858547704, Test Error: 0.09389116524387718\n",
      "Train Error: 0.0956525753323194, Test Error: 0.09356712748990648\n",
      "Train Error: 0.09844433132032815, Test Error: 0.0932221286091027\n",
      "Train Error: 0.08944205522982875, Test Error: 0.09439746743268965\n",
      "Train Error: 0.09996336929764331, Test Error: 0.09308836833821245\n",
      "Train Error: 0.10629184992857933, Test Error: 0.09267019001716985\n",
      "Train Error: 0.1127767149131087, Test Error: 0.0924856320452664\n",
      "Train Error: 0.09000934894573369, Test Error: 0.09431362987613857\n",
      "Train Error: 0.10386566553454168, Test Error: 0.09278711640654762\n",
      "Train Error: 0.11198687465444628, Test Error: 0.09250310569687849\n",
      "Train Error: 0.12014140631220858, Test Error: 0.09250966935742366\n",
      "Train Error: 0.07820374666276879, Test Error: 0.10872583596395843\n",
      "Train Error: 0.078522930704208, Test Error: 0.10877180199301281\n",
      "Train Error: 0.07867186542102948, Test Error: 0.10879734929515754\n",
      "Train Error: 0.07883857698747455, Test Error: 0.108835659973203\n",
      "Train Error: 0.07837187436535539, Test Error: 0.10877933899886243\n",
      "Train Error: 0.0791421610698427, Test Error: 0.10891546404235253\n",
      "Train Error: 0.07963281979784141, Test Error: 0.10931153874860391\n",
      "Train Error: 0.0801730128602889, Test Error: 0.1099165046825002\n",
      "Train Error: 0.0785463718357668, Test Error: 0.10879027852798122\n",
      "Train Error: 0.0804593943168012, Test Error: 0.11023391744829272\n",
      "Train Error: 0.08156794978691541, Test Error: 0.11145668816685939\n",
      "Train Error: 0.08278596962260774, Test Error: 0.1127430630984732\n",
      "Train Error: 0.07875150759434214, Test Error: 0.10883706791821539\n",
      "Train Error: 0.08188670809302735, Test Error: 0.111792990734289\n",
      "Train Error: 0.0837620683740715, Test Error: 0.11366872751696394\n",
      "Train Error: 0.08590370143381862, Test Error: 0.11541565671949158\n",
      "Train Error: 0.07896952283462702, Test Error: 0.10889420971163802\n",
      "Train Error: 0.08343755528262059, Test Error: 0.11340041360456247\n",
      "Train Error: 0.08629111519918282, Test Error: 0.11573017960198545\n",
      "Train Error: 0.08928696372595382, Test Error: 0.11825644443184793\n",
      "Train Error: 0.08000862037695103, Test Error: 0.10971855944157712\n",
      "Train Error: 0.0909070672516182, Test Error: 0.1196145991275431\n",
      "Train Error: 0.09739866506798288, Test Error: 0.12504932408362437\n",
      "Train Error: 0.10399266987225689, Test Error: 0.1305408750834606\n",
      "Train Error: 0.08058107078295375, Test Error: 0.11035383227035563\n",
      "Train Error: 0.09498984417691303, Test Error: 0.12302179919317931\n",
      "Train Error: 0.10319513987991474, Test Error: 0.1299023926946534\n",
      "Train Error: 0.1120130106184795, Test Error: 0.13704254855611833\n",
      "Train Error: 0.05550118603299054, Test Error: 0.05781320025421438\n",
      "Train Error: 0.05616212955924944, Test Error: 0.05769083293437641\n",
      "Train Error: 0.05650203152979356, Test Error: 0.057633167225607226\n",
      "Train Error: 0.05684159463848442, Test Error: 0.05757717023515269\n",
      "Train Error: 0.05576247896826724, Test Error: 0.05777752998204844\n",
      "Train Error: 0.05744512890205615, Test Error: 0.05751010466924504\n",
      "Train Error: 0.058199812630483884, Test Error: 0.057447356763943636\n",
      "Train Error: 0.058979448058258194, Test Error: 0.05739719071877974\n",
      "Train Error: 0.05619834063517499, Test Error: 0.05771176094249977\n",
      "Train Error: 0.059399374128128506, Test Error: 0.057389811859423746\n",
      "Train Error: 0.06104292985826014, Test Error: 0.05733630453255338\n",
      "Train Error: 0.06278798863824625, Test Error: 0.05734074986574915\n",
      "Train Error: 0.05664617742341749, Test Error: 0.05765331845324742\n",
      "Train Error: 0.061498216194146645, Test Error: 0.05734706579656457\n",
      "Train Error: 0.0642713506064183, Test Error: 0.05742135768711822\n",
      "Train Error: 0.06718253310819589, Test Error: 0.0576157471324794\n",
      "Train Error: 0.057105356238456875, Test Error: 0.057602229243773395\n",
      "Train Error: 0.063828906034099, Test Error: 0.05741441899623481\n",
      "Train Error: 0.0677055304119235, Test Error: 0.057674011285983456\n",
      "Train Error: 0.0717821267726028, Test Error: 0.058133606126749324\n",
      "Train Error: 0.05876041513875509, Test Error: 0.057522064309116425\n",
      "Train Error: 0.07399603921470209, Test Error: 0.05851509520594001\n",
      "Train Error: 0.08233168086785018, Test Error: 0.06012130963630167\n",
      "Train Error: 0.09080378261117622, Test Error: 0.06239832567587595\n",
      "Train Error: 0.05960653061024886, Test Error: 0.057508508171268236\n",
      "Train Error: 0.07926512289883444, Test Error: 0.05947230252942128\n",
      "Train Error: 0.0897988330496235, Test Error: 0.062110056776153645\n",
      "Train Error: 0.10058210963263413, Test Error: 0.06571124345578526\n",
      "Train Error: 0.047476258395671773, Test Error: 0.11286828269951227\n",
      "Train Error: 0.047734090146531544, Test Error: 0.11272199650762167\n",
      "Train Error: 0.047846131283504394, Test Error: 0.11274212588495502\n",
      "Train Error: 0.04796371298886503, Test Error: 0.11276773543674692\n",
      "Train Error: 0.047597333027491355, Test Error: 0.1128370105621118\n",
      "Train Error: 0.048187112175837016, Test Error: 0.11285191295533697\n",
      "Train Error: 0.048516550450079236, Test Error: 0.11296799898782754\n",
      "Train Error: 0.048872459598438465, Test Error: 0.1131164767762172\n",
      "Train Error: 0.047756419213746897, Test Error: 0.1127835826581009\n",
      "Train Error: 0.04907615124705678, Test Error: 0.11328194796567935\n",
      "Train Error: 0.04988484845456986, Test Error: 0.11371260933217803\n",
      "Train Error: 0.05077837229615811, Test Error: 0.11418931871894653\n",
      "Train Error: 0.04791219314559546, Test Error: 0.1128499656794553\n",
      "Train Error: 0.050118770923369374, Test Error: 0.1138709599681102\n",
      "Train Error: 0.05152150526275452, Test Error: 0.1146622671815694\n",
      "Train Error: 0.05311209275605458, Test Error: 0.11574950618268368\n",
      "Train Error: 0.04807646055833264, Test Error: 0.11292443050283622\n",
      "Train Error: 0.051292270756247975, Test Error: 0.11455052698086626\n",
      "Train Error: 0.05341462659276188, Test Error: 0.11598893952720465\n",
      "Train Error: 0.05585031937362972, Test Error: 0.11783813152637994\n",
      "Train Error: 0.048799333562102816, Test Error: 0.11331720213574602\n",
      "Train Error: 0.05724793086018279, Test Error: 0.11906154871545833\n",
      "Train Error: 0.06306911627793974, Test Error: 0.12412555859283896\n",
      "Train Error: 0.06963009005623068, Test Error: 0.13045330080010126\n",
      "Train Error: 0.049203554234104975, Test Error: 0.11359717528682774\n",
      "Train Error: 0.06083038939882178, Test Error: 0.1221488662899418\n",
      "Train Error: 0.06882308763406078, Test Error: 0.12968036702351235\n",
      "Train Error: 0.07769775859865771, Test Error: 0.13888425447885944\n",
      "Train Error: 0.06144695408508274, Test Error: 0.15278587827540902\n",
      "Train Error: 0.06169302751232452, Test Error: 0.15143478612936903\n",
      "Train Error: 0.06183013955984851, Test Error: 0.15092531581391724\n",
      "Train Error: 0.06196531080618138, Test Error: 0.1504787704834876\n",
      "Train Error: 0.061535187259543545, Test Error: 0.15218336971454027\n",
      "Train Error: 0.062225327392499405, Test Error: 0.14970138640023178\n",
      "Train Error: 0.06259740619688461, Test Error: 0.14862250629139795\n",
      "Train Error: 0.06299134771405028, Test Error: 0.14756920045987387\n",
      "Train Error: 0.06172232163178949, Test Error: 0.15140216043170124\n",
      "Train Error: 0.06321421408702299, Test Error: 0.1470390976839391\n",
      "Train Error: 0.06409312229226227, Test Error: 0.14512361071619798\n",
      "Train Error: 0.06505179236331245, Test Error: 0.14323112810597088\n",
      "Train Error: 0.06191193364538804, Test Error: 0.1507694426798599\n",
      "Train Error: 0.06434709601810842, Test Error: 0.1446464701153509\n",
      "Train Error: 0.06583581414937062, Test Error: 0.14180784637057278\n",
      "Train Error: 0.06743979109244526, Test Error: 0.13884230368239162\n",
      "Train Error: 0.06210168127240076, Test Error: 0.15019760797055867\n",
      "Train Error: 0.06560124083280108, Test Error: 0.14232119724760853\n",
      "Train Error: 0.06774135027737795, Test Error: 0.1383954190791781\n",
      "Train Error: 0.07013510976525504, Test Error: 0.1350723282264512\n",
      "Train Error: 0.06292302635710878, Test Error: 0.1479951934328072\n",
      "Train Error: 0.07150365250608834, Test Error: 0.13364412027059783\n",
      "Train Error: 0.07703493481162683, Test Error: 0.12995498993876808\n",
      "Train Error: 0.08334258005146128, Test Error: 0.12960302167860502\n",
      "Train Error: 0.06336771841765515, Test Error: 0.1469413704632025\n",
      "Train Error: 0.07492833337642153, Test Error: 0.13108056331456788\n",
      "Train Error: 0.08256522901678388, Test Error: 0.12959545294173616\n",
      "Train Error: 0.09101904295481358, Test Error: 0.13105120180687704\n",
      "Train Error: 0.07408621745921837, Test Error: 41.30986113935009\n",
      "Train Error: 0.07414342962743423, Test Error: 40.73284942339524\n",
      "Train Error: 0.07410326637695944, Test Error: 40.52758542135297\n",
      "Train Error: 0.0740889534770946, Test Error: 40.36433248764286\n",
      "Train Error: 0.07411052726859753, Test Error: 41.09021002201247\n",
      "Train Error: 0.07407824132345527, Test Error: 40.06713019279626\n",
      "Train Error: 0.07407926360363598, Test Error: 39.6553082681752\n",
      "Train Error: 0.0741140339541758, Test Error: 39.24875990990998\n",
      "Train Error: 0.0741621223671503, Test Error: 40.707911867102354\n",
      "Train Error: 0.07415289094662518, Test Error: 39.028904809308436\n",
      "Train Error: 0.0743398820116494, Test Error: 38.21810967210735\n",
      "Train Error: 0.07463568721109641, Test Error: 37.404616468294634\n",
      "Train Error: 0.0741247416351603, Test Error: 40.44895141058986\n",
      "Train Error: 0.07441455963284009, Test Error: 37.99736510212241\n",
      "Train Error: 0.07494421228066803, Test Error: 36.77938308888962\n",
      "Train Error: 0.07560314707676202, Test Error: 35.57109361059365\n",
      "Train Error: 0.07412260193090092, Test Error: 40.23281399974425\n",
      "Train Error: 0.07485368625718336, Test Error: 36.96825005189832\n",
      "Train Error: 0.0757574089053141, Test Error: 35.35657946417019\n",
      "Train Error: 0.07707895254024526, Test Error: 33.73736274233847\n",
      "Train Error: 0.07418097905398816, Test Error: 39.357931768909275\n",
      "Train Error: 0.07797233951789187, Test Error: 32.87602303631852\n",
      "Train Error: 0.08199229262392968, Test Error: 29.47695092700882\n",
      "Train Error: 0.08742785344293474, Test Error: 25.18261555007968\n",
      "Train Error: 0.07425580548649945, Test Error: 38.91811840963378\n",
      "Train Error: 0.08042472672022397, Test Error: 30.794622958727178\n",
      "Train Error: 0.08674577003475555, Test Error: 25.708488202597284\n",
      "Train Error: 0.09446448588425062, Test Error: 20.34022381090858\n",
      "Train Error: 9.203673454945788, Test Error: 0.5087378132361573\n",
      "Train Error: 9.1937732343907, Test Error: 0.5128285835978682\n",
      "Train Error: 9.189693141186083, Test Error: 0.5150438711617871\n",
      "Train Error: 9.185716221510555, Test Error: 0.5172295084255178\n",
      "Train Error: 9.198785504346217, Test Error: 0.5100834326995565\n",
      "Train Error: 9.178152145945859, Test Error: 0.5212705504830712\n",
      "Train Error: 9.168201752143409, Test Error: 0.5269960458638642\n",
      "Train Error: 9.183182346404584, Test Error: 0.5308967400700354\n",
      "Train Error: 9.192170139214973, Test Error: 0.513096364027692\n",
      "Train Error: 9.175292742182359, Test Error: 0.5341264205674338\n",
      "Train Error: 9.158713733353123, Test Error: 0.5459165032361829\n",
      "Train Error: 9.190049402388501, Test Error: 0.5598539061527318\n",
      "Train Error: 9.185897997791441, Test Error: 0.5162771788779078\n",
      "Train Error: 9.15148845837281, Test Error: 0.5491464744478091\n",
      "Train Error: 9.166457594452002, Test Error: 0.5694006800382786\n",
      "Train Error: 9.121198260499728, Test Error: 0.588476612517362\n",
      "Train Error: 9.17980521128302, Test Error: 0.5191874667466714\n",
      "Train Error: 9.172618744254581, Test Error: 0.566626677560635\n",
      "Train Error: 9.11023076102088, Test Error: 0.5922397348127629\n",
      "Train Error: 9.007033790849329, Test Error: 0.623423972570607\n",
      "Train Error: 9.155047065800515, Test Error: 0.5313925166967436\n",
      "Train Error: 8.96446327047696, Test Error: 0.638424431117931\n",
      "Train Error: 8.756751990237275, Test Error: 0.6913964048891668\n",
      "Train Error: 8.469382023631622, Test Error: 0.7365909627101782\n",
      "Train Error: 9.16549181865718, Test Error: 0.5358387704171552\n",
      "Train Error: 8.84705926761632, Test Error: 0.6738394425795919\n",
      "Train Error: 8.49340569195577, Test Error: 0.7318238369924798\n",
      "Train Error: 8.229887337388908, Test Error: 0.7847692475885\n",
      "Train Error: 6.586799253790387, Test Error: 0.13644214431724447\n",
      "Train Error: 6.5843605487874415, Test Error: 0.13635212965648422\n",
      "Train Error: 6.569499730886235, Test Error: 0.13649521913426926\n",
      "Train Error: 6.554043890403396, Test Error: 0.13663997060978503\n",
      "Train Error: 6.599177645233819, Test Error: 0.1360521364250687\n",
      "Train Error: 6.527529367259619, Test Error: 0.1367655009004414\n",
      "Train Error: 6.492517765923242, Test Error: 0.13775609792609364\n",
      "Train Error: 6.465023246614927, Test Error: 0.13808782515189455\n",
      "Train Error: 6.578185151886326, Test Error: 0.1360161418461696\n",
      "Train Error: 6.443684303409916, Test Error: 0.13803618085142805\n",
      "Train Error: 6.381871672143393, Test Error: 0.13869655827825514\n",
      "Train Error: 6.324508981649455, Test Error: 0.13928159722311406\n",
      "Train Error: 6.556229851347071, Test Error: 0.13598379973357333\n",
      "Train Error: 6.366640900217129, Test Error: 0.13862807384448997\n",
      "Train Error: 6.277353826945917, Test Error: 0.1395122939875399\n",
      "Train Error: 6.172612227895148, Test Error: 0.13990430760660683\n",
      "Train Error: 6.537995154585716, Test Error: 0.13594867219920911\n",
      "Train Error: 6.285934634979895, Test Error: 0.1391493519499828\n",
      "Train Error: 6.15413269873772, Test Error: 0.13986037127564258\n",
      "Train Error: 5.993544924830746, Test Error: 0.1412712930622899\n",
      "Train Error: 6.453927457549736, Test Error: 0.13642651771849446\n",
      "Train Error: 5.905495471400355, Test Error: 0.1410826059710308\n",
      "Train Error: 5.601992643463256, Test Error: 0.14405917130093787\n",
      "Train Error: 5.299464002358416, Test Error: 0.1471919764454735\n",
      "Train Error: 6.416265274082842, Test Error: 0.1363286229386339\n",
      "Train Error: 5.709156759595916, Test Error: 0.14245893047112607\n",
      "Train Error: 5.340116333815181, Test Error: 0.14631161726777075\n",
      "Train Error: 4.85703976796547, Test Error: 0.14819380016072012\n",
      "Train Error: 6.36064813732175, Test Error: 0.08044048861566469\n",
      "Train Error: 6.3352620998768145, Test Error: 0.08062216402468499\n",
      "Train Error: 6.309667993936976, Test Error: 0.08066562719539881\n",
      "Train Error: 6.282880859793356, Test Error: 0.08070756045691299\n",
      "Train Error: 6.333862156653302, Test Error: 0.08045621399161129\n",
      "Train Error: 6.2332361129383, Test Error: 0.08076044758091665\n",
      "Train Error: 6.097354192754408, Test Error: 0.08074282397098512\n",
      "Train Error: 6.040380325614803, Test Error: 0.08087116410909333\n",
      "Train Error: 6.321676402159497, Test Error: 0.08055452223617941\n",
      "Train Error: 6.005099666666839, Test Error: 0.08089699762325445\n",
      "Train Error: 5.918686440296595, Test Error: 0.08121200239223234\n",
      "Train Error: 5.8088185707110735, Test Error: 0.08150149883114802\n",
      "Train Error: 6.281481028389025, Test Error: 0.08056711994091874\n",
      "Train Error: 5.882401238350732, Test Error: 0.08123983353618691\n",
      "Train Error: 5.720903766446264, Test Error: 0.08168824892618845\n",
      "Train Error: 5.619370569493385, Test Error: 0.08227469232813645\n",
      "Train Error: 6.239532980865661, Test Error: 0.08057740139346677\n",
      "Train Error: 5.737144603887755, Test Error: 0.08156465389764948\n",
      "Train Error: 5.580479546313711, Test Error: 0.08230609479442937\n",
      "Train Error: 5.3436151012838495, Test Error: 0.08294096466163249\n",
      "Train Error: 6.015059237672635, Test Error: 0.08051998659101131\n",
      "Train Error: 5.16929607765539, Test Error: 0.08305410383573211\n",
      "Train Error: 4.690125315330243, Test Error: 0.08446854748404563\n",
      "Train Error: 4.219137333865969, Test Error: 0.08606028706412981\n",
      "Train Error: 5.943278463387428, Test Error: 0.08056903329162585\n",
      "Train Error: 4.853513864372297, Test Error: 0.08383185917904294\n",
      "Train Error: 4.26050744758881, Test Error: 0.08576011968926807\n",
      "Train Error: 3.6383191441611435, Test Error: 0.08782382475947242\n",
      "Train Error: 3.957933881089317, Test Error: 0.08863017966428598\n",
      "Train Error: 3.9619659033505754, Test Error: 0.08854059814860551\n",
      "Train Error: 3.935305568480409, Test Error: 0.08846606321553538\n",
      "Train Error: 3.908172933246112, Test Error: 0.08839269777853205\n",
      "Train Error: 3.992924851048259, Test Error: 0.08860458980501905\n",
      "Train Error: 3.854333631767356, Test Error: 0.08823791144185116\n",
      "Train Error: 3.7172348922144915, Test Error: 0.08802753252197913\n",
      "Train Error: 3.655700666814425, Test Error: 0.08785234925926712\n",
      "Train Error: 3.9550401863164657, Test Error: 0.0884599903926992\n",
      "Train Error: 3.619012835187729, Test Error: 0.08772058739551661\n",
      "Train Error: 3.5273643054980064, Test Error: 0.08736168193022517\n",
      "Train Error: 3.4081707201276954, Test Error: 0.08708414508499224\n",
      "Train Error: 3.9110065588547878, Test Error: 0.08832063079316892\n",
      "Train Error: 3.4905274620064044, Test Error: 0.08724325091663955\n",
      "Train Error: 3.3113710448994897, Test Error: 0.08684836186761355\n",
      "Train Error: 3.1892868666914493, Test Error: 0.08656816322521556\n",
      "Train Error: 3.8703937275779303, Test Error: 0.08818105931677236\n",
      "Train Error: 3.3358779012606115, Test Error: 0.08686369422217949\n",
      "Train Error: 3.15103838780077, Test Error: 0.08647563991673228\n",
      "Train Error: 2.8684892188640827, Test Error: 0.08618936519475494\n",
      "Train Error: 3.642656961099348, Test Error: 0.08760515912404611\n",
      "Train Error: 2.7208353295646894, Test Error: 0.08588820825387983\n",
      "Train Error: 2.233496497138842, Test Error: 0.08565512866934606\n",
      "Train Error: 1.6745822648909172, Test Error: 0.08588119618341808\n",
      "Train Error: 3.5696007088719712, Test Error: 0.0873451259524566\n",
      "Train Error: 2.402599016533176, Test Error: 0.0856087975840423\n",
      "Train Error: 1.7801092203048694, Test Error: 0.08581904790501178\n",
      "Train Error: 1.2097435893899584, Test Error: 0.08667214875815013\n",
      "Train Error: 0.08848596007427803, Test Error: 0.1280026470330866\n",
      "Train Error: 0.08864556203006009, Test Error: 0.128482693605255\n",
      "Train Error: 0.08864107659252371, Test Error: 0.12870883219926296\n",
      "Train Error: 0.08860717792287748, Test Error: 0.12891734502614366\n",
      "Train Error: 0.08858436864666756, Test Error: 0.1281947636335513\n",
      "Train Error: 0.08850186520228706, Test Error: 0.12934859576856614\n",
      "Train Error: 0.08813347621763026, Test Error: 0.12980001518133583\n",
      "Train Error: 0.08808575012232732, Test Error: 0.13033505131227088\n",
      "Train Error: 0.08870204479790968, Test Error: 0.12853006531691494\n",
      "Train Error: 0.0880942981770185, Test Error: 0.13063330153451455\n",
      "Train Error: 0.08803963141079417, Test Error: 0.13172906279097787\n",
      "Train Error: 0.08801639734461043, Test Error: 0.13284989395167843\n",
      "Train Error: 0.08873773196658558, Test Error: 0.12884604173516978\n",
      "Train Error: 0.08806496421604411, Test Error: 0.1320370029488817\n",
      "Train Error: 0.08808135258891371, Test Error: 0.13375127533619868\n",
      "Train Error: 0.08821479701798023, Test Error: 0.13554054364079032\n",
      "Train Error: 0.08868118576647112, Test Error: 0.12918691534725646\n",
      "Train Error: 0.08811339934529741, Test Error: 0.13349426743643758\n",
      "Train Error: 0.08830036089565349, Test Error: 0.13588520296382445\n",
      "Train Error: 0.08884271481500991, Test Error: 0.13859605917920936\n",
      "Train Error: 0.08837797306869256, Test Error: 0.13029237828375306\n",
      "Train Error: 0.08938605508661196, Test Error: 0.1402017270715183\n",
      "Train Error: 0.09195340422616054, Test Error: 0.14637292356652104\n",
      "Train Error: 0.09487306733196352, Test Error: 0.15280011969770907\n",
      "Train Error: 0.08843466206544184, Test Error: 0.130905725379147\n",
      "Train Error: 0.09076112432819741, Test Error: 0.14404616419060018\n",
      "Train Error: 0.09461235770540599, Test Error: 0.15205200866651877\n",
      "Train Error: 0.10094865028958872, Test Error: 0.16006035654909842\n",
      "Percentile: 0.0, Tracking Error: 1.517964627496765\n",
      "Starting percentile: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.548e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.457e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0003653651432320111, Test Error: 0.8039627508730395\n",
      "Train Error: 0.0016237809853191561, Test Error: 0.5070831131718748\n",
      "Train Error: 0.0017556270345344203, Test Error: 0.4527188603894994\n",
      "Train Error: 0.0021673464216280893, Test Error: 0.456029650460561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.732e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0009532322022536122, Test Error: 0.8616580134956678\n",
      "Train Error: 0.0030216909540117824, Test Error: 0.4521319871812885\n",
      "Train Error: 0.00421331647685697, Test Error: 0.45818498285931736\n",
      "Train Error: 0.005455287159038699, Test Error: 0.4621115373647015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.232e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0016206898187516824, Test Error: 0.441619246017919\n",
      "Train Error: 0.006028117476660296, Test Error: 0.4549281484201786\n",
      "Train Error: 0.008364048770933033, Test Error: 0.4512994386881538\n",
      "Train Error: 0.010137094479232073, Test Error: 0.4599868213037786\n",
      "Train Error: 0.0019301368891987743, Test Error: 0.42310549882596465\n",
      "Train Error: 0.008913477505674268, Test Error: 0.43716637059949986\n",
      "Train Error: 0.011359551863426328, Test Error: 0.4708844171796801\n",
      "Train Error: 0.013946820913461588, Test Error: 0.4929303298445456\n",
      "Train Error: 0.002551773714770114, Test Error: 0.42137132068569094\n",
      "Train Error: 0.010936960974016774, Test Error: 0.4647610592334258\n",
      "Train Error: 0.014338321367151394, Test Error: 0.49661889298422307\n",
      "Train Error: 0.01751130095539281, Test Error: 0.5247898255459543\n",
      "Train Error: 0.004969643677198006, Test Error: 0.41473253939844285\n",
      "Train Error: 0.018972929356754275, Test Error: 0.5332307911865221\n",
      "Train Error: 0.02497146014153128, Test Error: 0.5977343904846811\n",
      "Train Error: 0.030321848289578134, Test Error: 0.6344237931278728\n",
      "Train Error: 0.006081956161011585, Test Error: 0.418390203847192\n",
      "Train Error: 0.02258845124075943, Test Error: 0.56733432865548\n",
      "Train Error: 0.02956092140477998, Test Error: 0.6259455076303572\n",
      "Train Error: 0.033912754977078775, Test Error: 0.6195398597264707\n",
      "Train Error: 0.13233475042165366, Test Error: 0.34803833931905365\n",
      "Train Error: 0.1323725844209386, Test Error: 0.3472869532175425\n",
      "Train Error: 0.132392363953224, Test Error: 0.3468280521551455\n",
      "Train Error: 0.13241584564026973, Test Error: 0.34639390955494065\n",
      "Train Error: 0.1323433931605383, Test Error: 0.3476498315425859\n",
      "Train Error: 0.13246511938680583, Test Error: 0.3455781921720618\n",
      "Train Error: 0.1325345383321672, Test Error: 0.3443112774220283\n",
      "Train Error: 0.13251151669461297, Test Error: 0.343191500072252\n",
      "Train Error: 0.13237001884677368, Test Error: 0.34720340604860417\n",
      "Train Error: 0.13250392391715932, Test Error: 0.3426032099064161\n",
      "Train Error: 0.13251336609616993, Test Error: 0.34042580118330074\n",
      "Train Error: 0.132577790614631, Test Error: 0.3382867127334476\n",
      "Train Error: 0.13239383299137597, Test Error: 0.34657855473834\n",
      "Train Error: 0.132523478736963, Test Error: 0.33983305294088767\n",
      "Train Error: 0.13266136057088396, Test Error: 0.3366609045976705\n",
      "Train Error: 0.13311799840209332, Test Error: 0.3348321266558122\n",
      "Train Error: 0.13242344166578415, Test Error: 0.34598039500277733\n",
      "Train Error: 0.13262946759185745, Test Error: 0.337126053962984\n",
      "Train Error: 0.13320463027817275, Test Error: 0.3345098655653693\n",
      "Train Error: 0.1339845942972353, Test Error: 0.3323434480075061\n",
      "Train Error: 0.13251220927217258, Test Error: 0.3434078212409438\n",
      "Train Error: 0.13443204522643623, Test Error: 0.33086265036967566\n",
      "Train Error: 0.13665275407525282, Test Error: 0.32547457883503883\n",
      "Train Error: 0.13959487832406609, Test Error: 0.3195987605760027\n",
      "Train Error: 0.13249495708544073, Test Error: 0.3421945930067117\n",
      "Train Error: 0.13570531432912766, Test Error: 0.3276842778804978\n",
      "Train Error: 0.13919115329249934, Test Error: 0.3202280995884105\n",
      "Train Error: 0.14353992557173995, Test Error: 0.3137151648351417\n",
      "Train Error: 0.1484750592470136, Test Error: 0.12880248728245516\n",
      "Train Error: 0.1486407699251, Test Error: 0.12884956382985688\n",
      "Train Error: 0.14872819604341622, Test Error: 0.12893373501116434\n",
      "Train Error: 0.14880407863168052, Test Error: 0.1290411904051217\n",
      "Train Error: 0.14851572896953397, Test Error: 0.1288891099729084\n",
      "Train Error: 0.14885104470149182, Test Error: 0.1292814703962665\n",
      "Train Error: 0.14893045638690017, Test Error: 0.12962683132379987\n",
      "Train Error: 0.1490241941025783, Test Error: 0.1299419713324994\n",
      "Train Error: 0.1486243471859814, Test Error: 0.12889261618919357\n",
      "Train Error: 0.14906235105593776, Test Error: 0.1301634712220112\n",
      "Train Error: 0.14925844480279551, Test Error: 0.13117254741482637\n",
      "Train Error: 0.14948997546160211, Test Error: 0.13230035644041205\n",
      "Train Error: 0.14872479544035644, Test Error: 0.12901696448201\n",
      "Train Error: 0.14930091191858802, Test Error: 0.13147141975849722\n",
      "Train Error: 0.1496757020168756, Test Error: 0.1332344837619703\n",
      "Train Error: 0.1501489380499826, Test Error: 0.1351500663005532\n",
      "Train Error: 0.14877831127092683, Test Error: 0.12919453869478495\n",
      "Train Error: 0.14959429056512635, Test Error: 0.13295840426667538\n",
      "Train Error: 0.15022509674163007, Test Error: 0.1355138046336936\n",
      "Train Error: 0.15081542818613033, Test Error: 0.13858780958406544\n",
      "Train Error: 0.14889570634699933, Test Error: 0.12999793826778874\n",
      "Train Error: 0.15111073459628188, Test Error: 0.1402625022307585\n",
      "Train Error: 0.15275290509006625, Test Error: 0.146815121463035\n",
      "Train Error: 0.1547796864371249, Test Error: 0.15372072863751474\n",
      "Train Error: 0.14897388691189917, Test Error: 0.13036364263235067\n",
      "Train Error: 0.15209339996789856, Test Error: 0.14436041522315918\n",
      "Train Error: 0.15448157563710205, Test Error: 0.15287912256499736\n",
      "Train Error: 0.1574727302409511, Test Error: 0.16143677240276938\n",
      "Train Error: 0.1284645494037662, Test Error: 0.17234386262534196\n",
      "Train Error: 0.12857222442926913, Test Error: 0.17183640070222317\n",
      "Train Error: 0.12863996452160797, Test Error: 0.17166461876905295\n",
      "Train Error: 0.1287172593368985, Test Error: 0.17138074294670952\n",
      "Train Error: 0.1284928177683993, Test Error: 0.17207056354066613\n",
      "Train Error: 0.12886097760883683, Test Error: 0.17092034718935614\n",
      "Train Error: 0.1290921568935488, Test Error: 0.17031366312764945\n",
      "Train Error: 0.1292093744874411, Test Error: 0.17152706815794627\n",
      "Train Error: 0.12857242471486588, Test Error: 0.17179642455766342\n",
      "Train Error: 0.129266587242699, Test Error: 0.17224879973089283\n",
      "Train Error: 0.12953416512911675, Test Error: 0.17514853639768954\n",
      "Train Error: 0.12986100314802188, Test Error: 0.1780789022940674\n",
      "Train Error: 0.12866197801406765, Test Error: 0.17151471475894683\n",
      "Train Error: 0.12960896974321526, Test Error: 0.17589111977066202\n",
      "Train Error: 0.13011879868083273, Test Error: 0.180306424732496\n",
      "Train Error: 0.13068153632519589, Test Error: 0.18482057879777966\n",
      "Train Error: 0.1287586630501129, Test Error: 0.17118043660134932\n",
      "Train Error: 0.130037148113996, Test Error: 0.17956853002494572\n",
      "Train Error: 0.13078649726132024, Test Error: 0.18558117363819315\n",
      "Train Error: 0.13173468044429665, Test Error: 0.19162782732314387\n",
      "Train Error: 0.1291506898046649, Test Error: 0.1708649851078124\n",
      "Train Error: 0.1322787343650309, Test Error: 0.19466289229878248\n",
      "Train Error: 0.13488342364969963, Test Error: 0.20662751137145102\n",
      "Train Error: 0.1380959947610601, Test Error: 0.21784677526521715\n",
      "Train Error: 0.1292603965864568, Test Error: 0.17232435723272443\n",
      "Train Error: 0.13382782311411698, Test Error: 0.20216447148390657\n",
      "Train Error: 0.13768584209624762, Test Error: 0.21652471774109458\n",
      "Train Error: 0.14225219042213033, Test Error: 0.22997061606551608\n",
      "Train Error: 0.09485129422034164, Test Error: 0.09422383123647145\n",
      "Train Error: 0.09468163610171458, Test Error: 0.09398309818065995\n",
      "Train Error: 0.09461818882482927, Test Error: 0.09388271346896655\n",
      "Train Error: 0.09455235046451493, Test Error: 0.09378388662333063\n",
      "Train Error: 0.09480362957082968, Test Error: 0.09413223873926359\n",
      "Train Error: 0.09448811086253431, Test Error: 0.09347830554833365\n",
      "Train Error: 0.0944214995459442, Test Error: 0.09306124584515982\n",
      "Train Error: 0.09440087386419489, Test Error: 0.09267467067557258\n",
      "Train Error: 0.09468244142298932, Test Error: 0.0939818215464325\n",
      "Train Error: 0.09464992593172936, Test Error: 0.092621491098697\n",
      "Train Error: 0.09565454213093946, Test Error: 0.0924126084858547\n",
      "Train Error: 0.09671742372158112, Test Error: 0.09221509543273648\n",
      "Train Error: 0.09459901311381425, Test Error: 0.09385423568093179\n",
      "Train Error: 0.095920751096731, Test Error: 0.09236249490628591\n",
      "Train Error: 0.09749603323449905, Test Error: 0.09206289736387321\n",
      "Train Error: 0.09926614917950151, Test Error: 0.09180795934191645\n",
      "Train Error: 0.09453988515097302, Test Error: 0.09369123465529051\n",
      "Train Error: 0.09722565362535207, Test Error: 0.0921109853174727\n",
      "Train Error: 0.09958906183909472, Test Error: 0.0917712354497965\n",
      "Train Error: 0.10214971232071945, Test Error: 0.09148848777242864\n",
      "Train Error: 0.09445031584185476, Test Error: 0.09286703314658273\n",
      "Train Error: 0.10357342036819447, Test Error: 0.09137874512076703\n",
      "Train Error: 0.10944976522036337, Test Error: 0.09104712118611329\n",
      "Train Error: 0.115852781939274, Test Error: 0.09098401457295886\n",
      "Train Error: 0.09473770424830222, Test Error: 0.09264146552565952\n",
      "Train Error: 0.10719566457029413, Test Error: 0.09113498904616506\n",
      "Train Error: 0.11506822113999958, Test Error: 0.0909930133998398\n",
      "Train Error: 0.12324443783531534, Test Error: 0.09118263168320771\n",
      "Train Error: 0.08104791063569698, Test Error: 0.12100401442781641\n",
      "Train Error: 0.08142617411081089, Test Error: 0.12090869855286232\n",
      "Train Error: 0.08160970517365844, Test Error: 0.12087302143274221\n",
      "Train Error: 0.0817967636225488, Test Error: 0.12083957104497034\n",
      "Train Error: 0.08119593033662932, Test Error: 0.12095226584532792\n",
      "Train Error: 0.08216445554120885, Test Error: 0.12106523578158244\n",
      "Train Error: 0.08271917142715961, Test Error: 0.1214815679306376\n",
      "Train Error: 0.08328756438166655, Test Error: 0.1219050140525784\n",
      "Train Error: 0.08145216120463726, Test Error: 0.12093094992876992\n",
      "Train Error: 0.08357075385541522, Test Error: 0.12212814968160862\n",
      "Train Error: 0.08479041138319793, Test Error: 0.12301971939531806\n",
      "Train Error: 0.08608463282881651, Test Error: 0.12396401627612785\n",
      "Train Error: 0.0817020498502183, Test Error: 0.12090352842707813\n",
      "Train Error: 0.08512272509906277, Test Error: 0.1232639273562069\n",
      "Train Error: 0.08708339643417487, Test Error: 0.12467837679711735\n",
      "Train Error: 0.08912916745253682, Test Error: 0.12611826184518105\n",
      "Train Error: 0.08194326617520965, Test Error: 0.12091196529352372\n",
      "Train Error: 0.0867718334962984, Test Error: 0.12446295393027294\n",
      "Train Error: 0.08951478620657105, Test Error: 0.12639890891059222\n",
      "Train Error: 0.09247629909459214, Test Error: 0.1284897912069593\n",
      "Train Error: 0.0831170571598216, Test Error: 0.12181229667016091\n",
      "Train Error: 0.09404138875497221, Test Error: 0.1296915453705687\n",
      "Train Error: 0.10020725916108525, Test Error: 0.1344226155116391\n",
      "Train Error: 0.10678275236954678, Test Error: 0.13942776969820703\n",
      "Train Error: 0.08370895423152244, Test Error: 0.12226938549856778\n",
      "Train Error: 0.09788518767674967, Test Error: 0.13265341620552445\n",
      "Train Error: 0.10599654657959354, Test Error: 0.13883563527743148\n",
      "Train Error: 0.11474614666977109, Test Error: 0.14533606274368222\n",
      "Train Error: 0.06092532407843912, Test Error: 0.05575222257775697\n",
      "Train Error: 0.06143880341683636, Test Error: 0.05566698893248492\n",
      "Train Error: 0.06170468472348107, Test Error: 0.05562844662998109\n",
      "Train Error: 0.06197016856652636, Test Error: 0.05559099045765987\n",
      "Train Error: 0.06112922585825542, Test Error: 0.055726707980578556\n",
      "Train Error: 0.062460979425713534, Test Error: 0.055538972127120355\n",
      "Train Error: 0.06321904990639557, Test Error: 0.05546769848917107\n",
      "Train Error: 0.06393638214773252, Test Error: 0.05540580048708888\n",
      "Train Error: 0.061478652836758546, Test Error: 0.05568786731253411\n",
      "Train Error: 0.0643222403558713, Test Error: 0.055393014444343726\n",
      "Train Error: 0.0658222988164256, Test Error: 0.05532149646743977\n",
      "Train Error: 0.0673874755944781, Test Error: 0.05530180308421578\n",
      "Train Error: 0.061829944463459074, Test Error: 0.055651452155147024\n",
      "Train Error: 0.06624029447116979, Test Error: 0.055324807741265865\n",
      "Train Error: 0.06863199635874456, Test Error: 0.055336478817682874\n",
      "Train Error: 0.07126240976558883, Test Error: 0.055494019159396245\n",
      "Train Error: 0.06219159690911067, Test Error: 0.055619416776580395\n",
      "Train Error: 0.06824991112678669, Test Error: 0.05533851524840698\n",
      "Train Error: 0.0717444725762494, Test Error: 0.055544932545187324\n",
      "Train Error: 0.07543670006184994, Test Error: 0.055964011122080465\n",
      "Train Error: 0.06374945060036999, Test Error: 0.05552666392809527\n",
      "Train Error: 0.07745892273745114, Test Error: 0.05630670288500102\n",
      "Train Error: 0.08537811951367252, Test Error: 0.057937846149495355\n",
      "Train Error: 0.09363317342157737, Test Error: 0.060296600742029804\n",
      "Train Error: 0.06453088036690059, Test Error: 0.055498871468405764\n",
      "Train Error: 0.08242409487552889, Test Error: 0.057264052494172824\n",
      "Train Error: 0.09265466126227376, Test Error: 0.05999302086422746\n",
      "Train Error: 0.10317164227214344, Test Error: 0.06371052326434465\n",
      "Train Error: 0.0642387476624306, Test Error: 0.1481640671033595\n",
      "Train Error: 0.0642786441026499, Test Error: 0.14830476739394724\n",
      "Train Error: 0.06430299957327715, Test Error: 0.14840683443913658\n",
      "Train Error: 0.06432797147178694, Test Error: 0.14851290316772506\n",
      "Train Error: 0.06425797052369357, Test Error: 0.14820379919947696\n",
      "Train Error: 0.06438446836727976, Test Error: 0.148729805859936\n",
      "Train Error: 0.06447886934574096, Test Error: 0.14902164426938094\n",
      "Train Error: 0.06459409195858062, Test Error: 0.1493334538723291\n",
      "Train Error: 0.06428723877587991, Test Error: 0.14836717009357445\n",
      "Train Error: 0.06466820139805592, Test Error: 0.14953427182548495\n",
      "Train Error: 0.06498479187356658, Test Error: 0.15023049290819762\n",
      "Train Error: 0.06538261747922224, Test Error: 0.15100021999684501\n",
      "Train Error: 0.06432168837102759, Test Error: 0.1485350135379338\n",
      "Train Error: 0.06508322924622936, Test Error: 0.15045775222392685\n",
      "Train Error: 0.0657426757680706, Test Error: 0.15166493361569766\n",
      "Train Error: 0.0665787905654322, Test Error: 0.1530304636977441\n",
      "Train Error: 0.0643614643666406, Test Error: 0.14870907803746772\n",
      "Train Error: 0.0656295737933065, Test Error: 0.1514922484221082\n",
      "Train Error: 0.06674658439829416, Test Error: 0.15331054153606574\n",
      "Train Error: 0.06816230817808236, Test Error: 0.15539705396439388\n",
      "Train Error: 0.06458408951179834, Test Error: 0.14944779235616004\n",
      "Train Error: 0.06902730345048375, Test Error: 0.1567293085017233\n",
      "Train Error: 0.07304427853593622, Test Error: 0.16175435523425205\n",
      "Train Error: 0.07800580092071865, Test Error: 0.1674655321768471\n",
      "Train Error: 0.0647284697039597, Test Error: 0.14984682119652265\n",
      "Train Error: 0.07143918908251765, Test Error: 0.1598550368052534\n",
      "Train Error: 0.07737567512632533, Test Error: 0.16681616701042307\n",
      "Train Error: 0.08449786217909076, Test Error: 0.17444469720168265\n",
      "Train Error: 0.08653378306824672, Test Error: 0.19231038739392736\n",
      "Train Error: 0.08648940327365476, Test Error: 0.1894104156807045\n",
      "Train Error: 0.08652517619643009, Test Error: 0.18803630041397068\n",
      "Train Error: 0.08662699617416429, Test Error: 0.1870089620925172\n",
      "Train Error: 0.08650189523261902, Test Error: 0.19125449185801036\n",
      "Train Error: 0.08685695758854343, Test Error: 0.1854764845108284\n",
      "Train Error: 0.08715258492455898, Test Error: 0.18371743717465394\n",
      "Train Error: 0.08739209941508873, Test Error: 0.18277422334744764\n",
      "Train Error: 0.0865074900971166, Test Error: 0.18940371308238801\n",
      "Train Error: 0.08752635800839838, Test Error: 0.18228483820750171\n",
      "Train Error: 0.0880368063712263, Test Error: 0.1802994177883394\n",
      "Train Error: 0.0886083660429098, Test Error: 0.1785366749937748\n",
      "Train Error: 0.08657607724260155, Test Error: 0.18773896418056435\n",
      "Train Error: 0.08818563528228149, Test Error: 0.17986671123743853\n",
      "Train Error: 0.08908718692468512, Test Error: 0.1772764421675345\n",
      "Train Error: 0.09008916759858317, Test Error: 0.17479107691427076\n",
      "Train Error: 0.08673711792439415, Test Error: 0.1865864325055408\n",
      "Train Error: 0.08894353102031233, Test Error: 0.17770772732107745\n",
      "Train Error: 0.09027709598696619, Test Error: 0.17438447859406564\n",
      "Train Error: 0.09177419755828899, Test Error: 0.17125097849802898\n",
      "Train Error: 0.08735239614460052, Test Error: 0.18333686868405194\n",
      "Train Error: 0.09264587231151991, Test Error: 0.1699259587965384\n",
      "Train Error: 0.09630431908588212, Test Error: 0.16517486709253298\n",
      "Train Error: 0.1005395463502646, Test Error: 0.16148211548082034\n",
      "Train Error: 0.08762703485075214, Test Error: 0.18242915826120706\n",
      "Train Error: 0.09490109467265372, Test Error: 0.16681853935929677\n",
      "Train Error: 0.10001515712217977, Test Error: 0.1619167194166218\n",
      "Train Error: 0.10600122691766933, Test Error: 0.1585417789193576\n",
      "Train Error: 0.1012142762903458, Test Error: 50.2764236539523\n",
      "Train Error: 0.10087243826135184, Test Error: 49.6171613125675\n",
      "Train Error: 0.10071735836048934, Test Error: 49.317892759041605\n",
      "Train Error: 0.10061581857669231, Test Error: 49.1004279841664\n",
      "Train Error: 0.10106316308978701, Test Error: 50.020969711919946\n",
      "Train Error: 0.10044688343305126, Test Error: 48.73528839567407\n",
      "Train Error: 0.10021962867598051, Test Error: 48.22053258619741\n",
      "Train Error: 0.10001323103340301, Test Error: 47.71187209586154\n",
      "Train Error: 0.10087957482766079, Test Error: 49.58655815636182\n",
      "Train Error: 0.09992522848522012, Test Error: 47.44322689998891\n",
      "Train Error: 0.09959766111079585, Test Error: 46.42576321799971\n",
      "Train Error: 0.09938415940762133, Test Error: 45.406186333123\n",
      "Train Error: 0.10070737585952474, Test Error: 49.21433339776834\n",
      "Train Error: 0.09954347422910956, Test Error: 46.15634005842204\n",
      "Train Error: 0.09928667392828047, Test Error: 44.62310531466808\n",
      "Train Error: 0.09922266725481804, Test Error: 43.098782606841546\n",
      "Train Error: 0.10058703157426607, Test Error: 48.93307545514838\n",
      "Train Error: 0.09932312204434561, Test Error: 44.86030957243564\n",
      "Train Error: 0.09923975287793789, Test Error: 42.82961137234329\n",
      "Train Error: 0.09949234431986202, Test Error: 40.73136280139299\n",
      "Train Error: 0.1001615639788112, Test Error: 47.84237074396038\n",
      "Train Error: 0.09978714453844807, Test Error: 39.57685078689029\n",
      "Train Error: 0.10114519644146111, Test Error: 34.93217756134889\n",
      "Train Error: 0.10348272804344746, Test Error: 30.14137317616878\n",
      "Train Error: 0.09998297853798181, Test Error: 47.299230583667914\n",
      "Train Error: 0.10056397081687285, Test Error: 36.735762127781854\n",
      "Train Error: 0.10316948250503477, Test Error: 30.71320800116737\n",
      "Train Error: 0.10731564911324712, Test Error: 24.858990841671385\n",
      "Train Error: 8.633234987407075, Test Error: 0.47173125826312595\n",
      "Train Error: 8.652449549832346, Test Error: 0.47650571607889924\n",
      "Train Error: 8.652461622638677, Test Error: 0.48001277080774374\n",
      "Train Error: 8.652951220511975, Test Error: 0.48354456258358325\n",
      "Train Error: 8.62980671893508, Test Error: 0.473523168754946\n",
      "Train Error: 8.643251179077565, Test Error: 0.48998807479285916\n",
      "Train Error: 8.635520430016802, Test Error: 0.49451166737263447\n",
      "Train Error: 8.694281048027003, Test Error: 0.5001041239391141\n",
      "Train Error: 8.650483470575185, Test Error: 0.4768636708024264\n",
      "Train Error: 8.683903676843594, Test Error: 0.5028630216768698\n",
      "Train Error: 8.648651703871101, Test Error: 0.5126021080164797\n",
      "Train Error: 8.613356760325658, Test Error: 0.522426670841616\n",
      "Train Error: 8.648919883121874, Test Error: 0.48149762568969645\n",
      "Train Error: 8.63730642918605, Test Error: 0.5152711354268521\n",
      "Train Error: 8.558721664147257, Test Error: 0.531866293154607\n",
      "Train Error: 8.501296758388309, Test Error: 0.5468022627207972\n",
      "Train Error: 8.643195781024108, Test Error: 0.4862781975776475\n",
      "Train Error: 8.564492040740902, Test Error: 0.5297174078657\n",
      "Train Error: 8.488268528628543, Test Error: 0.5496131576621314\n",
      "Train Error: 8.349105599074012, Test Error: 0.5688311916892352\n",
      "Train Error: 8.613002796158595, Test Error: 0.49854620776603525\n",
      "Train Error: 8.370800929131626, Test Error: 0.5818889586532139\n",
      "Train Error: 8.119078633770657, Test Error: 0.6292311309423472\n",
      "Train Error: 7.929206190828963, Test Error: 0.679658514911205\n",
      "Train Error: 8.6640162648906, Test Error: 0.5046666502005727\n",
      "Train Error: 8.184369985777293, Test Error: 0.6113601670877807\n",
      "Train Error: 7.947825195443871, Test Error: 0.6735371456235552\n",
      "Train Error: 7.705682430167364, Test Error: 0.737846848251229\n",
      "Train Error: 5.715221454115823, Test Error: 0.1658121462001726\n",
      "Train Error: 5.678517370869316, Test Error: 0.16480328722054488\n",
      "Train Error: 5.650122255856874, Test Error: 0.16453044106845582\n",
      "Train Error: 5.632453716406733, Test Error: 0.16447128253528953\n",
      "Train Error: 5.693874195734297, Test Error: 0.16529100576295733\n",
      "Train Error: 5.597853644807744, Test Error: 0.1642430273326879\n",
      "Train Error: 5.590440796385892, Test Error: 0.16401952854262655\n",
      "Train Error: 5.546026832477536, Test Error: 0.16387841079240706\n",
      "Train Error: 5.657157202178794, Test Error: 0.16451587075714325\n",
      "Train Error: 5.51837849486113, Test Error: 0.16359814160965955\n",
      "Train Error: 5.432686808377405, Test Error: 0.16331987643400434\n",
      "Train Error: 5.344332905012883, Test Error: 0.1630577805646987\n",
      "Train Error: 5.634772579441568, Test Error: 0.16396431634015216\n",
      "Train Error: 5.405660966355749, Test Error: 0.16304165588921013\n",
      "Train Error: 5.275258569640404, Test Error: 0.1626519673972412\n",
      "Train Error: 5.15507501056891, Test Error: 0.16226933735059496\n",
      "Train Error: 5.607883963344435, Test Error: 0.16368010680038053\n",
      "Train Error: 5.290777358543173, Test Error: 0.16250610369127844\n",
      "Train Error: 5.127377514900616, Test Error: 0.16200094152469857\n",
      "Train Error: 4.900641245735274, Test Error: 0.1616684712156803\n",
      "Train Error: 5.53476324875842, Test Error: 0.16247662008127547\n",
      "Train Error: 4.790697498056348, Test Error: 0.16062262717978293\n",
      "Train Error: 4.4117310844862105, Test Error: 0.1599621943068991\n",
      "Train Error: 4.033605749674177, Test Error: 0.15947608933568433\n",
      "Train Error: 5.483092360642995, Test Error: 0.16191458240273013\n",
      "Train Error: 4.543951616458389, Test Error: 0.15976859001371932\n",
      "Train Error: 4.072407657017815, Test Error: 0.1591016805004211\n",
      "Train Error: 3.5702178506610585, Test Error: 0.15883012324290105\n",
      "Train Error: 4.010054576849404, Test Error: 0.08450928619309642\n",
      "Train Error: 3.948703004704611, Test Error: 0.08454191077331237\n",
      "Train Error: 3.930514932196633, Test Error: 0.08457521427928182\n",
      "Train Error: 3.9146474430487324, Test Error: 0.08461181358737233\n",
      "Train Error: 3.9654411007309234, Test Error: 0.08447478502626685\n",
      "Train Error: 3.8885890239089886, Test Error: 0.08466453711980942\n",
      "Train Error: 3.8491940843315993, Test Error: 0.08476019136211185\n",
      "Train Error: 3.81156191208906, Test Error: 0.08486069301815616\n",
      "Train Error: 3.934540303224229, Test Error: 0.08447481228046687\n",
      "Train Error: 3.7816896941502236, Test Error: 0.08486662531190348\n",
      "Train Error: 3.702992537913389, Test Error: 0.0850722842779097\n",
      "Train Error: 3.690676912265057, Test Error: 0.0853773771007654\n",
      "Train Error: 3.905925256795609, Test Error: 0.08447835495402838\n",
      "Train Error: 3.673662055769844, Test Error: 0.0850818649892441\n",
      "Train Error: 3.6146582015869244, Test Error: 0.08549123551289534\n",
      "Train Error: 3.4701719131081563, Test Error: 0.0857968799509\n",
      "Train Error: 3.8851070809054704, Test Error: 0.08449663027047574\n",
      "Train Error: 3.6294126311617534, Test Error: 0.08539818456187001\n",
      "Train Error: 3.4370948075360257, Test Error: 0.08580795535627146\n",
      "Train Error: 3.2522528800763983, Test Error: 0.08626883031067177\n",
      "Train Error: 3.771282496568134, Test Error: 0.08452147969798246\n",
      "Train Error: 3.12831014457133, Test Error: 0.0863474597006888\n",
      "Train Error: 2.6991403874193973, Test Error: 0.08731117034958624\n",
      "Train Error: 2.357403243038936, Test Error: 0.08853753489463304\n",
      "Train Error: 3.715065332441863, Test Error: 0.08453863879810161\n",
      "Train Error: 2.886074173552833, Test Error: 0.08691553729766634\n",
      "Train Error: 2.3821133169379114, Test Error: 0.08828746144076241\n",
      "Train Error: 1.9652156626927004, Test Error: 0.09000163730956445\n",
      "Train Error: 1.7690433826526102, Test Error: 0.08765317575624805\n",
      "Train Error: 1.6718697732197, Test Error: 0.08760016816913986\n",
      "Train Error: 1.6241653089448411, Test Error: 0.08757487581575514\n",
      "Train Error: 1.572762172684128, Test Error: 0.08754286416266653\n",
      "Train Error: 1.728569682048763, Test Error: 0.08760049806765514\n",
      "Train Error: 1.5090348622631824, Test Error: 0.0872918016297548\n",
      "Train Error: 1.5178153318769605, Test Error: 0.0870379117963841\n",
      "Train Error: 1.4935650206103421, Test Error: 0.08681408214066931\n",
      "Train Error: 1.663155333026141, Test Error: 0.08751540476714213\n",
      "Train Error: 1.4746203293831597, Test Error: 0.0866492727163578\n",
      "Train Error: 1.4239307782817203, Test Error: 0.08622535150669174\n",
      "Train Error: 1.3737811178079407, Test Error: 0.08582779952934536\n",
      "Train Error: 1.5993332517104601, Test Error: 0.08743214646825666\n",
      "Train Error: 1.4070416923685989, Test Error: 0.08607120340376827\n",
      "Train Error: 1.3354355509942668, Test Error: 0.08550099996164898\n",
      "Train Error: 1.2841516829706603, Test Error: 0.0850725130411548\n",
      "Train Error: 1.541816337742731, Test Error: 0.08733500807566419\n",
      "Train Error: 1.3419478515441692, Test Error: 0.0855379881997527\n",
      "Train Error: 1.2377495894457666, Test Error: 0.08496025664539385\n",
      "Train Error: 1.1413438222676617, Test Error: 0.08436482509143198\n",
      "Train Error: 1.4781694591927537, Test Error: 0.08656275297992272\n",
      "Train Error: 1.0916451413406536, Test Error: 0.08392721317555975\n",
      "Train Error: 0.9740553928386426, Test Error: 0.08331831965204312\n",
      "Train Error: 0.961256644590739, Test Error: 0.0832374658687934\n",
      "Train Error: 1.4446988250843245, Test Error: 0.08624031630859771\n",
      "Train Error: 1.0084247077638655, Test Error: 0.08338030255126827\n",
      "Train Error: 0.9573227960746504, Test Error: 0.08313000508877573\n",
      "Train Error: 1.0992718007102105, Test Error: 0.08361139639356636\n",
      "Train Error: 0.08529487041578791, Test Error: 0.1199106871372157\n",
      "Train Error: 0.08534231021753487, Test Error: 0.1204981271200576\n",
      "Train Error: 0.0853708380360512, Test Error: 0.12073792767762076\n",
      "Train Error: 0.08542039552917874, Test Error: 0.12099666016332243\n",
      "Train Error: 0.08535474545803043, Test Error: 0.12024466431708983\n",
      "Train Error: 0.08550806698620715, Test Error: 0.12145036152166105\n",
      "Train Error: 0.0856055171877826, Test Error: 0.12206127877155308\n",
      "Train Error: 0.08573652372609186, Test Error: 0.12269610396157156\n",
      "Train Error: 0.08538706243932763, Test Error: 0.12055011391036348\n",
      "Train Error: 0.08584860368624966, Test Error: 0.12305774076435991\n",
      "Train Error: 0.08617368773810402, Test Error: 0.12435176793138882\n",
      "Train Error: 0.0865415782817045, Test Error: 0.1256527530268965\n",
      "Train Error: 0.08545709757444393, Test Error: 0.12088732440880995\n",
      "Train Error: 0.08628828256746766, Test Error: 0.1247036122752423\n",
      "Train Error: 0.08691915964293913, Test Error: 0.12670568139687444\n",
      "Train Error: 0.08785207920675862, Test Error: 0.12877987530380297\n",
      "Train Error: 0.08553034527516587, Test Error: 0.1212253517663784\n",
      "Train Error: 0.08683516150484656, Test Error: 0.1263961122365799\n",
      "Train Error: 0.08803746019732664, Test Error: 0.12917082553686574\n",
      "Train Error: 0.08942517864880006, Test Error: 0.13211601682733687\n",
      "Train Error: 0.0858761994703067, Test Error: 0.12260046916407324\n",
      "Train Error: 0.09038013834960636, Test Error: 0.13377058815552348\n",
      "Train Error: 0.09395363607172941, Test Error: 0.1399634951738926\n",
      "Train Error: 0.09835069720639242, Test Error: 0.14648339895434115\n",
      "Train Error: 0.08608481621995112, Test Error: 0.12330572149417711\n",
      "Train Error: 0.0926077477357903, Test Error: 0.13767619191379574\n",
      "Train Error: 0.09786861652989812, Test Error: 0.14573158941105693\n",
      "Train Error: 0.10481554662807617, Test Error: 0.15388499628342314\n",
      "Percentile: 0.1, Tracking Error: 1.8252308526532148\n",
      "Starting percentile: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.429e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.341e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.00041346983987696146, Test Error: 0.7977708132198701\n",
      "Train Error: 0.002739026970199781, Test Error: 0.5616611835818855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.891e-04, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.443e-04, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0034269027835705478, Test Error: 0.5309935456533883\n",
      "Train Error: 0.004330702914474214, Test Error: 0.5076169634555295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.745e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0016329100106950059, Test Error: 0.6290898365845551\n",
      "Train Error: 0.006039347402018365, Test Error: 0.48676249527354576\n",
      "Train Error: 0.00830232117401214, Test Error: 0.4697627229130241\n",
      "Train Error: 0.010399142459291836, Test Error: 0.4734539808334442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.422e-04, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0026930405735532976, Test Error: 0.5545453781353206\n",
      "Train Error: 0.011293628762342063, Test Error: 0.4797691316461362\n",
      "Train Error: 0.014307101844346626, Test Error: 0.5085313500226747\n",
      "Train Error: 0.01737557110829258, Test Error: 0.5194998141531895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.651e-04, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.003778105652844956, Test Error: 0.5245382173937415\n",
      "Train Error: 0.015025229667812195, Test Error: 0.5144417161969337\n",
      "Train Error: 0.019383968573994156, Test Error: 0.5210704891151459\n",
      "Train Error: 0.02175193244167013, Test Error: 0.5273522530578774\n",
      "Train Error: 0.005083350923240935, Test Error: 0.5093773275838553\n",
      "Train Error: 0.018773298512317925, Test Error: 0.5196762606207002\n",
      "Train Error: 0.02206124800908704, Test Error: 0.5285151954622486\n",
      "Train Error: 0.024608453507409197, Test Error: 0.5408163555569293\n",
      "Train Error: 0.009731292480241158, Test Error: 0.5053083572188263\n",
      "Train Error: 0.02566207891000653, Test Error: 0.5444987095176134\n",
      "Train Error: 0.029988932885688355, Test Error: 0.557562995303679\n",
      "Train Error: 0.03237270893494754, Test Error: 0.5636267603302942\n",
      "Train Error: 0.011623144474296373, Test Error: 0.501413855436113\n",
      "Train Error: 0.028584197781804, Test Error: 0.552980411309314\n",
      "Train Error: 0.03212243902441994, Test Error: 0.5623920174507406\n",
      "Train Error: 0.03475262108779715, Test Error: 0.5599392009793445\n",
      "Train Error: 0.13642290725239967, Test Error: 0.3458643370561799\n",
      "Train Error: 0.13639265582160226, Test Error: 0.34543063903735866\n",
      "Train Error: 0.13638442845398763, Test Error: 0.3451492298615604\n",
      "Train Error: 0.13638007608530225, Test Error: 0.34485224786814167\n",
      "Train Error: 0.13640445871244272, Test Error: 0.34577793029115084\n",
      "Train Error: 0.1363770606946588, Test Error: 0.3443243028210966\n",
      "Train Error: 0.13639578335876182, Test Error: 0.34360871157057554\n",
      "Train Error: 0.13645529697663392, Test Error: 0.3428644383468811\n",
      "Train Error: 0.13639199399406404, Test Error: 0.34535354422510744\n",
      "Train Error: 0.1364891038866733, Test Error: 0.342452146492026\n",
      "Train Error: 0.13665479464461372, Test Error: 0.34089035120402594\n",
      "Train Error: 0.13686323267489303, Test Error: 0.33951523628379016\n",
      "Train Error: 0.1363822599111499, Test Error: 0.3449501301024007\n",
      "Train Error: 0.13670347571994937, Test Error: 0.3404988430041359\n",
      "Train Error: 0.13705163951944557, Test Error: 0.33843456066136485\n",
      "Train Error: 0.1375232119533848, Test Error: 0.33671659219591354\n",
      "Train Error: 0.1363780425301494, Test Error: 0.3445311119018653\n",
      "Train Error: 0.13698799162571837, Test Error: 0.3387207496171366\n",
      "Train Error: 0.1376114771577605, Test Error: 0.33641439594472117\n",
      "Train Error: 0.13838579449858743, Test Error: 0.33441277174900624\n",
      "Train Error: 0.13642764648319047, Test Error: 0.34290709059719077\n",
      "Train Error: 0.138831741289354, Test Error: 0.33328164840667995\n",
      "Train Error: 0.14092993430383624, Test Error: 0.32962109297510683\n",
      "Train Error: 0.14380929727491873, Test Error: 0.3242722949846334\n",
      "Train Error: 0.13649543408850023, Test Error: 0.3420776892780688\n",
      "Train Error: 0.14007834814725192, Test Error: 0.33091532221988235\n",
      "Train Error: 0.1434132705628654, Test Error: 0.324929809682515\n",
      "Train Error: 0.14767918851223438, Test Error: 0.3180879226447176\n",
      "Train Error: 0.15062548999133404, Test Error: 0.13406891593497167\n",
      "Train Error: 0.1507622238168621, Test Error: 0.13416374744003953\n",
      "Train Error: 0.1508078725949041, Test Error: 0.13430372370775406\n",
      "Train Error: 0.1508144013884029, Test Error: 0.13448721389926288\n",
      "Train Error: 0.15068190208484658, Test Error: 0.13402411227130384\n",
      "Train Error: 0.15082467658398582, Test Error: 0.1348234104260606\n",
      "Train Error: 0.150850880014783, Test Error: 0.13535948804604017\n",
      "Train Error: 0.1508920523001862, Test Error: 0.13589207423292488\n",
      "Train Error: 0.1507470011043009, Test Error: 0.1341999558015932\n",
      "Train Error: 0.15092743847368037, Test Error: 0.136173385762966\n",
      "Train Error: 0.15111069564364868, Test Error: 0.13744737664692866\n",
      "Train Error: 0.15133379364529065, Test Error: 0.13878012293826586\n",
      "Train Error: 0.15078104581904142, Test Error: 0.1344127339369178\n",
      "Train Error: 0.15115058003901832, Test Error: 0.1378126758577099\n",
      "Train Error: 0.15151738958410185, Test Error: 0.13983468840488503\n",
      "Train Error: 0.15197839074501096, Test Error: 0.14194827548064584\n",
      "Train Error: 0.1507754065173242, Test Error: 0.13469884264379092\n",
      "Train Error: 0.15143863266237184, Test Error: 0.13951550426510334\n",
      "Train Error: 0.15203419983073477, Test Error: 0.14236163076845257\n",
      "Train Error: 0.1526389147035608, Test Error: 0.1454635582826053\n",
      "Train Error: 0.15079575148634175, Test Error: 0.13578660028104142\n",
      "Train Error: 0.15294391448376665, Test Error: 0.14713473211796005\n",
      "Train Error: 0.15465838375011534, Test Error: 0.15366981455457335\n",
      "Train Error: 0.15679154691244424, Test Error: 0.16029373913415515\n",
      "Train Error: 0.15083880022625762, Test Error: 0.1363689683628112\n",
      "Train Error: 0.15393432182281072, Test Error: 0.15119003904221368\n",
      "Train Error: 0.15649276643755086, Test Error: 0.15951236380718847\n",
      "Train Error: 0.15949232670530575, Test Error: 0.16773606342131403\n",
      "Train Error: 0.12866493141824548, Test Error: 0.17043412029311944\n",
      "Train Error: 0.12876377745432915, Test Error: 0.17162388190039712\n",
      "Train Error: 0.1288221849367592, Test Error: 0.17228170827640715\n",
      "Train Error: 0.1288810283388555, Test Error: 0.17284885593094873\n",
      "Train Error: 0.12868999352144034, Test Error: 0.17080935350716506\n",
      "Train Error: 0.12899220133016406, Test Error: 0.17386341680851097\n",
      "Train Error: 0.12916078802640668, Test Error: 0.17529440347529468\n",
      "Train Error: 0.12934591639206192, Test Error: 0.17674849371594167\n",
      "Train Error: 0.1287674437147589, Test Error: 0.17165342681220525\n",
      "Train Error: 0.12944255471839133, Test Error: 0.177471543111325\n",
      "Train Error: 0.12986738600657863, Test Error: 0.18038522151247663\n",
      "Train Error: 0.13034369459943193, Test Error: 0.18331816604463766\n",
      "Train Error: 0.12883506883230067, Test Error: 0.1724273861511216\n",
      "Train Error: 0.12998180110817956, Test Error: 0.18111391001667967\n",
      "Train Error: 0.13066861026934187, Test Error: 0.1855600657366549\n",
      "Train Error: 0.13138976130553764, Test Error: 0.19002251677038243\n",
      "Train Error: 0.12891182413087554, Test Error: 0.1731554722942145\n",
      "Train Error: 0.13055791901483904, Test Error: 0.1848191607296077\n",
      "Train Error: 0.1315190963773077, Test Error: 0.1907624762022263\n",
      "Train Error: 0.1326471031637648, Test Error: 0.1966862207714191\n",
      "Train Error: 0.12925106417688031, Test Error: 0.1760388775376058\n",
      "Train Error: 0.13327935430832735, Test Error: 0.1997889813818054\n",
      "Train Error: 0.13616648875448517, Test Error: 0.21147532152160148\n",
      "Train Error: 0.1395616062542131, Test Error: 0.22287440225884025\n",
      "Train Error: 0.1294440522780333, Test Error: 0.17749856369359648\n",
      "Train Error: 0.13500916690069503, Test Error: 0.20696018737915028\n",
      "Train Error: 0.1391274918021662, Test Error: 0.2214887109254696\n",
      "Train Error: 0.14374554550728763, Test Error: 0.23450152957664314\n",
      "Train Error: 0.09178696489248053, Test Error: 0.09702554674391503\n",
      "Train Error: 0.09218449177914892, Test Error: 0.0969335379600849\n",
      "Train Error: 0.09238913439443024, Test Error: 0.09689033555667667\n",
      "Train Error: 0.09259210438195767, Test Error: 0.09684384403780198\n",
      "Train Error: 0.09194419272720458, Test Error: 0.09699331128898493\n",
      "Train Error: 0.092965018202399, Test Error: 0.09676806781584182\n",
      "Train Error: 0.09349793900333879, Test Error: 0.09666057009966483\n",
      "Train Error: 0.0940451402747216, Test Error: 0.09655628923243033\n",
      "Train Error: 0.09220386144774091, Test Error: 0.0969344243663085\n",
      "Train Error: 0.09433487871830595, Test Error: 0.09650403052932377\n",
      "Train Error: 0.0954856093944774, Test Error: 0.0963090063317499\n",
      "Train Error: 0.0966809895781701, Test Error: 0.09612180585062297\n",
      "Train Error: 0.09247151497105321, Test Error: 0.09688009362852505\n",
      "Train Error: 0.0957925065628957, Test Error: 0.09626076914032476\n",
      "Train Error: 0.09761862093053177, Test Error: 0.09598649263357657\n",
      "Train Error: 0.09955845796714023, Test Error: 0.09574443118093813\n",
      "Train Error: 0.09274376739025364, Test Error: 0.0968268076362157\n",
      "Train Error: 0.09732383699825661, Test Error: 0.09603233311882185\n",
      "Train Error: 0.09990478333272772, Test Error: 0.09570617442447296\n",
      "Train Error: 0.10264438473573993, Test Error: 0.09542225685732908\n",
      "Train Error: 0.09386640426494706, Test Error: 0.09661548019220412\n",
      "Train Error: 0.1041415377702255, Test Error: 0.09530284907154701\n",
      "Train Error: 0.11017954713446973, Test Error: 0.09492269074959901\n",
      "Train Error: 0.1166200345406219, Test Error: 0.09472287000626385\n",
      "Train Error: 0.09445193573432142, Test Error: 0.09651533031694556\n",
      "Train Error: 0.10790167757918734, Test Error: 0.09504919068660428\n",
      "Train Error: 0.11586594680039737, Test Error: 0.09474867997155914\n",
      "Train Error: 0.1241923804219538, Test Error: 0.09463271074713274\n",
      "Train Error: 0.08065678435677494, Test Error: 0.12594965430141727\n",
      "Train Error: 0.08084732386197753, Test Error: 0.126265482457537\n",
      "Train Error: 0.08095836965443377, Test Error: 0.1264267500550129\n",
      "Train Error: 0.0810649790177963, Test Error: 0.1265870622772375\n",
      "Train Error: 0.08073734518996711, Test Error: 0.1260778120457328\n",
      "Train Error: 0.08128401066932153, Test Error: 0.12688311288176365\n",
      "Train Error: 0.08159952346274661, Test Error: 0.12729511486995113\n",
      "Train Error: 0.08193304716190569, Test Error: 0.1277101518032302\n",
      "Train Error: 0.08088711970033015, Test Error: 0.12629518121857924\n",
      "Train Error: 0.0821410178820253, Test Error: 0.1279413128511846\n",
      "Train Error: 0.08292182587224489, Test Error: 0.12879697740512402\n",
      "Train Error: 0.08383973788077818, Test Error: 0.12971901628395074\n",
      "Train Error: 0.08103564750720464, Test Error: 0.12651213983412785\n",
      "Train Error: 0.08316165778818882, Test Error: 0.12904307568116868\n",
      "Train Error: 0.08461799688282888, Test Error: 0.13044460256897764\n",
      "Train Error: 0.08623500738867157, Test Error: 0.13195258708374974\n",
      "Train Error: 0.08120125144261647, Test Error: 0.12673340090623636\n",
      "Train Error: 0.08437987833162094, Test Error: 0.1302247497622479\n",
      "Train Error: 0.08654787927645094, Test Error: 0.13222812910971685\n",
      "Train Error: 0.08901343299740576, Test Error: 0.13432745539364244\n",
      "Train Error: 0.08192209346222791, Test Error: 0.12763193292509936\n",
      "Train Error: 0.09046438427557293, Test Error: 0.13548478643583473\n",
      "Train Error: 0.09645711048675462, Test Error: 0.14005789161974808\n",
      "Train Error: 0.10311142965997776, Test Error: 0.14458837872976926\n",
      "Train Error: 0.08231805138342668, Test Error: 0.12808896925083754\n",
      "Train Error: 0.09414325001700655, Test Error: 0.1383024801655394\n",
      "Train Error: 0.10232333224902838, Test Error: 0.14406068964851812\n",
      "Train Error: 0.11131490694816783, Test Error: 0.14984684603148266\n",
      "Train Error: 0.07226361668446686, Test Error: 0.061918383502839014\n",
      "Train Error: 0.07279463954787141, Test Error: 0.061875031189857994\n",
      "Train Error: 0.07305214210885601, Test Error: 0.061856927212378114\n",
      "Train Error: 0.07331178486400364, Test Error: 0.06184047934719146\n",
      "Train Error: 0.07249504240667101, Test Error: 0.061906280800281065\n",
      "Train Error: 0.07378665088957073, Test Error: 0.061821483229400856\n",
      "Train Error: 0.07445682491552681, Test Error: 0.06179490191745397\n",
      "Train Error: 0.0751382294456405, Test Error: 0.061778409443438397\n",
      "Train Error: 0.07283832280436223, Test Error: 0.061890742631211824\n",
      "Train Error: 0.0755147950548941, Test Error: 0.061784688366905774\n",
      "Train Error: 0.07693662021203387, Test Error: 0.06179377849360317\n",
      "Train Error: 0.07840476953706714, Test Error: 0.06184379022123617\n",
      "Train Error: 0.07318846293314638, Test Error: 0.06187847400793192\n",
      "Train Error: 0.07732742796064718, Test Error: 0.061813213342007636\n",
      "Train Error: 0.07956993048813484, Test Error: 0.06191984178302916\n",
      "Train Error: 0.08190943017775988, Test Error: 0.062118321378224955\n",
      "Train Error: 0.0735405235392488, Test Error: 0.061868767555848005\n",
      "Train Error: 0.07922070106682003, Test Error: 0.061907048819620905\n",
      "Train Error: 0.08233723510777753, Test Error: 0.0621718537883832\n",
      "Train Error: 0.08560594845141914, Test Error: 0.06259760723833264\n",
      "Train Error: 0.07499723337568356, Test Error: 0.06185842017430617\n",
      "Train Error: 0.08742790514710437, Test Error: 0.06292013734487366\n",
      "Train Error: 0.09442740944832001, Test Error: 0.06438944553922009\n",
      "Train Error: 0.1017527638041757, Test Error: 0.06642886333158982\n",
      "Train Error: 0.07575050962130764, Test Error: 0.06187001493554665\n",
      "Train Error: 0.09182868364286124, Test Error: 0.0637950663176398\n",
      "Train Error: 0.10089828559050491, Test Error: 0.06617172404157044\n",
      "Train Error: 0.11032407622963818, Test Error: 0.06936180115401236\n",
      "Train Error: 0.07071386185371735, Test Error: 0.16382753699927152\n",
      "Train Error: 0.07078802447741969, Test Error: 0.1635239709602524\n",
      "Train Error: 0.07083786073461289, Test Error: 0.16341008377908167\n",
      "Train Error: 0.07089990388251634, Test Error: 0.16344637984197152\n",
      "Train Error: 0.07074522717557952, Test Error: 0.16373368699734453\n",
      "Train Error: 0.07102245761566442, Test Error: 0.1635332771417888\n",
      "Train Error: 0.07120019611537211, Test Error: 0.16364253296587492\n",
      "Train Error: 0.0713929031004966, Test Error: 0.16376741801051467\n",
      "Train Error: 0.07080212491711195, Test Error: 0.16357912289833665\n",
      "Train Error: 0.07150064245863341, Test Error: 0.1639133143588071\n",
      "Train Error: 0.07194891258409015, Test Error: 0.16421916878880494\n",
      "Train Error: 0.07245603545614362, Test Error: 0.16457639963918455\n",
      "Train Error: 0.0708784934948556, Test Error: 0.16351812478725974\n",
      "Train Error: 0.07208229470828305, Test Error: 0.1643419184731148\n",
      "Train Error: 0.07288769415465339, Test Error: 0.16493265116008318\n",
      "Train Error: 0.07382056259267, Test Error: 0.16565300835650112\n",
      "Train Error: 0.07096754442566987, Test Error: 0.16360366121723385\n",
      "Train Error: 0.07275762924027776, Test Error: 0.16486617617084642\n",
      "Train Error: 0.07400234039905862, Test Error: 0.1658235698830029\n",
      "Train Error: 0.07546100403908625, Test Error: 0.16699440750466743\n",
      "Train Error: 0.07137026991765993, Test Error: 0.16397481519617746\n",
      "Train Error: 0.07633387082363469, Test Error: 0.16782920232807413\n",
      "Train Error: 0.08003606760560339, Test Error: 0.171019227298124\n",
      "Train Error: 0.08440687379251895, Test Error: 0.17498625972757184\n",
      "Train Error: 0.07159583235352558, Test Error: 0.16418724571099683\n",
      "Train Error: 0.07860038936003974, Test Error: 0.1698114792970046\n",
      "Train Error: 0.0838687273825428, Test Error: 0.17453112444324767\n",
      "Train Error: 0.09004748948556984, Test Error: 0.1803675807934649\n",
      "Train Error: 0.10015423304229061, Test Error: 0.18223720952998843\n",
      "Train Error: 0.1002601652601513, Test Error: 0.18010018653914295\n",
      "Train Error: 0.10037099668025579, Test Error: 0.1792890782412922\n",
      "Train Error: 0.10048671181022764, Test Error: 0.17846163572142404\n",
      "Train Error: 0.1001769345340172, Test Error: 0.18146320617045\n",
      "Train Error: 0.10070656838323962, Test Error: 0.17706847265610592\n",
      "Train Error: 0.10105179497783164, Test Error: 0.17517795746436352\n",
      "Train Error: 0.10125996229691761, Test Error: 0.1742696999194596\n",
      "Train Error: 0.10028547772032417, Test Error: 0.18011702431825466\n",
      "Train Error: 0.10138770418363882, Test Error: 0.17385199532297912\n",
      "Train Error: 0.10184175595783472, Test Error: 0.1721416830917907\n",
      "Train Error: 0.10233587166441864, Test Error: 0.17050757171770195\n",
      "Train Error: 0.10044109613531221, Test Error: 0.17911071344253257\n",
      "Train Error: 0.10197591983344548, Test Error: 0.17175074220478273\n",
      "Train Error: 0.10275000308987682, Test Error: 0.1693573120974273\n",
      "Train Error: 0.10359345064990728, Test Error: 0.16716625726352782\n",
      "Train Error: 0.10060271518625809, Test Error: 0.17812134258087703\n",
      "Train Error: 0.10262718497735528, Test Error: 0.1697741309404471\n",
      "Train Error: 0.1037658897281439, Test Error: 0.16683403133996672\n",
      "Train Error: 0.10505655134161104, Test Error: 0.16416878683343777\n",
      "Train Error: 0.10126525186529435, Test Error: 0.17487378329031739\n",
      "Train Error: 0.10581506272676335, Test Error: 0.16313115231896455\n",
      "Train Error: 0.1087203558394236, Test Error: 0.15917199206685737\n",
      "Train Error: 0.11207069454063799, Test Error: 0.15641740484151687\n",
      "Train Error: 0.10151221219892782, Test Error: 0.17403624903868534\n",
      "Train Error: 0.10762453950569259, Test Error: 0.16055650437717547\n",
      "Train Error: 0.11166984477765342, Test Error: 0.1567483814997944\n",
      "Train Error: 0.11638288641569473, Test Error: 0.15514345622049688\n",
      "Train Error: 0.09441482642972143, Test Error: 26.44440325174781\n",
      "Train Error: 0.0944879403166367, Test Error: 25.950315532146448\n",
      "Train Error: 0.09455799586966304, Test Error: 25.691996821142506\n",
      "Train Error: 0.09463048730642695, Test Error: 25.429809054500744\n",
      "Train Error: 0.0944231436039866, Test Error: 26.265350158688896\n",
      "Train Error: 0.09478740536335582, Test Error: 24.963198604572238\n",
      "Train Error: 0.09497966205065714, Test Error: 24.421855182642457\n",
      "Train Error: 0.0950833478709652, Test Error: 24.084880516240275\n",
      "Train Error: 0.09450971439728573, Test Error: 25.919009712820387\n",
      "Train Error: 0.09515316195346708, Test Error: 23.89581563451419\n",
      "Train Error: 0.09543475125104153, Test Error: 23.221376154613797\n",
      "Train Error: 0.09578933959999272, Test Error: 22.545633389225312\n",
      "Train Error: 0.09460783366061416, Test Error: 25.573000094963437\n",
      "Train Error: 0.09552902867665398, Test Error: 23.03194279311898\n",
      "Train Error: 0.09611776333625517, Test Error: 22.01976323163522\n",
      "Train Error: 0.0968666841501472, Test Error: 21.006433007098334\n",
      "Train Error: 0.09471785105920481, Test Error: 25.227439325019436\n",
      "Train Error: 0.09602408457899717, Test Error: 22.170747579926115\n",
      "Train Error: 0.09702484996963032, Test Error: 20.821115088608682\n",
      "Train Error: 0.09830891246154257, Test Error: 19.414571228343867\n",
      "Train Error: 0.0951253109605883, Test Error: 24.130962414407254\n",
      "Train Error: 0.09914550114389209, Test Error: 18.59455189241235\n",
      "Train Error: 0.1026849104781743, Test Error: 15.378358962931882\n",
      "Train Error: 0.10688194227044504, Test Error: 11.966129644736215\n",
      "Train Error: 0.09526806934246353, Test Error: 23.757727320320537\n",
      "Train Error: 0.10130570257234688, Test Error: 16.59675604656208\n",
      "Train Error: 0.10636770047869551, Test Error: 12.364771910586958\n",
      "Train Error: 0.11234253797160515, Test Error: 8.173415397735923\n",
      "Train Error: 7.03985036449181, Test Error: 0.5391106557279698\n",
      "Train Error: 7.032413512162575, Test Error: 0.539906450356933\n",
      "Train Error: 7.06588613802969, Test Error: 0.541861153009581\n",
      "Train Error: 7.052562437942064, Test Error: 0.5435407790049372\n",
      "Train Error: 7.031640656926861, Test Error: 0.5393251728180952\n",
      "Train Error: 7.028269939259505, Test Error: 0.5466345796425747\n",
      "Train Error: 6.994579860453333, Test Error: 0.5508595295376221\n",
      "Train Error: 6.960750017194236, Test Error: 0.5551704403343413\n",
      "Train Error: 7.030991576801592, Test Error: 0.5401362388248064\n",
      "Train Error: 6.942095066649109, Test Error: 0.5574797591665991\n",
      "Train Error: 6.855946918142915, Test Error: 0.5690225507251521\n",
      "Train Error: 6.7848038986977075, Test Error: 0.5777890405780595\n",
      "Train Error: 7.058267431980131, Test Error: 0.5427162280550908\n",
      "Train Error: 6.836907339107215, Test Error: 0.5713726901613748\n",
      "Train Error: 6.730659348166659, Test Error: 0.5845405922118124\n",
      "Train Error: 6.58421615581645, Test Error: 0.597511788647896\n",
      "Train Error: 7.039375227900561, Test Error: 0.5448865547942817\n",
      "Train Error: 6.745594574501043, Test Error: 0.582585259082089\n",
      "Train Error: 6.565381676356681, Test Error: 0.5997938228220036\n",
      "Train Error: 6.435527503916807, Test Error: 0.6174691127568478\n",
      "Train Error: 6.96559528105593, Test Error: 0.5542203407851444\n",
      "Train Error: 6.363123872607212, Test Error: 0.6273734255843098\n",
      "Train Error: 6.101026998147557, Test Error: 0.6636974910828864\n",
      "Train Error: 5.839011382260559, Test Error: 0.7010113738296172\n",
      "Train Error: 6.928580534357706, Test Error: 0.5590258921867666\n",
      "Train Error: 6.1949437473212825, Test Error: 0.6503619583918756\n",
      "Train Error: 5.866932331034663, Test Error: 0.6967211298610629\n",
      "Train Error: 5.416577848777451, Test Error: 0.7440617802508334\n",
      "Train Error: 11.017886048084863, Test Error: 0.17094508321589338\n",
      "Train Error: 10.979415472314919, Test Error: 0.17068411074404657\n",
      "Train Error: 10.963084905772684, Test Error: 0.17058412830784517\n",
      "Train Error: 10.94954818203465, Test Error: 0.1704916317828464\n",
      "Train Error: 10.998089239483743, Test Error: 0.17077707548686866\n",
      "Train Error: 10.917886592539793, Test Error: 0.17028133665702924\n",
      "Train Error: 10.864054149625066, Test Error: 0.17015356498986575\n",
      "Train Error: 10.810394524087155, Test Error: 0.17002870784728896\n",
      "Train Error: 10.970825647657257, Test Error: 0.17036125206207303\n",
      "Train Error: 10.777534322186318, Test Error: 0.16976741228160533\n",
      "Train Error: 10.660030053519089, Test Error: 0.16980789938612822\n",
      "Train Error: 10.551079218007587, Test Error: 0.1695584255267992\n",
      "Train Error: 10.94417725897989, Test Error: 0.1700471402417256\n",
      "Train Error: 10.626774717436646, Test Error: 0.169549376921693\n",
      "Train Error: 10.46277097940342, Test Error: 0.16918487848548075\n",
      "Train Error: 10.296174226968459, Test Error: 0.16885145755324724\n",
      "Train Error: 10.920443782076259, Test Error: 0.16974796588924493\n",
      "Train Error: 10.484082630552514, Test Error: 0.1690490930681411\n",
      "Train Error: 10.262508220235445, Test Error: 0.16860045595172354\n",
      "Train Error: 10.042284239795071, Test Error: 0.16817467384837165\n",
      "Train Error: 10.796869039410897, Test Error: 0.168713275151462\n",
      "Train Error: 9.904890240051412, Test Error: 0.1671780087904598\n",
      "Train Error: 9.478902554113743, Test Error: 0.16622060147636397\n",
      "Train Error: 9.022118511904246, Test Error: 0.16583764717089824\n",
      "Train Error: 10.732140419629877, Test Error: 0.16820473746502387\n",
      "Train Error: 9.625028285251354, Test Error: 0.1663265394641758\n",
      "Train Error: 9.067877305081826, Test Error: 0.1654865476284471\n",
      "Train Error: 8.496511390460398, Test Error: 0.16517650097444359\n",
      "Train Error: 7.895110042614221, Test Error: 0.1011996095497706\n",
      "Train Error: 7.6267967967185974, Test Error: 0.10094826434972971\n",
      "Train Error: 7.51400370982926, Test Error: 0.10086058268288135\n",
      "Train Error: 7.512780497793688, Test Error: 0.10095527236293322\n",
      "Train Error: 7.7695199734828515, Test Error: 0.10106457890899223\n",
      "Train Error: 7.424895645183564, Test Error: 0.10096534409485555\n",
      "Train Error: 7.364054921192459, Test Error: 0.10100736323884277\n",
      "Train Error: 7.322057456533666, Test Error: 0.10113450990437765\n",
      "Train Error: 7.6085449221162325, Test Error: 0.10088700322120157\n",
      "Train Error: 7.2812387022859255, Test Error: 0.1011152218657592\n",
      "Train Error: 7.15171248340736, Test Error: 0.10119194346636887\n",
      "Train Error: 7.0236780269653725, Test Error: 0.1012767081568602\n",
      "Train Error: 7.480301888711762, Test Error: 0.1007714806170602\n",
      "Train Error: 7.110501145686031, Test Error: 0.10117494502131326\n",
      "Train Error: 6.918179941897111, Test Error: 0.1013073236329132\n",
      "Train Error: 6.680910074367665, Test Error: 0.10138228656414518\n",
      "Train Error: 7.454912763527709, Test Error: 0.10083323196323156\n",
      "Train Error: 6.941044559714281, Test Error: 0.1012463885691948\n",
      "Train Error: 6.683063578125111, Test Error: 0.101443299703314\n",
      "Train Error: 6.391841059796395, Test Error: 0.10160854925395242\n",
      "Train Error: 7.270346308568854, Test Error: 0.10076763600175896\n",
      "Train Error: 6.2308413425901055, Test Error: 0.10158304036346877\n",
      "Train Error: 5.717992010543568, Test Error: 0.10211669929369772\n",
      "Train Error: 5.217455953169889, Test Error: 0.10285150906863946\n",
      "Train Error: 7.212151252819994, Test Error: 0.10082108814353344\n",
      "Train Error: 5.883641880104209, Test Error: 0.10179348436675617\n",
      "Train Error: 5.2649754376081885, Test Error: 0.1026748439336412\n",
      "Train Error: 4.633485571264444, Test Error: 0.10373478764887262\n",
      "Train Error: 4.039366726019979, Test Error: 0.10543396449011976\n",
      "Train Error: 3.979226447707465, Test Error: 0.10545988356382782\n",
      "Train Error: 3.993415350670275, Test Error: 0.10555704455269771\n",
      "Train Error: 4.009111217511987, Test Error: 0.10567295222908037\n",
      "Train Error: 4.025872466699686, Test Error: 0.10548979772552018\n",
      "Train Error: 4.073917424731607, Test Error: 0.10599432093707932\n",
      "Train Error: 4.015800691950043, Test Error: 0.10617300489627855\n",
      "Train Error: 3.9519876269076764, Test Error: 0.1063396413812874\n",
      "Train Error: 3.964987154273372, Test Error: 0.1054067857070197\n",
      "Train Error: 3.9120617294927813, Test Error: 0.10638397897798804\n",
      "Train Error: 3.791373337884725, Test Error: 0.10673184066619595\n",
      "Train Error: 3.6586198114035287, Test Error: 0.10712656909788552\n",
      "Train Error: 3.9756118079445897, Test Error: 0.10549502131684875\n",
      "Train Error: 3.7529804893028924, Test Error: 0.1067845718309994\n",
      "Train Error: 3.5592776971839424, Test Error: 0.10736275331477885\n",
      "Train Error: 3.3064556560659115, Test Error: 0.10781819385297661\n",
      "Train Error: 3.9807560293751467, Test Error: 0.10560399221202221\n",
      "Train Error: 3.583812291194447, Test Error: 0.10723554391463845\n",
      "Train Error: 3.2682913966675415, Test Error: 0.10787626790906087\n",
      "Train Error: 3.0326042877972452, Test Error: 0.10865883341903668\n",
      "Train Error: 3.9244575262009516, Test Error: 0.10601340029147173\n",
      "Train Error: 2.8832761306702386, Test Error: 0.10893488254218042\n",
      "Train Error: 2.421543428185187, Test Error: 0.11072857260579796\n",
      "Train Error: 1.972393019575745, Test Error: 0.11267106681214724\n",
      "Train Error: 3.844045422985951, Test Error: 0.10611443323219735\n",
      "Train Error: 2.5792280513042587, Test Error: 0.10997601253954727\n",
      "Train Error: 2.016017677479692, Test Error: 0.11235226434544246\n",
      "Train Error: 1.4494060141081329, Test Error: 0.11503389188946353\n",
      "Train Error: 0.08875868792106724, Test Error: 0.13518099751302717\n",
      "Train Error: 0.08881771593360628, Test Error: 0.13555580323759986\n",
      "Train Error: 0.08884741033989398, Test Error: 0.13582665595721485\n",
      "Train Error: 0.08885624070436386, Test Error: 0.13595722358415327\n",
      "Train Error: 0.08882177200628931, Test Error: 0.1353949417906717\n",
      "Train Error: 0.08891760518137158, Test Error: 0.136327372364306\n",
      "Train Error: 0.08900378257937082, Test Error: 0.13683800378465738\n",
      "Train Error: 0.08912546148566024, Test Error: 0.1373542023695824\n",
      "Train Error: 0.08884837633727759, Test Error: 0.1355972207375334\n",
      "Train Error: 0.08921927718710132, Test Error: 0.1376420475401661\n",
      "Train Error: 0.08967906806458274, Test Error: 0.13876759300241193\n",
      "Train Error: 0.09008744849391738, Test Error: 0.13985765782795923\n",
      "Train Error: 0.08889539714451228, Test Error: 0.13587319787642263\n",
      "Train Error: 0.08981243997602639, Test Error: 0.13907297171053679\n",
      "Train Error: 0.09047064763694147, Test Error: 0.14071057403984782\n",
      "Train Error: 0.0912680063278972, Test Error: 0.14236416941669344\n",
      "Train Error: 0.08895077359305407, Test Error: 0.13615148815715183\n",
      "Train Error: 0.09038501447685789, Test Error: 0.1404672309395569\n",
      "Train Error: 0.09145187100029492, Test Error: 0.14267337702458624\n",
      "Train Error: 0.09285998957427917, Test Error: 0.14497672314235777\n",
      "Train Error: 0.08923646882611366, Test Error: 0.1372838566068443\n",
      "Train Error: 0.09384929944695004, Test Error: 0.14629002784457384\n",
      "Train Error: 0.09777040318329787, Test Error: 0.15117340119746214\n",
      "Train Error: 0.10265824208699194, Test Error: 0.15629869786371675\n",
      "Train Error: 0.08941577121677188, Test Error: 0.13786110465683898\n",
      "Train Error: 0.09628946929170486, Test Error: 0.14939462487436983\n",
      "Train Error: 0.10210257598279968, Test Error: 0.15571946816765772\n",
      "Train Error: 0.10922347534506373, Test Error: 0.16238930547065777\n",
      "Percentile: 0.2, Tracking Error: 0.727485411084925\n",
      "Starting percentile: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.487e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.004e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0007164869695616471, Test Error: 0.48316881786806226\n",
      "Train Error: 0.003113259569845735, Test Error: 0.29686721024385176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.009e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.004255418414344446, Test Error: 0.30563182491861773\n",
      "Train Error: 0.00496637231539274, Test Error: 0.3132697122334549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.345e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0016512924134957957, Test Error: 0.3166927092736583\n",
      "Train Error: 0.006414899036192622, Test Error: 0.3048562058739022\n",
      "Train Error: 0.008182629943198719, Test Error: 0.2994507688427709\n",
      "Train Error: 0.010279279820136518, Test Error: 0.29239059605475065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.277e-03, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0033608407510913782, Test Error: 0.2998588144219561\n",
      "Train Error: 0.011521631410190645, Test Error: 0.2888184242792898\n",
      "Train Error: 0.015821265092666483, Test Error: 0.2854839025810291\n",
      "Train Error: 0.019319649756633597, Test Error: 0.2626398243840268\n",
      "Train Error: 0.004952689034748321, Test Error: 0.3066046313297983\n",
      "Train Error: 0.01676547543754636, Test Error: 0.27954160288546903\n",
      "Train Error: 0.02152395478455597, Test Error: 0.2511084518556664\n",
      "Train Error: 0.02476170788167267, Test Error: 0.2440685315196812\n",
      "Train Error: 0.006145773601186643, Test Error: 0.30640317100389747\n",
      "Train Error: 0.021028536581459926, Test Error: 0.2533434561238217\n",
      "Train Error: 0.025074111990440393, Test Error: 0.2440084113548577\n",
      "Train Error: 0.02697956156315006, Test Error: 0.2476991720141925\n",
      "Train Error: 0.01031357962686467, Test Error: 0.28629354851039374\n",
      "Train Error: 0.027635275807527327, Test Error: 0.24853806678996224\n",
      "Train Error: 0.03042222869739676, Test Error: 0.2534496330293103\n",
      "Train Error: 0.03282618794655627, Test Error: 0.25863204076824653\n",
      "Train Error: 0.012618544723146415, Test Error: 0.2771953413879303\n",
      "Train Error: 0.029373786083420502, Test Error: 0.251461880340551\n",
      "Train Error: 0.03244986041790957, Test Error: 0.25777421502790837\n",
      "Train Error: 0.03569289273939884, Test Error: 0.2652151317888204\n",
      "Train Error: 0.14286789102473127, Test Error: 0.3740170025805134\n",
      "Train Error: 0.14294870783126942, Test Error: 0.3726523708187271\n",
      "Train Error: 0.1429917463767856, Test Error: 0.3719727783536927\n",
      "Train Error: 0.14301844832742766, Test Error: 0.37147450859146447\n",
      "Train Error: 0.14289808107358423, Test Error: 0.3735045466731518\n",
      "Train Error: 0.14306222585994227, Test Error: 0.3706324183010118\n",
      "Train Error: 0.14312912024064378, Test Error: 0.3694832966205422\n",
      "Train Error: 0.1432059579899572, Test Error: 0.368341788447049\n",
      "Train Error: 0.14295687802759005, Test Error: 0.37256130312580565\n",
      "Train Error: 0.14325248285939954, Test Error: 0.36773312651986145\n",
      "Train Error: 0.14343097567342283, Test Error: 0.36564162086933016\n",
      "Train Error: 0.14361641560119898, Test Error: 0.36389059683915836\n",
      "Train Error: 0.1430148717737271, Test Error: 0.37167732034401796\n",
      "Train Error: 0.14347902653684608, Test Error: 0.3651573563956254\n",
      "Train Error: 0.14380841987500864, Test Error: 0.3624336080842215\n",
      "Train Error: 0.14423886802655211, Test Error: 0.35965920470376816\n",
      "Train Error: 0.14304717434255768, Test Error: 0.3710434591669249\n",
      "Train Error: 0.14374706593969744, Test Error: 0.362855186897341\n",
      "Train Error: 0.1443206189262765, Test Error: 0.3591666009654936\n",
      "Train Error: 0.145013393275762, Test Error: 0.3555905914248006\n",
      "Train Error: 0.14320097137976787, Test Error: 0.3685423931620843\n",
      "Train Error: 0.14541896978690258, Test Error: 0.3537064697170182\n",
      "Train Error: 0.14726411152129562, Test Error: 0.3470854798960545\n",
      "Train Error: 0.14961252138689882, Test Error: 0.34047148167167063\n",
      "Train Error: 0.14329322452627544, Test Error: 0.36730584708014435\n",
      "Train Error: 0.14652724642385853, Test Error: 0.3494493301309038\n",
      "Train Error: 0.14930092211081766, Test Error: 0.34121900146720424\n",
      "Train Error: 0.15285697736882223, Test Error: 0.33290662438331803\n",
      "Train Error: 0.17011787664681574, Test Error: 0.16100326241596427\n",
      "Train Error: 0.1700562614800226, Test Error: 0.16107321250105192\n",
      "Train Error: 0.17002766329978894, Test Error: 0.16114205894814237\n",
      "Train Error: 0.17000288814260048, Test Error: 0.16119374827826286\n",
      "Train Error: 0.17009713394043194, Test Error: 0.16094035991600728\n",
      "Train Error: 0.16995278937943917, Test Error: 0.16132070720248182\n",
      "Train Error: 0.16990422648443376, Test Error: 0.16147224979614677\n",
      "Train Error: 0.1698633909284087, Test Error: 0.1616558721259994\n",
      "Train Error: 0.1700320414524858, Test Error: 0.16112161879317186\n",
      "Train Error: 0.16983157720364328, Test Error: 0.16181608443267417\n",
      "Train Error: 0.16976886131344723, Test Error: 0.1626006837486445\n",
      "Train Error: 0.16973963207857734, Test Error: 0.16343298453902705\n",
      "Train Error: 0.1699858864232834, Test Error: 0.1612014434776799\n",
      "Train Error: 0.16974399432513593, Test Error: 0.1628361432163258\n",
      "Train Error: 0.1697267207518327, Test Error: 0.16410822327654845\n",
      "Train Error: 0.16978999167834566, Test Error: 0.16576828272541788\n",
      "Train Error: 0.1699399707630033, Test Error: 0.1613053370204186\n",
      "Train Error: 0.1697110455521661, Test Error: 0.1639118299105589\n",
      "Train Error: 0.16979453270074182, Test Error: 0.1660936749784548\n",
      "Train Error: 0.16999097700945795, Test Error: 0.16856865833402077\n",
      "Train Error: 0.16978615870522015, Test Error: 0.1617207718125931\n",
      "Train Error: 0.17007509698252216, Test Error: 0.16992341699620617\n",
      "Train Error: 0.17083079440502708, Test Error: 0.17517105917114845\n",
      "Train Error: 0.1718443773543427, Test Error: 0.18077977549823754\n",
      "Train Error: 0.16972388548388354, Test Error: 0.1619874680045525\n",
      "Train Error: 0.17049160647247286, Test Error: 0.1732042027316317\n",
      "Train Error: 0.17167405639033986, Test Error: 0.18010606624141973\n",
      "Train Error: 0.1734061946594606, Test Error: 0.18737635969312344\n",
      "Train Error: 0.14115844649880557, Test Error: 0.17499396207985315\n",
      "Train Error: 0.1412527591974323, Test Error: 0.17600296403738183\n",
      "Train Error: 0.14130111735503148, Test Error: 0.17649979650494108\n",
      "Train Error: 0.1413509376306287, Test Error: 0.17699747945520874\n",
      "Train Error: 0.14119212430820413, Test Error: 0.1753709573718159\n",
      "Train Error: 0.1414414562398482, Test Error: 0.17787549290493535\n",
      "Train Error: 0.14158050970321537, Test Error: 0.17914129038447796\n",
      "Train Error: 0.1417265015249949, Test Error: 0.18039415078387494\n",
      "Train Error: 0.14125214563662375, Test Error: 0.1760170739952458\n",
      "Train Error: 0.14181383625169322, Test Error: 0.18115521776210033\n",
      "Train Error: 0.14214250918618276, Test Error: 0.18368487683472676\n",
      "Train Error: 0.142504591043699, Test Error: 0.186203884091797\n",
      "Train Error: 0.14131264537406912, Test Error: 0.1766473209147987\n",
      "Train Error: 0.1422302620126228, Test Error: 0.18432194663907892\n",
      "Train Error: 0.14280038863119182, Test Error: 0.18812484565654106\n",
      "Train Error: 0.14344633225769807, Test Error: 0.19193840770101953\n",
      "Train Error: 0.1413755569037228, Test Error: 0.17727869833648585\n",
      "Train Error: 0.1427009574373497, Test Error: 0.18750141019736993\n",
      "Train Error: 0.1435628216557928, Test Error: 0.19259556883496062\n",
      "Train Error: 0.14454304078957622, Test Error: 0.197743093431167\n",
      "Train Error: 0.1416548627875912, Test Error: 0.17984585106025888\n",
      "Train Error: 0.14508588142249107, Test Error: 0.20034933682445924\n",
      "Train Error: 0.14753232967443877, Test Error: 0.21060799373165315\n",
      "Train Error: 0.1504479068361771, Test Error: 0.22083058257610721\n",
      "Train Error: 0.141806599913596, Test Error: 0.18111648897887034\n",
      "Train Error: 0.1465656966393561, Test Error: 0.20679087822325046\n",
      "Train Error: 0.15006460334559596, Test Error: 0.21957098554365304\n",
      "Train Error: 0.15424323394344022, Test Error: 0.23221685157427696\n",
      "Train Error: 0.10115245470838685, Test Error: 0.09680188764562275\n",
      "Train Error: 0.10136571044603848, Test Error: 0.09676916163528101\n",
      "Train Error: 0.101483984925997, Test Error: 0.09675687499739499\n",
      "Train Error: 0.1015978084659496, Test Error: 0.09674148064181232\n",
      "Train Error: 0.10123980575445833, Test Error: 0.09678808263236446\n",
      "Train Error: 0.10181361438063517, Test Error: 0.09671438632191533\n",
      "Train Error: 0.10213080155005184, Test Error: 0.09668285628327793\n",
      "Train Error: 0.10245848566368274, Test Error: 0.0966505256016108\n",
      "Train Error: 0.10138966897698076, Test Error: 0.096765730063945\n",
      "Train Error: 0.10265272222318982, Test Error: 0.0966365790152865\n",
      "Train Error: 0.10337575284196565, Test Error: 0.09658121180885396\n",
      "Train Error: 0.10416959101337156, Test Error: 0.0965377124882663\n",
      "Train Error: 0.10155175195819344, Test Error: 0.09674751251920012\n",
      "Train Error: 0.10359217123434301, Test Error: 0.09657032219182703\n",
      "Train Error: 0.10481927768725766, Test Error: 0.09650696344431217\n",
      "Train Error: 0.10618768129106805, Test Error: 0.09646613108627661\n",
      "Train Error: 0.10171170796264181, Test Error: 0.09672656124488224\n",
      "Train Error: 0.10462435494680838, Test Error: 0.09651462988528914\n",
      "Train Error: 0.10644755242005695, Test Error: 0.09645938908764949\n",
      "Train Error: 0.10849228221625061, Test Error: 0.0964384801710257\n",
      "Train Error: 0.10241584469441144, Test Error: 0.09665668333880441\n",
      "Train Error: 0.10967773946350115, Test Error: 0.0964353947590305\n",
      "Train Error: 0.11453773086146592, Test Error: 0.09652843087890157\n",
      "Train Error: 0.12002528001137358, Test Error: 0.09675413882664412\n",
      "Train Error: 0.10279692443840004, Test Error: 0.09662578491305544\n",
      "Train Error: 0.11268493901725608, Test Error: 0.09647754718847952\n",
      "Train Error: 0.11936951304785355, Test Error: 0.09672113477234609\n",
      "Train Error: 0.1267419885720187, Test Error: 0.09711884660048177\n",
      "Train Error: 0.09865274432663197, Test Error: 0.12546061734221697\n",
      "Train Error: 0.09913992308562013, Test Error: 0.12584611357024236\n",
      "Train Error: 0.09936720013307836, Test Error: 0.12604502277130047\n",
      "Train Error: 0.09958720739320435, Test Error: 0.12622329801333587\n",
      "Train Error: 0.09886844744957823, Test Error: 0.1256280634242269\n",
      "Train Error: 0.10000691388905424, Test Error: 0.1265517508333649\n",
      "Train Error: 0.10058098943183537, Test Error: 0.12700814321049594\n",
      "Train Error: 0.10116193520207813, Test Error: 0.12746946210941376\n",
      "Train Error: 0.09917718956021722, Test Error: 0.12587122572544074\n",
      "Train Error: 0.10148704491177146, Test Error: 0.1277220408141444\n",
      "Train Error: 0.1026896072213288, Test Error: 0.12867044287099497\n",
      "Train Error: 0.10393314163742116, Test Error: 0.1296449311174022\n",
      "Train Error: 0.09948015536007344, Test Error: 0.12613132672076463\n",
      "Train Error: 0.10302518942424016, Test Error: 0.12893095499580345\n",
      "Train Error: 0.1049128839770476, Test Error: 0.1304094161325208\n",
      "Train Error: 0.10684976090144697, Test Error: 0.1319295475680692\n",
      "Train Error: 0.09979296497456971, Test Error: 0.1263746895662693\n",
      "Train Error: 0.10462204828814958, Test Error: 0.1301793065892423\n",
      "Train Error: 0.10720984305248567, Test Error: 0.13220906743414054\n",
      "Train Error: 0.10990939705424684, Test Error: 0.13432319356406125\n",
      "Train Error: 0.10104774593095142, Test Error: 0.12736010184359894\n",
      "Train Error: 0.11140722782068424, Test Error: 0.13549417106518277\n",
      "Train Error: 0.11709652553876698, Test Error: 0.13980257872360086\n",
      "Train Error: 0.12306341118907307, Test Error: 0.14437478017976443\n",
      "Train Error: 0.10168753054793456, Test Error: 0.12786221568612594\n",
      "Train Error: 0.11499134327606446, Test Error: 0.13821181002845145\n",
      "Train Error: 0.12237457130178014, Test Error: 0.1438488237373793\n",
      "Train Error: 0.13010462537789072, Test Error: 0.14987731439279184\n",
      "Train Error: 0.13986764683367184, Test Error: 0.0865382499698572\n",
      "Train Error: 0.14038612286550395, Test Error: 0.08650439367241712\n",
      "Train Error: 0.1406134942297274, Test Error: 0.08650439388165461\n",
      "Train Error: 0.14084498078207713, Test Error: 0.08650772294501259\n",
      "Train Error: 0.14011172906580843, Test Error: 0.08651246932969243\n",
      "Train Error: 0.14125984481925646, Test Error: 0.08651641791876825\n",
      "Train Error: 0.14179705024817651, Test Error: 0.08651471758401649\n",
      "Train Error: 0.14238686233190195, Test Error: 0.0865385092809479\n",
      "Train Error: 0.14041867003216263, Test Error: 0.08651227541575927\n",
      "Train Error: 0.14269703391216235, Test Error: 0.08655636840380097\n",
      "Train Error: 0.14387180145407752, Test Error: 0.08662976712978813\n",
      "Train Error: 0.14507340980866062, Test Error: 0.08672758580695947\n",
      "Train Error: 0.1407245523683342, Test Error: 0.08651812432857278\n",
      "Train Error: 0.14420290682118259, Test Error: 0.08665600668790804\n",
      "Train Error: 0.14600521926126783, Test Error: 0.08682381170755518\n",
      "Train Error: 0.14784638077342688, Test Error: 0.08705122995935415\n",
      "Train Error: 0.14103123689934863, Test Error: 0.0865254085229427\n",
      "Train Error: 0.1457293931598101, Test Error: 0.08679670955253332\n",
      "Train Error: 0.14817844180145284, Test Error: 0.08710035757432605\n",
      "Train Error: 0.15068240732395777, Test Error: 0.08750522325742721\n",
      "Train Error: 0.14223989030752157, Test Error: 0.08656250911052857\n",
      "Train Error: 0.1520457996952129, Test Error: 0.0877733685742796\n",
      "Train Error: 0.15722480862388383, Test Error: 0.08898710310562886\n",
      "Train Error: 0.1625438002133683, Test Error: 0.0905862391649006\n",
      "Train Error: 0.14286791900699125, Test Error: 0.08659920641777247\n",
      "Train Error: 0.15531350419733753, Test Error: 0.08849939110640438\n",
      "Train Error: 0.16192342863096254, Test Error: 0.0903807289197952\n",
      "Train Error: 0.1689072670362118, Test Error: 0.09272179928261694\n",
      "Train Error: 0.07770233031672358, Test Error: 0.16355961520568418\n",
      "Train Error: 0.07778443139959433, Test Error: 0.16341201089449237\n",
      "Train Error: 0.0778192640753818, Test Error: 0.16332445127427928\n",
      "Train Error: 0.07786166338593938, Test Error: 0.16324112266485905\n",
      "Train Error: 0.07774862249112573, Test Error: 0.16354283866988398\n",
      "Train Error: 0.07794127331662287, Test Error: 0.1633537236523561\n",
      "Train Error: 0.07805634664648001, Test Error: 0.16354639057120401\n",
      "Train Error: 0.07818514458691325, Test Error: 0.1637518371321849\n",
      "Train Error: 0.07780711248225125, Test Error: 0.16346184074454098\n",
      "Train Error: 0.07826602343008734, Test Error: 0.16389478737365287\n",
      "Train Error: 0.07857742490500376, Test Error: 0.164354159248184\n",
      "Train Error: 0.07894473035483256, Test Error: 0.16486929641517825\n",
      "Train Error: 0.07785608692300655, Test Error: 0.1633848990788992\n",
      "Train Error: 0.07867828426260577, Test Error: 0.16451815027603237\n",
      "Train Error: 0.07926816371958359, Test Error: 0.16532430649052093\n",
      "Train Error: 0.0799721625818691, Test Error: 0.16623974149476503\n",
      "Train Error: 0.07792033601346934, Test Error: 0.16336724345714815\n",
      "Train Error: 0.07917370842017106, Test Error: 0.16521843488248558\n",
      "Train Error: 0.08011511939215338, Test Error: 0.16644063140937862\n",
      "Train Error: 0.08125860127429398, Test Error: 0.16785369706667005\n",
      "Train Error: 0.07820424869424127, Test Error: 0.16390317518017106\n",
      "Train Error: 0.08198722894866238, Test Error: 0.16880090802387315\n",
      "Train Error: 0.08510848037592024, Test Error: 0.17242591246910782\n",
      "Train Error: 0.0888901580713308, Test Error: 0.17670073331729516\n",
      "Train Error: 0.07836835019694405, Test Error: 0.1641879042450496\n",
      "Train Error: 0.08389323385920064, Test Error: 0.1710613164723241\n",
      "Train Error: 0.08842562417839088, Test Error: 0.1762077119831928\n",
      "Train Error: 0.09386438871810793, Test Error: 0.18228634877805025\n",
      "Train Error: 0.11294669811629018, Test Error: 0.16829079289401\n",
      "Train Error: 0.11290661806288664, Test Error: 0.16606195660734263\n",
      "Train Error: 0.11290726392765905, Test Error: 0.16487302100368462\n",
      "Train Error: 0.11291526758832328, Test Error: 0.16371756913164065\n",
      "Train Error: 0.11293014736725256, Test Error: 0.1674767671130269\n",
      "Train Error: 0.11292452607496122, Test Error: 0.16222227181492307\n",
      "Train Error: 0.11293708282388365, Test Error: 0.16019595737435358\n",
      "Train Error: 0.11298031587400476, Test Error: 0.15820324181725168\n",
      "Train Error: 0.1129362340373915, Test Error: 0.16599868732101639\n",
      "Train Error: 0.11302685402018374, Test Error: 0.15766796779112474\n",
      "Train Error: 0.11318917983704078, Test Error: 0.15565180596644976\n",
      "Train Error: 0.11369707549806471, Test Error: 0.15436611927423702\n",
      "Train Error: 0.11295953924490026, Test Error: 0.16446779271145795\n",
      "Train Error: 0.11328020375211777, Test Error: 0.15518523951589788\n",
      "Train Error: 0.11417903493777545, Test Error: 0.15351963801565316\n",
      "Train Error: 0.11512507649606822, Test Error: 0.15211761063325005\n",
      "Train Error: 0.11298076847300265, Test Error: 0.16318255315765076\n",
      "Train Error: 0.11404797550217946, Test Error: 0.15376024480319186\n",
      "Train Error: 0.1153098187919417, Test Error: 0.1518938335788926\n",
      "Train Error: 0.11669215802330718, Test Error: 0.15028937861896358\n",
      "Train Error: 0.11309103186396419, Test Error: 0.15900428045853657\n",
      "Train Error: 0.11750694770599766, Test Error: 0.14961702395534154\n",
      "Train Error: 0.12068739714190771, Test Error: 0.1477605267222337\n",
      "Train Error: 0.12424133346693941, Test Error: 0.1472827374562242\n",
      "Train Error: 0.11318525390055145, Test Error: 0.1575317633195178\n",
      "Train Error: 0.11949504868501566, Test Error: 0.14829064849461066\n",
      "Train Error: 0.12382328905831659, Test Error: 0.14728826205495782\n",
      "Train Error: 0.12868741888225593, Test Error: 0.14833539657574304\n",
      "Train Error: 0.12129863819571199, Test Error: 8.83626353943999\n",
      "Train Error: 0.1214269178344561, Test Error: 9.191220959170462\n",
      "Train Error: 0.12150269195382306, Test Error: 9.376518215321397\n",
      "Train Error: 0.1215880994844737, Test Error: 9.559094251683355\n",
      "Train Error: 0.12135768044358562, Test Error: 8.966709282726104\n",
      "Train Error: 0.12181073916540856, Test Error: 9.895047130424844\n",
      "Train Error: 0.12223824812455593, Test Error: 10.386196840940663\n",
      "Train Error: 0.12269603255248367, Test Error: 10.87809113074534\n",
      "Train Error: 0.1214658506114274, Test Error: 9.205928038984386\n",
      "Train Error: 0.12296146733499147, Test Error: 11.134156821242916\n",
      "Train Error: 0.12374877873988574, Test Error: 11.757630144933728\n",
      "Train Error: 0.12444736953583527, Test Error: 12.144373023487192\n",
      "Train Error: 0.12158705622302243, Test Error: 9.44571441240111\n",
      "Train Error: 0.12394503381118206, Test Error: 11.860914284650898\n",
      "Train Error: 0.12502201340848457, Test Error: 12.44230994207042\n",
      "Train Error: 0.12618155469297318, Test Error: 13.026049997594347\n",
      "Train Error: 0.12172126723211513, Test Error: 9.685757393697415\n",
      "Train Error: 0.12486320790967406, Test Error: 12.351979266296052\n",
      "Train Error: 0.1264081225312812, Test Error: 13.131308176035565\n",
      "Train Error: 0.12809297516602425, Test Error: 13.914090019380039\n",
      "Train Error: 0.12264738710247779, Test Error: 10.695562422467434\n",
      "Train Error: 0.1290902726755756, Test Error: 14.340154461368824\n",
      "Train Error: 0.13296752472404572, Test Error: 15.933594025297333\n",
      "Train Error: 0.13730657992554285, Test Error: 17.543984328695085\n",
      "Train Error: 0.1231736034951739, Test Error: 11.20311220678477\n",
      "Train Error: 0.13151221208753883, Test Error: 15.348379694973769\n",
      "Train Error: 0.13679597968472976, Test Error: 17.358137652251404\n",
      "Train Error: 0.1427425688157489, Test Error: 19.380902073434974\n",
      "Train Error: 5.529069207776935, Test Error: 0.3888207734637564\n",
      "Train Error: 5.649884797670928, Test Error: 0.3812176283950947\n",
      "Train Error: 5.708766819389862, Test Error: 0.37777521452077534\n",
      "Train Error: 5.7673912812304, Test Error: 0.37436793646217675\n",
      "Train Error: 5.575621983536016, Test Error: 0.3856684411538286\n",
      "Train Error: 5.86905505202776, Test Error: 0.3685924618050231\n",
      "Train Error: 6.015178418441386, Test Error: 0.36058615653482745\n",
      "Train Error: 6.16093477751104, Test Error: 0.35292967722228424\n",
      "Train Error: 5.648141351513895, Test Error: 0.38138865064883076\n",
      "Train Error: 6.231724112783177, Test Error: 0.3493421503727966\n",
      "Train Error: 6.2635638315939675, Test Error: 0.35150037077215324\n",
      "Train Error: 6.299199334280154, Test Error: 0.3582053482694831\n",
      "Train Error: 5.720636954951097, Test Error: 0.3773481855759295\n",
      "Train Error: 6.270083471020648, Test Error: 0.3532921762360922\n",
      "Train Error: 6.321667634264949, Test Error: 0.3636350677710914\n",
      "Train Error: 6.369656522577902, Test Error: 0.37475349462147584\n",
      "Train Error: 5.792674357063304, Test Error: 0.373364742226333\n",
      "Train Error: 6.310903958904713, Test Error: 0.36202185253725555\n",
      "Train Error: 6.375056808372853, Test Error: 0.37686806982081295\n",
      "Train Error: 6.42061589054992, Test Error: 0.3939581108635132\n",
      "Train Error: 6.07834966841985, Test Error: 0.35782056390560313\n",
      "Train Error: 6.452719103994254, Test Error: 0.4032609088705761\n",
      "Train Error: 6.604339129318562, Test Error: 0.4396623828998501\n",
      "Train Error: 6.756682320458395, Test Error: 0.4789407201630372\n",
      "Train Error: 6.219850100181527, Test Error: 0.35061239030633407\n",
      "Train Error: 6.545228810227626, Test Error: 0.42612318338821004\n",
      "Train Error: 6.736864173904218, Test Error: 0.4745270515924907\n",
      "Train Error: 6.929226104401406, Test Error: 0.5263073533117082\n",
      "Train Error: 5.99958326909731, Test Error: 0.22509847689153942\n",
      "Train Error: 5.9226534193961795, Test Error: 0.22517401777207866\n",
      "Train Error: 5.885605061641107, Test Error: 0.22528786964056938\n",
      "Train Error: 5.841557294129787, Test Error: 0.2257738798611377\n",
      "Train Error: 5.9664424680989505, Test Error: 0.22498935305613935\n",
      "Train Error: 5.765841968842048, Test Error: 0.22599029209244859\n",
      "Train Error: 5.671573903494314, Test Error: 0.22647267266938614\n",
      "Train Error: 5.617512619751887, Test Error: 0.22631121191292067\n",
      "Train Error: 5.911485733152022, Test Error: 0.22486724711052017\n",
      "Train Error: 5.583804016900977, Test Error: 0.22604526845000145\n",
      "Train Error: 5.475032938828545, Test Error: 0.22571820562620887\n",
      "Train Error: 5.368973685855504, Test Error: 0.22483323736936076\n",
      "Train Error: 5.857090771074225, Test Error: 0.22484851896247493\n",
      "Train Error: 5.442447974667731, Test Error: 0.22488633776279113\n",
      "Train Error: 5.282895605481323, Test Error: 0.2243648685401192\n",
      "Train Error: 5.123328088377969, Test Error: 0.2238868765857896\n",
      "Train Error: 5.799347779584435, Test Error: 0.22487485438404903\n",
      "Train Error: 5.303063683694929, Test Error: 0.22424664571917843\n",
      "Train Error: 5.090477845627177, Test Error: 0.2235953057824699\n",
      "Train Error: 4.878340544945525, Test Error: 0.22297416393372715\n",
      "Train Error: 5.599880928128637, Test Error: 0.22503318606242745\n",
      "Train Error: 4.74636918984366, Test Error: 0.22188500239125453\n",
      "Train Error: 4.323966950579938, Test Error: 0.22077219848854224\n",
      "Train Error: 3.905208901721939, Test Error: 0.21975263296819803\n",
      "Train Error: 5.532966349857974, Test Error: 0.22448945361600145\n",
      "Train Error: 4.469002555355474, Test Error: 0.22078121330548447\n",
      "Train Error: 3.944100794503582, Test Error: 0.21948397786248938\n",
      "Train Error: 3.424615384539345, Test Error: 0.2189145530851397\n",
      "Train Error: 3.3520206806540274, Test Error: 0.12368783625420024\n",
      "Train Error: 3.2855319481984666, Test Error: 0.12360431229637284\n",
      "Train Error: 3.2386541855712974, Test Error: 0.12358060384015401\n",
      "Train Error: 3.185589893657014, Test Error: 0.12359588821791972\n",
      "Train Error: 3.330800107896398, Test Error: 0.12361964321339027\n",
      "Train Error: 3.096020160068102, Test Error: 0.12351283606604484\n",
      "Train Error: 2.9749218695354425, Test Error: 0.12343759697288947\n",
      "Train Error: 2.8725680053534486, Test Error: 0.12338395644126332\n",
      "Train Error: 3.273626850953575, Test Error: 0.12352975865874515\n",
      "Train Error: 2.8340896816993038, Test Error: 0.1233202700572237\n",
      "Train Error: 2.7071483320533307, Test Error: 0.12325708749596884\n",
      "Train Error: 2.5803243762764403, Test Error: 0.12319291446528434\n",
      "Train Error: 3.2069187169098967, Test Error: 0.123453671056456\n",
      "Train Error: 2.6686912324719727, Test Error: 0.12319418567482288\n",
      "Train Error: 2.4789291380962752, Test Error: 0.12310353711270369\n",
      "Train Error: 2.29194009743529, Test Error: 0.12300657682833563\n",
      "Train Error: 3.133649956985964, Test Error: 0.12341337168119187\n",
      "Train Error: 2.503824150478288, Test Error: 0.1230744439793102\n",
      "Train Error: 2.2542644500444857, Test Error: 0.12294722555492893\n",
      "Train Error: 2.0036593414163755, Test Error: 0.12288256537963273\n",
      "Train Error: 2.8573647141477294, Test Error: 0.12308208446629339\n",
      "Train Error: 1.8535433379614998, Test Error: 0.122669194265002\n",
      "Train Error: 1.3752448697098543, Test Error: 0.12261808434400227\n",
      "Train Error: 0.9080834934512654, Test Error: 0.12267286279087768\n",
      "Train Error: 2.780207902315216, Test Error: 0.12295156025160918\n",
      "Train Error: 1.5325468462585734, Test Error: 0.12253996050196006\n",
      "Train Error: 0.9523643838284843, Test Error: 0.12256743736092444\n",
      "Train Error: 0.48627800851934283, Test Error: 0.12274375335004159\n",
      "Train Error: 1.2184051652222063, Test Error: 0.10724206668946096\n",
      "Train Error: 1.235000980610259, Test Error: 0.10729492375030329\n",
      "Train Error: 1.2533867539158368, Test Error: 0.10740028164122147\n",
      "Train Error: 1.235572038524903, Test Error: 0.1074317323046188\n",
      "Train Error: 1.231270861781039, Test Error: 0.10720886080087705\n",
      "Train Error: 1.2014061965477236, Test Error: 0.10746038865704889\n",
      "Train Error: 1.1585162043493002, Test Error: 0.10754328508440533\n",
      "Train Error: 1.1175161328270666, Test Error: 0.10763097852595883\n",
      "Train Error: 1.224958123712222, Test Error: 0.10722095030347893\n",
      "Train Error: 1.0785041368827897, Test Error: 0.10761483270044346\n",
      "Train Error: 0.9682743954768862, Test Error: 0.10772715762997731\n",
      "Train Error: 0.9055732830267693, Test Error: 0.10794428138574069\n",
      "Train Error: 1.2346235554276312, Test Error: 0.1072907555717101\n",
      "Train Error: 0.9474618436691618, Test Error: 0.10773512833399053\n",
      "Train Error: 0.8587766807209094, Test Error: 0.10807299066943811\n",
      "Train Error: 0.7873731216923774, Test Error: 0.10844416245544311\n",
      "Train Error: 1.2066882378492152, Test Error: 0.10728576032720007\n",
      "Train Error: 0.8685449137199812, Test Error: 0.1079699694494711\n",
      "Train Error: 0.7749342513200665, Test Error: 0.1084669229422776\n",
      "Train Error: 0.7225701947162213, Test Error: 0.10903085735547353\n",
      "Train Error: 1.102523305825026, Test Error: 0.10727806137036504\n",
      "Train Error: 0.7133537393264519, Test Error: 0.1091708103402297\n",
      "Train Error: 0.8074852430533874, Test Error: 0.11056173967648598\n",
      "Train Error: 0.9922841649078131, Test Error: 0.11223097827959753\n",
      "Train Error: 1.0520606300907582, Test Error: 0.10728194648704385\n",
      "Train Error: 0.7608528319183677, Test Error: 0.10992173850636958\n",
      "Train Error: 1.0230424148114468, Test Error: 0.11187915069906758\n",
      "Train Error: 1.3746219675148896, Test Error: 0.11428484758698983\n",
      "Train Error: 0.16600326849978386, Test Error: 0.1475363788734605\n",
      "Train Error: 0.16444190525132243, Test Error: 0.14789763116218582\n",
      "Train Error: 0.1638469891026551, Test Error: 0.14808304907558906\n",
      "Train Error: 0.16325047281996038, Test Error: 0.1482707731141218\n",
      "Train Error: 0.1653908127093544, Test Error: 0.1476760595671987\n",
      "Train Error: 0.16213588209122468, Test Error: 0.14863111792589204\n",
      "Train Error: 0.1607357835974023, Test Error: 0.14909509375725696\n",
      "Train Error: 0.15934949075005583, Test Error: 0.14956570438744451\n",
      "Train Error: 0.1643248926598485, Test Error: 0.14795449055412835\n",
      "Train Error: 0.15861882058693325, Test Error: 0.14983131357291204\n",
      "Train Error: 0.1558792552225169, Test Error: 0.15079907504817178\n",
      "Train Error: 0.15323417262045325, Test Error: 0.1517735906200665\n",
      "Train Error: 0.16344759829494349, Test Error: 0.14823190044838072\n",
      "Train Error: 0.15515694067154406, Test Error: 0.15107362213677805\n",
      "Train Error: 0.15123619296893903, Test Error: 0.15254563525028364\n",
      "Train Error: 0.14741075356342045, Test Error: 0.15406308426605206\n",
      "Train Error: 0.16264847012737219, Test Error: 0.1485009165894846\n",
      "Train Error: 0.15180685862329826, Test Error: 0.15233739031873184\n",
      "Train Error: 0.14672566037934082, Test Error: 0.1543560805018231\n",
      "Train Error: 0.14189615111253456, Test Error: 0.1565699017809468\n",
      "Train Error: 0.1594993458502443, Test Error: 0.14959127144287004\n",
      "Train Error: 0.13929733356870866, Test Error: 0.1577979569737128\n",
      "Train Error: 0.13083745549940504, Test Error: 0.1622351893954714\n",
      "Train Error: 0.12389903975906788, Test Error: 0.16685730370534746\n",
      "Train Error: 0.15797548356680377, Test Error: 0.1501411042301577\n",
      "Train Error: 0.1337368414626159, Test Error: 0.16062717130207854\n",
      "Train Error: 0.12460019539050603, Test Error: 0.16635537511893522\n",
      "Train Error: 0.11772012086176663, Test Error: 0.1723928157214297\n",
      "Percentile: 0.3, Tracking Error: 0.7542386044468673\n",
      "Starting percentile: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.292e-04, tolerance: 6.305e-04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: 0.0016524470032929877, Test Error: 0.579875389355997\n",
      "Train Error: 0.005325866782202956, Test Error: 0.521556691028967\n",
      "Train Error: 0.006846852046393477, Test Error: 0.5299617265217803\n",
      "Train Error: 0.007853277136874682, Test Error: 0.5337419614437355\n",
      "Train Error: 0.003914339011311105, Test Error: 0.49995689950310035\n",
      "Train Error: 0.010171550563868037, Test Error: 0.5354239393568264\n",
      "Train Error: 0.013550041815943048, Test Error: 0.5441981328990654\n",
      "Train Error: 0.017018115596567705, Test Error: 0.5480897941179637\n",
      "Train Error: 0.005932677686176687, Test Error: 0.5090375479991878\n",
      "Train Error: 0.018964228545889635, Test Error: 0.5491752500140145\n",
      "Train Error: 0.024037558694460524, Test Error: 0.5522420495652072\n",
      "Train Error: 0.028427448465510558, Test Error: 0.5481421302984939\n",
      "Train Error: 0.0081091725561463, Test Error: 0.5204639587708639\n",
      "Train Error: 0.025302387207582263, Test Error: 0.5506930439441795\n",
      "Train Error: 0.029035100421358668, Test Error: 0.5501562265891992\n",
      "Train Error: 0.030434082274147962, Test Error: 0.5616199531240109\n",
      "Train Error: 0.009762859192798139, Test Error: 0.5132057461218839\n",
      "Train Error: 0.028819867061427307, Test Error: 0.5494565159805395\n",
      "Train Error: 0.03070099891924141, Test Error: 0.5634636433990164\n",
      "Train Error: 0.03278526422669502, Test Error: 0.5770571866795277\n",
      "Train Error: 0.016644053817050735, Test Error: 0.5209512760763972\n",
      "Train Error: 0.03401940352876816, Test Error: 0.5797112701667959\n",
      "Train Error: 0.03855966316865226, Test Error: 0.6052644113510108\n",
      "Train Error: 0.04243955566024981, Test Error: 0.6225351653567424\n",
      "Train Error: 0.01991876011047123, Test Error: 0.5317385216155098\n",
      "Train Error: 0.036908233954873866, Test Error: 0.5935840617756223\n",
      "Train Error: 0.041991714801680256, Test Error: 0.6184949784871628\n",
      "Train Error: 0.046579820239929036, Test Error: 0.6326654900439941\n",
      "Train Error: 0.15383230293761574, Test Error: 0.2741348182783872\n",
      "Train Error: 0.1538891120827225, Test Error: 0.27382079021609684\n",
      "Train Error: 0.153937771401968, Test Error: 0.27374836144349024\n",
      "Train Error: 0.15398751623651546, Test Error: 0.27366847354260543\n",
      "Train Error: 0.15382892767133452, Test Error: 0.2739097417819882\n",
      "Train Error: 0.15407728437395352, Test Error: 0.27353758671661926\n",
      "Train Error: 0.15416087502016113, Test Error: 0.2733668453831233\n",
      "Train Error: 0.15413726954611429, Test Error: 0.2732157223368465\n",
      "Train Error: 0.1538850139920322, Test Error: 0.27379922832493164\n",
      "Train Error: 0.15412434299866876, Test Error: 0.27313329587524865\n",
      "Train Error: 0.15412252987377667, Test Error: 0.2728205904459671\n",
      "Train Error: 0.15416641683213125, Test Error: 0.27250500227518093\n",
      "Train Error: 0.1539433935475538, Test Error: 0.27368627491376907\n",
      "Train Error: 0.1541253609117019, Test Error: 0.27272537723535234\n",
      "Train Error: 0.15422309449597071, Test Error: 0.2722849903463482\n",
      "Train Error: 0.15447231226834862, Test Error: 0.27186504012890295\n",
      "Train Error: 0.1540046297462356, Test Error: 0.27358448936323526\n",
      "Train Error: 0.15419691573447947, Test Error: 0.27233721492357893\n",
      "Train Error: 0.15455673078265564, Test Error: 0.2717343417745896\n",
      "Train Error: 0.15530019698921393, Test Error: 0.2708872426149975\n",
      "Train Error: 0.15411343076468093, Test Error: 0.27319095444582175\n",
      "Train Error: 0.15569856673495627, Test Error: 0.27043066156161716\n",
      "Train Error: 0.15756839194774142, Test Error: 0.26923261166537665\n",
      "Train Error: 0.15980038588140222, Test Error: 0.2685086080939393\n",
      "Train Error: 0.15408731999869787, Test Error: 0.27302308874004205\n",
      "Train Error: 0.15681367776004915, Test Error: 0.26956929810715297\n",
      "Train Error: 0.1594995384125746, Test Error: 0.26852876880566523\n",
      "Train Error: 0.16281680088716388, Test Error: 0.2675023609976706\n",
      "Train Error: 0.19185507031929264, Test Error: 0.1901753983492879\n",
      "Train Error: 0.19181448072657745, Test Error: 0.19041174357472693\n",
      "Train Error: 0.1917896472144175, Test Error: 0.1905167037110748\n",
      "Train Error: 0.19176918979814936, Test Error: 0.19062179333428378\n",
      "Train Error: 0.191831309378844, Test Error: 0.19028600863894884\n",
      "Train Error: 0.19172300761952796, Test Error: 0.1908234402673731\n",
      "Train Error: 0.19168429473223114, Test Error: 0.1910891283857162\n",
      "Train Error: 0.19164652363515067, Test Error: 0.19137157569968738\n",
      "Train Error: 0.19179359700409768, Test Error: 0.1904429467133299\n",
      "Train Error: 0.1916218461375971, Test Error: 0.19151690190707005\n",
      "Train Error: 0.19155343085282137, Test Error: 0.19223304198824803\n",
      "Train Error: 0.19148402443041432, Test Error: 0.1931897314362595\n",
      "Train Error: 0.19175296528680622, Test Error: 0.19059294441636707\n",
      "Train Error: 0.19152071726473222, Test Error: 0.1924947713885079\n",
      "Train Error: 0.19145045646990277, Test Error: 0.1941454890280144\n",
      "Train Error: 0.1914418768406983, Test Error: 0.19603263048797684\n",
      "Train Error: 0.19171119149806545, Test Error: 0.19074632727629337\n",
      "Train Error: 0.19144410350995344, Test Error: 0.19385344418731523\n",
      "Train Error: 0.1914324771309035, Test Error: 0.19637405689437631\n",
      "Train Error: 0.19149961841212815, Test Error: 0.1989391049327162\n",
      "Train Error: 0.1915768598449607, Test Error: 0.19136754757460128\n",
      "Train Error: 0.19151411038862146, Test Error: 0.20033113518072315\n",
      "Train Error: 0.1918650524291056, Test Error: 0.20531642473242623\n",
      "Train Error: 0.19249338334065133, Test Error: 0.21036794653560467\n",
      "Train Error: 0.19152107409004943, Test Error: 0.19168366235420042\n",
      "Train Error: 0.19168684311866496, Test Error: 0.20350330816476098\n",
      "Train Error: 0.19238028286539688, Test Error: 0.20978706673492598\n",
      "Train Error: 0.19352873997602651, Test Error: 0.21623424137079392\n",
      "Train Error: 0.16451605101155453, Test Error: 0.1618114817831847\n",
      "Train Error: 0.16460556931855438, Test Error: 0.16288806136778655\n",
      "Train Error: 0.1646516411821008, Test Error: 0.16337069123740097\n",
      "Train Error: 0.16469602209081496, Test Error: 0.163871714498175\n",
      "Train Error: 0.16454416212987213, Test Error: 0.16223616884719108\n",
      "Train Error: 0.16477210036497547, Test Error: 0.16476283873670838\n",
      "Train Error: 0.16489264191916456, Test Error: 0.16601019723372554\n",
      "Train Error: 0.16501880592113638, Test Error: 0.16727372848502484\n",
      "Train Error: 0.16459453308702898, Test Error: 0.16291983901569887\n",
      "Train Error: 0.16507795181930204, Test Error: 0.1679269449271542\n",
      "Train Error: 0.1653560128557714, Test Error: 0.17045677357522238\n",
      "Train Error: 0.16565822691612073, Test Error: 0.173013886698525\n",
      "Train Error: 0.16464497823298174, Test Error: 0.16355027567889843\n",
      "Train Error: 0.16542336656721976, Test Error: 0.17111502190767164\n",
      "Train Error: 0.16589546509157846, Test Error: 0.1749593352767765\n",
      "Train Error: 0.16642248327013487, Test Error: 0.178818237329466\n",
      "Train Error: 0.16469444668826544, Test Error: 0.1641970568282272\n",
      "Train Error: 0.16580743422086502, Test Error: 0.17433662696560978\n",
      "Train Error: 0.16651052324290358, Test Error: 0.17948489916029908\n",
      "Train Error: 0.16730888841529853, Test Error: 0.18468114684534076\n",
      "Train Error: 0.16491060497637125, Test Error: 0.16678610959663212\n",
      "Train Error: 0.16772636854038903, Test Error: 0.18735102574635085\n",
      "Train Error: 0.1696959192299518, Test Error: 0.19780412680734663\n",
      "Train Error: 0.17199080420447108, Test Error: 0.20831619353798614\n",
      "Train Error: 0.16502939146068396, Test Error: 0.16807956424251463\n",
      "Train Error: 0.16890774310586565, Test Error: 0.1939096301583125\n",
      "Train Error: 0.17168280881819087, Test Error: 0.20702836805285568\n",
      "Train Error: 0.1749363267247667, Test Error: 0.22004692097948697\n",
      "Train Error: 0.13290107463432463, Test Error: 0.11182895035935786\n",
      "Train Error: 0.13334631688518958, Test Error: 0.11176497679323222\n",
      "Train Error: 0.1335520847499424, Test Error: 0.11173306524591892\n",
      "Train Error: 0.13375681429281516, Test Error: 0.11169916631886\n",
      "Train Error: 0.1330910067240809, Test Error: 0.11180076281965733\n",
      "Train Error: 0.13413494215000635, Test Error: 0.11163588991461162\n",
      "Train Error: 0.13466463361452546, Test Error: 0.11154839878363114\n",
      "Train Error: 0.13520169761855538, Test Error: 0.11146270635086422\n",
      "Train Error: 0.13338785104737091, Test Error: 0.11175710794562516\n",
      "Train Error: 0.1354962744232397, Test Error: 0.11141766975615369\n",
      "Train Error: 0.1366024956338484, Test Error: 0.1112535879384475\n",
      "Train Error: 0.13773754831999577, Test Error: 0.11109650673828915\n",
      "Train Error: 0.13367016580874133, Test Error: 0.11171351259774454\n",
      "Train Error: 0.13691198538732824, Test Error: 0.11120918888882901\n",
      "Train Error: 0.13863623259227312, Test Error: 0.1109789404807209\n",
      "Train Error: 0.14041688784751313, Test Error: 0.11076587324694445\n",
      "Train Error: 0.13395336334258104, Test Error: 0.11166592928863621\n",
      "Train Error: 0.13837341882698106, Test Error: 0.11101193624502145\n",
      "Train Error: 0.14075007333049874, Test Error: 0.11072756941637248\n",
      "Train Error: 0.14323223948125244, Test Error: 0.11046961935438415\n",
      "Train Error: 0.13511962689538118, Test Error: 0.11147489480860176\n",
      "Train Error: 0.14462697804759256, Test Error: 0.11033777430559964\n",
      "Train Error: 0.14994248156306453, Test Error: 0.10993650837457541\n",
      "Train Error: 0.15554426838818555, Test Error: 0.10965001361517293\n",
      "Train Error: 0.13571857806968726, Test Error: 0.11138168616460638\n",
      "Train Error: 0.14797644112665476, Test Error: 0.11006698012941829\n",
      "Train Error: 0.15489026258443672, Test Error: 0.10967452147588501\n",
      "Train Error: 0.16211417595620803, Test Error: 0.10945194954192031\n",
      "Train Error: 0.11580204740518645, Test Error: 0.11058299130279789\n",
      "Train Error: 0.11626147619820941, Test Error: 0.11076064484268737\n",
      "Train Error: 0.1164692707184975, Test Error: 0.1108355202990976\n",
      "Train Error: 0.11668456218646504, Test Error: 0.11091852782255236\n",
      "Train Error: 0.1159973338393622, Test Error: 0.1106554115074697\n",
      "Train Error: 0.11708288467019425, Test Error: 0.11107157340532742\n",
      "Train Error: 0.11762818157515796, Test Error: 0.11129204397048498\n",
      "Train Error: 0.11818757109746603, Test Error: 0.11152814074753531\n",
      "Train Error: 0.11630887293013252, Test Error: 0.11076791686424275\n",
      "Train Error: 0.11850223414840753, Test Error: 0.11165775666127416\n",
      "Train Error: 0.11965461545857427, Test Error: 0.11217737740673166\n",
      "Train Error: 0.12083342102463036, Test Error: 0.1127391114118166\n",
      "Train Error: 0.11660221091133283, Test Error: 0.1108697581994828\n",
      "Train Error: 0.1199752946805474, Test Error: 0.11231942862097885\n",
      "Train Error: 0.12177571602688664, Test Error: 0.11320337319493669\n",
      "Train Error: 0.1236424529627208, Test Error: 0.11418780702366862\n",
      "Train Error: 0.1169031000392936, Test Error: 0.11098067736713219\n",
      "Train Error: 0.1215050129736669, Test Error: 0.11305929452481994\n",
      "Train Error: 0.12398893741189478, Test Error: 0.11436959891562534\n",
      "Train Error: 0.12659254398201672, Test Error: 0.11585523041565639\n",
      "Train Error: 0.11812858934546788, Test Error: 0.11145683664166997\n",
      "Train Error: 0.12813958086125285, Test Error: 0.11673861800010563\n",
      "Train Error: 0.13395535599656533, Test Error: 0.12042894527114824\n",
      "Train Error: 0.140053513023713, Test Error: 0.12469687585802279\n",
      "Train Error: 0.11875379454727433, Test Error: 0.1117141614672337\n",
      "Train Error: 0.13181080720012686, Test Error: 0.1190078620298089\n",
      "Train Error: 0.1393575699555186, Test Error: 0.12417794946110813\n",
      "Train Error: 0.14721075542987505, Test Error: 0.13012640816879323\n",
      "Train Error: 0.1616228933426326, Test Error: 0.14775474659377055\n",
      "Train Error: 0.16189384287206277, Test Error: 0.14771329211848444\n",
      "Train Error: 0.16203712965035535, Test Error: 0.14769045498537473\n",
      "Train Error: 0.16217468879270783, Test Error: 0.14766673395768237\n",
      "Train Error: 0.16171507009479993, Test Error: 0.1477339204016554\n",
      "Train Error: 0.16243527704458574, Test Error: 0.14762159432310973\n",
      "Train Error: 0.16279324131633638, Test Error: 0.14756841814633728\n",
      "Train Error: 0.16316272883888788, Test Error: 0.14751990343172205\n",
      "Train Error: 0.16191327846622253, Test Error: 0.14769404340392336\n",
      "Train Error: 0.1633625396526115, Test Error: 0.14748524093693738\n",
      "Train Error: 0.16411409634456567, Test Error: 0.1473998346624047\n",
      "Train Error: 0.16489097314420448, Test Error: 0.14732886382634405\n",
      "Train Error: 0.16209862191567034, Test Error: 0.14765232817665253\n",
      "Train Error: 0.1643205923080884, Test Error: 0.14736950884046476\n",
      "Train Error: 0.16549373498569253, Test Error: 0.14727158757374298\n",
      "Train Error: 0.1667133202711084, Test Error: 0.1472041903744636\n",
      "Train Error: 0.1622917799834448, Test Error: 0.14761287719937008\n",
      "Train Error: 0.16530898094186458, Test Error: 0.14727454770422346\n",
      "Train Error: 0.16693609484292984, Test Error: 0.14718506988123178\n",
      "Train Error: 0.1686264667206711, Test Error: 0.14714629616945532\n",
      "Train Error: 0.16306870530042678, Test Error: 0.14746208121973428\n",
      "Train Error: 0.16956845225526948, Test Error: 0.14710659713036744\n",
      "Train Error: 0.17321314038957009, Test Error: 0.14724009002659502\n",
      "Train Error: 0.1770985129269768, Test Error: 0.14758275900934148\n",
      "Train Error: 0.16346929537857433, Test Error: 0.14739294762770966\n",
      "Train Error: 0.1718499399823506, Test Error: 0.14714534244264624\n",
      "Train Error: 0.17664196225761805, Test Error: 0.14751096279212889\n",
      "Train Error: 0.18174800468474805, Test Error: 0.14819411992256315\n",
      "Train Error: 0.11733062208140695, Test Error: 0.2160714161382785\n",
      "Train Error: 0.11735580040414073, Test Error: 0.2158826828050482\n",
      "Train Error: 0.1173743334954014, Test Error: 0.21579448737629586\n",
      "Train Error: 0.11739682069332567, Test Error: 0.21571128681040777\n",
      "Train Error: 0.11734114276951718, Test Error: 0.21601627119773983\n",
      "Train Error: 0.11744758177761454, Test Error: 0.21559814157899934\n",
      "Train Error: 0.11753583910500776, Test Error: 0.2154387484549153\n",
      "Train Error: 0.11769066307257652, Test Error: 0.21537260253342436\n",
      "Train Error: 0.11736519724663506, Test Error: 0.21592950270555566\n",
      "Train Error: 0.11777168286644174, Test Error: 0.2154121357616768\n",
      "Train Error: 0.1180879104574416, Test Error: 0.21547013398397769\n",
      "Train Error: 0.11843588184415985, Test Error: 0.21556706449691035\n",
      "Train Error: 0.11739516904260602, Test Error: 0.21585015641288852\n",
      "Train Error: 0.1181798251613916, Test Error: 0.21552322847817562\n",
      "Train Error: 0.11873147262951415, Test Error: 0.21568520949429842\n",
      "Train Error: 0.11934908613140946, Test Error: 0.2159469109128809\n",
      "Train Error: 0.11743168074814637, Test Error: 0.2157793261576416\n",
      "Train Error: 0.11864465423407992, Test Error: 0.21568201266582027\n",
      "Train Error: 0.11946915766444346, Test Error: 0.21603139122484122\n",
      "Train Error: 0.12041817553343202, Test Error: 0.21652970606983943\n",
      "Train Error: 0.11765837295207027, Test Error: 0.2156150556587848\n",
      "Train Error: 0.12098657539956166, Test Error: 0.2169620250067447\n",
      "Train Error: 0.12336627052336849, Test Error: 0.21853844963430846\n",
      "Train Error: 0.12618708704512616, Test Error: 0.22066096735387827\n",
      "Train Error: 0.1178343836656985, Test Error: 0.2156639774401261\n",
      "Train Error: 0.1224468700247282, Test Error: 0.2179481084002595\n",
      "Train Error: 0.1258425578281585, Test Error: 0.22043361811825296\n",
      "Train Error: 0.12987883020639737, Test Error: 0.22374385055954113\n",
      "Train Error: 0.1459629493292028, Test Error: 0.16576350630356695\n",
      "Train Error: 0.14606288370673284, Test Error: 0.16438298397642986\n",
      "Train Error: 0.146114344393502, Test Error: 0.16370162354493775\n",
      "Train Error: 0.14616598176465992, Test Error: 0.1630336000738693\n",
      "Train Error: 0.14600637298760605, Test Error: 0.16521471220043782\n",
      "Train Error: 0.14626667435127877, Test Error: 0.1618356490894706\n",
      "Train Error: 0.1464053377184491, Test Error: 0.16019765169787445\n",
      "Train Error: 0.1465507205584619, Test Error: 0.1585825710327588\n",
      "Train Error: 0.1460803553546559, Test Error: 0.164308228132926\n",
      "Train Error: 0.14663757251724652, Test Error: 0.15774095523770496\n",
      "Train Error: 0.1469563506333861, Test Error: 0.15460026838460214\n",
      "Train Error: 0.14730165376497284, Test Error: 0.15156763272151466\n",
      "Train Error: 0.14615581563146154, Test Error: 0.1634141218178009\n",
      "Train Error: 0.14705203527079727, Test Error: 0.15379267843368674\n",
      "Train Error: 0.14760915373126765, Test Error: 0.14940792318411011\n",
      "Train Error: 0.1482358217602202, Test Error: 0.14539792412534572\n",
      "Train Error: 0.146233266180673, Test Error: 0.16252612064419056\n",
      "Train Error: 0.14752252539815294, Test Error: 0.15005584344653028\n",
      "Train Error: 0.14835868624630968, Test Error: 0.14471411542447596\n",
      "Train Error: 0.14929752169571203, Test Error: 0.13979138975625405\n",
      "Train Error: 0.14655805644336914, Test Error: 0.15908106780231804\n",
      "Train Error: 0.14985961164827374, Test Error: 0.13738010311019175\n",
      "Train Error: 0.1521349640864578, Test Error: 0.12952296362724344\n",
      "Train Error: 0.1547821159446555, Test Error: 0.1239174078277899\n",
      "Train Error: 0.14673278911601975, Test Error: 0.15738981003577976\n",
      "Train Error: 0.1512654411815804, Test Error: 0.1321727730748193\n",
      "Train Error: 0.15446286604244377, Test Error: 0.12446493155561113\n",
      "Train Error: 0.1582029655321845, Test Error: 0.12061970006302111\n",
      "Train Error: 0.3291404662023813, Test Error: 20.796276180423106\n",
      "Train Error: 0.32812517359844934, Test Error: 21.1310522743785\n",
      "Train Error: 0.32745168322633644, Test Error: 21.286539944879955\n",
      "Train Error: 0.32690556151162947, Test Error: 21.45039841633947\n",
      "Train Error: 0.3286725702032986, Test Error: 20.92498738158964\n",
      "Train Error: 0.3260909990519171, Test Error: 21.75460907148416\n",
      "Train Error: 0.3251833910478188, Test Error: 21.96656380223223\n",
      "Train Error: 0.32430388721872677, Test Error: 22.167776970165498\n",
      "Train Error: 0.3281088452383464, Test Error: 21.153453557468733\n",
      "Train Error: 0.3238623000015582, Test Error: 22.280144048321272\n",
      "Train Error: 0.32216197495053306, Test Error: 22.67875275671523\n",
      "Train Error: 0.3204792133779925, Test Error: 23.075794471861283\n",
      "Train Error: 0.32728926249031637, Test Error: 21.36365553783223\n",
      "Train Error: 0.3217389711127935, Test Error: 22.789067676704466\n",
      "Train Error: 0.31922713342110864, Test Error: 23.38786573430571\n",
      "Train Error: 0.3167800653894223, Test Error: 23.984881563883434\n",
      "Train Error: 0.32673359477142727, Test Error: 21.591101624593982\n",
      "Train Error: 0.31963890404762646, Test Error: 23.29974409289885\n",
      "Train Error: 0.31637618060225936, Test Error: 24.096351613573756\n",
      "Train Error: 0.3132280725165284, Test Error: 24.891779229964392\n",
      "Train Error: 0.32470939446504854, Test Error: 22.150562919513376\n",
      "Train Error: 0.3116940561727182, Test Error: 25.333491084143905\n",
      "Train Error: 0.3058819257125266, Test Error: 26.91026081902497\n",
      "Train Error: 0.3005647922293425, Test Error: 28.47631814912082\n",
      "Train Error: 0.32384162945052636, Test Error: 22.374072378220312\n",
      "Train Error: 0.308003139496589, Test Error: 26.343023224445254\n",
      "Train Error: 0.3012051975346891, Test Error: 28.30427349250267\n",
      "Train Error: 0.2950002013139562, Test Error: 30.339788522642213\n",
      "Train Error: 3.6431098870089977, Test Error: 1.2245624620404674\n",
      "Train Error: 3.3958898466561407, Test Error: 1.2264532286897651\n",
      "Train Error: 3.2802994921595063, Test Error: 1.226277241514403\n",
      "Train Error: 3.161519699016162, Test Error: 1.2267490191983395\n",
      "Train Error: 3.5391858565851466, Test Error: 1.2262513994272075\n",
      "Train Error: 2.9985439059013266, Test Error: 1.228704098582586\n",
      "Train Error: 2.8935898013677317, Test Error: 1.2488296894696846\n",
      "Train Error: 2.781550018466017, Test Error: 1.2691901419234661\n",
      "Train Error: 3.3844217798738487, Test Error: 1.2263777302452983\n",
      "Train Error: 2.7222852955673376, Test Error: 1.2793044321180391\n",
      "Train Error: 2.543938202651499, Test Error: 1.3094838578317098\n",
      "Train Error: 2.390626155142324, Test Error: 1.3256384252065911\n",
      "Train Error: 3.2304823584456046, Test Error: 1.2265998865305743\n",
      "Train Error: 2.508896826529748, Test Error: 1.313431062955524\n",
      "Train Error: 2.2428298476295754, Test Error: 1.3394736720678206\n",
      "Train Error: 2.030385508484834, Test Error: 1.364407875923377\n",
      "Train Error: 3.0773124790760327, Test Error: 1.2268994801491127\n",
      "Train Error: 2.2808335770290156, Test Error: 1.3352570688758771\n",
      "Train Error: 1.9951448719713512, Test Error: 1.3684256015234892\n",
      "Train Error: 1.7771622225100565, Test Error: 1.387406082495921\n",
      "Train Error: 2.80107622918306, Test Error: 1.2598613045960498\n",
      "Train Error: 1.6670778384219158, Test Error: 1.3938939902205887\n",
      "Train Error: 1.2887624030640747, Test Error: 1.4215339040049153\n",
      "Train Error: 0.9080806391003319, Test Error: 1.4493971023037824\n",
      "Train Error: 2.6791488233043625, Test Error: 1.2803566320707618\n",
      "Train Error: 1.424967737472346, Test Error: 1.4109575177526585\n",
      "Train Error: 0.944529436808823, Test Error: 1.4457444633562244\n",
      "Train Error: 0.6647296394037907, Test Error: 1.4804938663888867\n",
      "Train Error: 10.434448837032589, Test Error: 0.32096622691389065\n",
      "Train Error: 10.401196333257035, Test Error: 0.3203965047074473\n",
      "Train Error: 10.406270461695259, Test Error: 0.31917283542055136\n",
      "Train Error: 10.390499831362071, Test Error: 0.3187764208134581\n",
      "Train Error: 10.421142616773645, Test Error: 0.3205077370160291\n",
      "Train Error: 10.35606952752863, Test Error: 0.31804238664747075\n",
      "Train Error: 10.308768320195025, Test Error: 0.31749748395762806\n",
      "Train Error: 10.261522564151159, Test Error: 0.31695000100863424\n",
      "Train Error: 10.388437186984957, Test Error: 0.32006176215460397\n",
      "Train Error: 10.230449865949495, Test Error: 0.31643013278189264\n",
      "Train Error: 10.131327075725185, Test Error: 0.3154169307571725\n",
      "Train Error: 10.029149407883843, Test Error: 0.31447016108108666\n",
      "Train Error: 10.382724091629742, Test Error: 0.3184930334073928\n",
      "Train Error: 10.098748295968742, Test Error: 0.3149870702628675\n",
      "Train Error: 9.945160586161888, Test Error: 0.3135429431910226\n",
      "Train Error: 9.791234296825323, Test Error: 0.3121375184696236\n",
      "Train Error: 10.355683379140563, Test Error: 0.3177444621828671\n",
      "Train Error: 9.963863264061592, Test Error: 0.31353475895486993\n",
      "Train Error: 9.758367971191868, Test Error: 0.31165606404952356\n",
      "Train Error: 9.547606788147489, Test Error: 0.30992776384099113\n",
      "Train Error: 10.232294897524847, Test Error: 0.31563684640795736\n",
      "Train Error: 9.417662787772963, Test Error: 0.3080576215044183\n",
      "Train Error: 9.011078611993385, Test Error: 0.30444541292207894\n",
      "Train Error: 8.580274097195787, Test Error: 0.30213377237092254\n",
      "Train Error: 10.169449541671614, Test Error: 0.31466764931779945\n",
      "Train Error: 9.149081868938332, Test Error: 0.30531445847827604\n",
      "Train Error: 8.61654571007435, Test Error: 0.30206747974564147\n",
      "Train Error: 8.11186898897056, Test Error: 0.2978165920705915\n",
      "Train Error: 11.398632042254135, Test Error: 0.11676620731141413\n",
      "Train Error: 11.311899651933658, Test Error: 0.11667291512401964\n",
      "Train Error: 11.268237605958475, Test Error: 0.11664689688121894\n",
      "Train Error: 11.223190029329565, Test Error: 0.11662419189444073\n",
      "Train Error: 11.358855780671641, Test Error: 0.11669689812654103\n",
      "Train Error: 11.138920860605335, Test Error: 0.11656386766568894\n",
      "Train Error: 11.02304920271025, Test Error: 0.11650573204044612\n",
      "Train Error: 10.923351614065913, Test Error: 0.11645042079240872\n",
      "Train Error: 11.299908400979872, Test Error: 0.11660820651442223\n",
      "Train Error: 10.878617320964398, Test Error: 0.11636837783132237\n",
      "Train Error: 10.726929244741134, Test Error: 0.11620460036498381\n",
      "Train Error: 10.572187201175634, Test Error: 0.11607943208678134\n",
      "Train Error: 11.23811863935172, Test Error: 0.11653519410476307\n",
      "Train Error: 10.68222931714057, Test Error: 0.11612525437857317\n",
      "Train Error: 10.452024223925996, Test Error: 0.11592054223716633\n",
      "Train Error: 10.224927483162967, Test Error: 0.11569988619480598\n",
      "Train Error: 11.174620528711438, Test Error: 0.11646835896240793\n",
      "Train Error: 10.483090728443008, Test Error: 0.11591833742795915\n",
      "Train Error: 10.18096815552627, Test Error: 0.11562029158708331\n",
      "Train Error: 9.880485701043664, Test Error: 0.11534330592159932\n",
      "Train Error: 10.914542581489142, Test Error: 0.11620188302972742\n",
      "Train Error: 9.69153630043901, Test Error: 0.11506445901707998\n",
      "Train Error: 9.096410263993837, Test Error: 0.11459777781658839\n",
      "Train Error: 8.504479313162804, Test Error: 0.11422729717956652\n",
      "Train Error: 10.824656536551483, Test Error: 0.11604261705747729\n",
      "Train Error: 9.306044132161475, Test Error: 0.11467852248536042\n",
      "Train Error: 8.565290713024492, Test Error: 0.11418520435990889\n",
      "Train Error: 7.831063648613364, Test Error: 0.11385494822451829\n",
      "Train Error: 2.3526937162523187, Test Error: 0.1465752154775487\n",
      "Train Error: 2.3817237969119143, Test Error: 0.14637674761765063\n",
      "Train Error: 2.397930087921482, Test Error: 0.146295590807449\n",
      "Train Error: 2.4148164665045306, Test Error: 0.14622145884013496\n",
      "Train Error: 2.3668813823063575, Test Error: 0.14646672061138494\n",
      "Train Error: 2.4494990876215152, Test Error: 0.14607400335241635\n",
      "Train Error: 2.49618471576354, Test Error: 0.14590230465880663\n",
      "Train Error: 2.5474545781812012, Test Error: 0.1457738598133118\n",
      "Train Error: 2.3897254916258968, Test Error: 0.14628546590547395\n",
      "Train Error: 2.576931237817783, Test Error: 0.14563454932589354\n",
      "Train Error: 2.6735820778816457, Test Error: 0.14529182860974993\n",
      "Train Error: 2.7725574097983827, Test Error: 0.14496288626913992\n",
      "Train Error: 2.4149207102074977, Test Error: 0.14612626322549493\n",
      "Train Error: 2.70403388032325, Test Error: 0.14515723897811011\n",
      "Train Error: 2.854854642394468, Test Error: 0.14467934244640202\n",
      "Train Error: 3.0100167217996407, Test Error: 0.14423186428610044\n",
      "Train Error: 2.4413146000263555, Test Error: 0.14597856062631615\n",
      "Train Error: 2.8336272086013747, Test Error: 0.14469192348884613\n",
      "Train Error: 3.040991737112717, Test Error: 0.14409968549958366\n",
      "Train Error: 3.265090633853782, Test Error: 0.14362113220903872\n",
      "Train Error: 2.5591264481426066, Test Error: 0.1454601822327088\n",
      "Train Error: 3.3947852346935528, Test Error: 0.14314418131197182\n",
      "Train Error: 3.850735521990229, Test Error: 0.14227367660427456\n",
      "Train Error: 4.320044968790502, Test Error: 0.14162549524202428\n",
      "Train Error: 2.61717466344901, Test Error: 0.1451725229476126\n",
      "Train Error: 3.688177248647424, Test Error: 0.14246596381117646\n",
      "Train Error: 4.271861852915576, Test Error: 0.1415920084095804\n",
      "Train Error: 4.866749969603083, Test Error: 0.141054705530583\n",
      "Train Error: 0.1637158871871093, Test Error: 0.15306751001974706\n",
      "Train Error: 0.16308685071288057, Test Error: 0.15347998536634513\n",
      "Train Error: 0.16263675126332913, Test Error: 0.15365293994204382\n",
      "Train Error: 0.1621202741983212, Test Error: 0.15383758085047422\n",
      "Train Error: 0.1634030583215254, Test Error: 0.15330991145770018\n",
      "Train Error: 0.16125101592878804, Test Error: 0.15417412061538124\n",
      "Train Error: 0.16027858535527917, Test Error: 0.15448349393210967\n",
      "Train Error: 0.15883043199420965, Test Error: 0.15509267895336237\n",
      "Train Error: 0.1630365782139353, Test Error: 0.15353368577211035\n",
      "Train Error: 0.15814657956797137, Test Error: 0.15536854315209775\n",
      "Train Error: 0.15601378414484876, Test Error: 0.15616844479258282\n",
      "Train Error: 0.15365960875607448, Test Error: 0.1571184314763944\n",
      "Train Error: 0.16230735704845142, Test Error: 0.15380508632021417\n",
      "Train Error: 0.15536780446052179, Test Error: 0.1564505326155745\n",
      "Train Error: 0.15189379527795144, Test Error: 0.1578921066937986\n",
      "Train Error: 0.14856895424923558, Test Error: 0.1593764822185946\n",
      "Train Error: 0.1616607449281619, Test Error: 0.15406567774739327\n",
      "Train Error: 0.15239953393405675, Test Error: 0.15769009607190154\n",
      "Train Error: 0.14793158722913852, Test Error: 0.1596574072521052\n",
      "Train Error: 0.1437269577843967, Test Error: 0.16168929975762109\n",
      "Train Error: 0.15892531982652913, Test Error: 0.15514976770734526\n",
      "Train Error: 0.14145886564685256, Test Error: 0.1628902035829342\n",
      "Train Error: 0.13403989668402325, Test Error: 0.16715777057908887\n",
      "Train Error: 0.12768941075760526, Test Error: 0.17160094673965914\n",
      "Train Error: 0.15762708937139247, Test Error: 0.1556913236262046\n",
      "Train Error: 0.13660893489063153, Test Error: 0.16562482737686599\n",
      "Train Error: 0.12833921754427896, Test Error: 0.17112642346398904\n",
      "Train Error: 0.12200049500670508, Test Error: 0.1768973133201164\n",
      "Percentile: 0.4, Tracking Error: 1.6366141586342524\n",
      "Starting percentile: 0.5\n",
      "Train Error: 0.02221641878384294, Test Error: 0.4877691484674281\n",
      "Train Error: 0.02182109946946262, Test Error: 0.4617642261799769\n",
      "Train Error: 0.021711555383980088, Test Error: 0.44938802623182117\n",
      "Train Error: 0.021618205999352498, Test Error: 0.4374541868224593\n",
      "Train Error: 0.022116838511666213, Test Error: 0.4753315108877941\n",
      "Train Error: 0.021154235246474792, Test Error: 0.4196832718791548\n",
      "Train Error: 0.021035181809423976, Test Error: 0.39782941444375103\n",
      "Train Error: 0.020183045921522858, Test Error: 0.38048607105971965\n",
      "Train Error: 0.02167476967351052, Test Error: 0.456473330000291\n",
      "Train Error: 0.01990325585467955, Test Error: 0.37044301807969776\n",
      "Train Error: 0.021332499114042062, Test Error: 0.3438108613976052\n",
      "Train Error: 0.026152429981688267, Test Error: 0.33051904746990174\n",
      "Train Error: 0.021407814551820035, Test Error: 0.4389941958398876\n",
      "Train Error: 0.02258686944408213, Test Error: 0.3402593540421003\n",
      "Train Error: 0.03001608439410481, Test Error: 0.32010304732891354\n",
      "Train Error: 0.034715894687631484, Test Error: 0.3060910720148631\n",
      "Train Error: 0.02135363705257417, Test Error: 0.42441885952318725\n",
      "Train Error: 0.029162436104651777, Test Error: 0.3231596286672702\n",
      "Train Error: 0.035313902321604224, Test Error: 0.30591584607854777\n",
      "Train Error: 0.039819826392097656, Test Error: 0.30520500683310375\n",
      "Train Error: 0.020743759687253952, Test Error: 0.3788142622114328\n",
      "Train Error: 0.042294155004920245, Test Error: 0.30539863952408064\n",
      "Train Error: 0.049147350537657994, Test Error: 0.30948445093757815\n",
      "Train Error: 0.05218760421148693, Test Error: 0.3122733415030082\n",
      "Train Error: 0.020520289350551144, Test Error: 0.36207971741964456\n",
      "Train Error: 0.048001376521388514, Test Error: 0.30844466608551\n",
      "Train Error: 0.05190463933421478, Test Error: 0.311630898845283\n",
      "Train Error: 0.056267609155388705, Test Error: 0.3179219155130851\n",
      "Train Error: 0.20973186893085732, Test Error: 0.3884043761216528\n",
      "Train Error: 0.20974880879047272, Test Error: 0.3871983591593666\n",
      "Train Error: 0.20976352198805176, Test Error: 0.3866140628917828\n",
      "Train Error: 0.20977760038786486, Test Error: 0.3860541246616403\n",
      "Train Error: 0.20973018585871003, Test Error: 0.3879121354658894\n",
      "Train Error: 0.2098149148132448, Test Error: 0.38502910439269206\n",
      "Train Error: 0.20987314772496782, Test Error: 0.38363260976031704\n",
      "Train Error: 0.2099393087002607, Test Error: 0.3822437522357726\n",
      "Train Error: 0.20974754695222533, Test Error: 0.3871125192460854\n",
      "Train Error: 0.2100343207317266, Test Error: 0.38178510617377404\n",
      "Train Error: 0.21044511067318686, Test Error: 0.38029579204581365\n",
      "Train Error: 0.21087084652656157, Test Error: 0.37882687723151126\n",
      "Train Error: 0.20976676102928596, Test Error: 0.38633301147357124\n",
      "Train Error: 0.21055242684376024, Test Error: 0.3798846865078142\n",
      "Train Error: 0.21120204129234957, Test Error: 0.3777015394952698\n",
      "Train Error: 0.21188278888822532, Test Error: 0.37558552983680865\n",
      "Train Error: 0.2097861669715402, Test Error: 0.3855772080226876\n",
      "Train Error: 0.21109399607409754, Test Error: 0.3780202159656606\n",
      "Train Error: 0.21200221725508736, Test Error: 0.37519657703660614\n",
      "Train Error: 0.21296714085861254, Test Error: 0.3724683034758536\n",
      "Train Error: 0.2099059850542057, Test Error: 0.3825750033165888\n",
      "Train Error: 0.21348244617854478, Test Error: 0.37099155238589826\n",
      "Train Error: 0.21563191294321962, Test Error: 0.36594977070648244\n",
      "Train Error: 0.21799024080328339, Test Error: 0.3613295041050173\n",
      "Train Error: 0.21005140980019937, Test Error: 0.3814512101990544\n",
      "Train Error: 0.21480802217451023, Test Error: 0.36771265160799277\n",
      "Train Error: 0.21769195492340532, Test Error: 0.3618152114168277\n",
      "Train Error: 0.22088713507383606, Test Error: 0.3565590942125978\n",
      "Train Error: 0.21139661690159697, Test Error: 0.21113316762989887\n",
      "Train Error: 0.2113703690684599, Test Error: 0.21141814552810145\n",
      "Train Error: 0.21134742722932967, Test Error: 0.21159193388394945\n",
      "Train Error: 0.21132259104558093, Test Error: 0.21176308562759275\n",
      "Train Error: 0.21137442978785337, Test Error: 0.21128220435347456\n",
      "Train Error: 0.21128198681584806, Test Error: 0.21207939500141346\n",
      "Train Error: 0.21123804902658647, Test Error: 0.21251694560645337\n",
      "Train Error: 0.21118738785606872, Test Error: 0.2129655689502747\n",
      "Train Error: 0.21135442903287058, Test Error: 0.21145768586033065\n",
      "Train Error: 0.21114702151131665, Test Error: 0.21322027175735278\n",
      "Train Error: 0.2110359480780753, Test Error: 0.21416416149106904\n",
      "Train Error: 0.21094090884643457, Test Error: 0.21512730886087175\n",
      "Train Error: 0.21131586698510327, Test Error: 0.21169984002009118\n",
      "Train Error: 0.21100226008897396, Test Error: 0.21442782846619562\n",
      "Train Error: 0.21087324285719172, Test Error: 0.21588661871882103\n",
      "Train Error: 0.21077754237320923, Test Error: 0.21739117964173008\n",
      "Train Error: 0.2112762738417229, Test Error: 0.21196643352253772\n",
      "Train Error: 0.21088305647451097, Test Error: 0.21566481941645604\n",
      "Train Error: 0.2107574469300889, Test Error: 0.21766976644709926\n",
      "Train Error: 0.2106991725292439, Test Error: 0.21974844850883551\n",
      "Train Error: 0.2111445288447471, Test Error: 0.21291085472784263\n",
      "Train Error: 0.21066213445320955, Test Error: 0.22091046076721144\n",
      "Train Error: 0.2108064854881883, Test Error: 0.22533920040405234\n",
      "Train Error: 0.21120185130505678, Test Error: 0.23002882935000732\n",
      "Train Error: 0.21107551486966047, Test Error: 0.21341408908913628\n",
      "Train Error: 0.2107046945822095, Test Error: 0.22369907110632675\n",
      "Train Error: 0.21112436795117243, Test Error: 0.2294721804402697\n",
      "Train Error: 0.21193672711671208, Test Error: 0.23559782864146336\n",
      "Train Error: 0.18750724345187417, Test Error: 0.19418644056816922\n",
      "Train Error: 0.18753540555214887, Test Error: 0.19505071781072741\n",
      "Train Error: 0.18755079017664503, Test Error: 0.1954835267341124\n",
      "Train Error: 0.18756447928752937, Test Error: 0.1959038562469928\n",
      "Train Error: 0.18751145195170274, Test Error: 0.19451316361993062\n",
      "Train Error: 0.18758933506939207, Test Error: 0.1966653293631047\n",
      "Train Error: 0.18763768830972655, Test Error: 0.19775134423536858\n",
      "Train Error: 0.18768883107426929, Test Error: 0.1988284197614754\n",
      "Train Error: 0.18751967171875827, Test Error: 0.19505823298446504\n",
      "Train Error: 0.1877085480305514, Test Error: 0.1993774101310905\n",
      "Train Error: 0.18782918270183185, Test Error: 0.201549212809337\n",
      "Train Error: 0.187980734964831, Test Error: 0.2037373496588015\n",
      "Train Error: 0.18752695024976695, Test Error: 0.19559064396255557\n",
      "Train Error: 0.18785631974125655, Test Error: 0.20209884026160604\n",
      "Train Error: 0.18810000856010392, Test Error: 0.2053862939314087\n",
      "Train Error: 0.18838909533568712, Test Error: 0.20869124356264865\n",
      "Train Error: 0.18753830352425788, Test Error: 0.19613685070419992\n",
      "Train Error: 0.18804765871154996, Test Error: 0.20484053970955807\n",
      "Train Error: 0.1884345738483873, Test Error: 0.2092454705428891\n",
      "Train Error: 0.18890940855670155, Test Error: 0.2136677792311845\n",
      "Train Error: 0.18759648725452335, Test Error: 0.19831474137212451\n",
      "Train Error: 0.18914778649176792, Test Error: 0.21590023321801452\n",
      "Train Error: 0.1904044499908339, Test Error: 0.2247472867682633\n",
      "Train Error: 0.19184398792722973, Test Error: 0.23339801252141254\n",
      "Train Error: 0.18763593668309642, Test Error: 0.19941278915120433\n",
      "Train Error: 0.18990125777453426, Test Error: 0.22145903271057082\n",
      "Train Error: 0.19164316157474223, Test Error: 0.23232954533826167\n",
      "Train Error: 0.1938157506894339, Test Error: 0.24313243414939886\n",
      "Train Error: 0.14352240513877676, Test Error: 0.12220705972559162\n",
      "Train Error: 0.14357546629789436, Test Error: 0.1221351235255337\n",
      "Train Error: 0.1436002651867973, Test Error: 0.1221086957263284\n",
      "Train Error: 0.14362695841186457, Test Error: 0.12208247445910016\n",
      "Train Error: 0.14354806270608297, Test Error: 0.122162767996535\n",
      "Train Error: 0.1436799062776653, Test Error: 0.12203150991884318\n",
      "Train Error: 0.14376421135009912, Test Error: 0.12196784631440918\n",
      "Train Error: 0.14386067753152887, Test Error: 0.12190546871148646\n",
      "Train Error: 0.14357801062758632, Test Error: 0.12212000461933814\n",
      "Train Error: 0.14391060915142814, Test Error: 0.12186165399514616\n",
      "Train Error: 0.1441532717637087, Test Error: 0.12174238629693643\n",
      "Train Error: 0.1444438796037958, Test Error: 0.12162878407070332\n",
      "Train Error: 0.14361151948418177, Test Error: 0.1220776667177731\n",
      "Train Error: 0.1442261563610661, Test Error: 0.12170436706366963\n",
      "Train Error: 0.14469870384324351, Test Error: 0.12153839741665286\n",
      "Train Error: 0.14526946667799484, Test Error: 0.12138154018452825\n",
      "Train Error: 0.1436486423278403, Test Error: 0.12203576699752658\n",
      "Train Error: 0.14461842052034254, Test Error: 0.12155623635229251\n",
      "Train Error: 0.14538312389469488, Test Error: 0.12134852471216258\n",
      "Train Error: 0.14632745119798196, Test Error: 0.12116418563762975\n",
      "Train Error: 0.14382647203635765, Test Error: 0.12186819457512944\n",
      "Train Error: 0.14690700704373208, Test Error: 0.12104855147026829\n",
      "Train Error: 0.14947549576976296, Test Error: 0.12077532130462708\n",
      "Train Error: 0.15265434250463447, Test Error: 0.12060101672047926\n",
      "Train Error: 0.14393991359239208, Test Error: 0.12178877759124034\n",
      "Train Error: 0.1484626648681279, Test Error: 0.12085082065874773\n",
      "Train Error: 0.1522563257991363, Test Error: 0.1206020652084852\n",
      "Train Error: 0.1568987931669197, Test Error: 0.12050244251247826\n",
      "Train Error: 0.21603239692641296, Test Error: 0.14331764663637328\n",
      "Train Error: 0.21631816418193084, Test Error: 0.14337683485394032\n",
      "Train Error: 0.21649603606145376, Test Error: 0.14341316020741438\n",
      "Train Error: 0.21665056103690225, Test Error: 0.14344809551501125\n",
      "Train Error: 0.2161721081496842, Test Error: 0.14334462796001743\n",
      "Train Error: 0.2169058059564771, Test Error: 0.14351293727087217\n",
      "Train Error: 0.217297586459056, Test Error: 0.1436150056958001\n",
      "Train Error: 0.21768363604214538, Test Error: 0.14372608584983415\n",
      "Train Error: 0.21637830613419246, Test Error: 0.143388795654876\n",
      "Train Error: 0.21790535708309186, Test Error: 0.14379346191187395\n",
      "Train Error: 0.2186762202796408, Test Error: 0.14405510742870192\n",
      "Train Error: 0.2194858065741365, Test Error: 0.14436088802332753\n",
      "Train Error: 0.2165941162321147, Test Error: 0.14343692178383421\n",
      "Train Error: 0.21890018000964417, Test Error: 0.14413625140548522\n",
      "Train Error: 0.2201244400341826, Test Error: 0.14462612182695153\n",
      "Train Error: 0.22137938128357662, Test Error: 0.14520518741291855\n",
      "Train Error: 0.21680418622776604, Test Error: 0.14348725787584254\n",
      "Train Error: 0.21994262885079133, Test Error: 0.14454801222157704\n",
      "Train Error: 0.22161632476478607, Test Error: 0.1453211449106488\n",
      "Train Error: 0.22334278841374314, Test Error: 0.14624566670049657\n",
      "Train Error: 0.2176501557597131, Test Error: 0.14371848239680082\n",
      "Train Error: 0.22432407635921067, Test Error: 0.14682038380701368\n",
      "Train Error: 0.22794469692042404, Test Error: 0.14925798004863153\n",
      "Train Error: 0.2317124439724557, Test Error: 0.15223415870405801\n",
      "Train Error: 0.21806389146917896, Test Error: 0.14385019208565455\n",
      "Train Error: 0.22662025029851462, Test Error: 0.14831112551912878\n",
      "Train Error: 0.23128813430780465, Test Error: 0.15187405937184137\n",
      "Train Error: 0.23615161134090898, Test Error: 0.15621728836131252\n",
      "Train Error: 0.20995312077310793, Test Error: 0.14845519014993971\n",
      "Train Error: 0.2102545134606475, Test Error: 0.1484181424418553\n",
      "Train Error: 0.21040496502347678, Test Error: 0.14839960499919222\n",
      "Train Error: 0.21055602496575268, Test Error: 0.14838157463191098\n",
      "Train Error: 0.21007693399316146, Test Error: 0.14843353239375953\n",
      "Train Error: 0.21083136632279037, Test Error: 0.14834284357534994\n",
      "Train Error: 0.2112153331998518, Test Error: 0.14830357119222837\n",
      "Train Error: 0.21159986728283495, Test Error: 0.14826579027395376\n",
      "Train Error: 0.21028069695523835, Test Error: 0.14839569884845746\n",
      "Train Error: 0.21181021824259802, Test Error: 0.14823452140884943\n",
      "Train Error: 0.21259376112312162, Test Error: 0.14817294431740272\n",
      "Train Error: 0.2133852042729012, Test Error: 0.14812184480005025\n",
      "Train Error: 0.2104855134674458, Test Error: 0.14835878912360134\n",
      "Train Error: 0.2128078984904704, Test Error: 0.1481458454527671\n",
      "Train Error: 0.21400300797269106, Test Error: 0.14807847914194905\n",
      "Train Error: 0.215220557815044, Test Error: 0.1480379320385959\n",
      "Train Error: 0.21069126473445446, Test Error: 0.14832279663267967\n",
      "Train Error: 0.21381985966263722, Test Error: 0.14807497017249688\n",
      "Train Error: 0.21544681108607608, Test Error: 0.14802284135211308\n",
      "Train Error: 0.2171086224965247, Test Error: 0.14801666903650804\n",
      "Train Error: 0.21152499948442832, Test Error: 0.1481893574803088\n",
      "Train Error: 0.2180279507045318, Test Error: 0.14798462340594723\n",
      "Train Error: 0.22152682711364957, Test Error: 0.14817185933833296\n",
      "Train Error: 0.22511348491684013, Test Error: 0.1485360156001411\n",
      "Train Error: 0.21194870972291552, Test Error: 0.14812888970400337\n",
      "Train Error: 0.22025260251583018, Test Error: 0.1480611099067267\n",
      "Train Error: 0.22470037929228834, Test Error: 0.1484607394649341\n",
      "Train Error: 0.2293116466234403, Test Error: 0.14914450737141433\n",
      "Train Error: 0.1588032472399781, Test Error: 0.3088722895795016\n",
      "Train Error: 0.15879863462571292, Test Error: 0.30873023955800705\n",
      "Train Error: 0.15880257075200818, Test Error: 0.30866020596016414\n",
      "Train Error: 0.1588010790304829, Test Error: 0.3085920782031019\n",
      "Train Error: 0.15879807680363864, Test Error: 0.3088227423560804\n",
      "Train Error: 0.15879962336901746, Test Error: 0.3084790803857504\n",
      "Train Error: 0.15880611843115017, Test Error: 0.3083166415606371\n",
      "Train Error: 0.1588186553891249, Test Error: 0.3081596979323263\n",
      "Train Error: 0.15879412323043407, Test Error: 0.3087408544634553\n",
      "Train Error: 0.15882569042382275, Test Error: 0.3080904799885422\n",
      "Train Error: 0.1588783054327724, Test Error: 0.30779891171710133\n",
      "Train Error: 0.15895683269663047, Test Error: 0.30752941554532615\n",
      "Train Error: 0.15879518784369498, Test Error: 0.30866046788786666\n",
      "Train Error: 0.15889443528732616, Test Error: 0.30773699883130823\n",
      "Train Error: 0.15903272213917005, Test Error: 0.3073496261663074\n",
      "Train Error: 0.15922971153511606, Test Error: 0.3070115048099333\n",
      "Train Error: 0.15879172037115488, Test Error: 0.3085826896049121\n",
      "Train Error: 0.15900518830581137, Test Error: 0.3074186554736901\n",
      "Train Error: 0.15926881310939478, Test Error: 0.30696835418653196\n",
      "Train Error: 0.15963488707026635, Test Error: 0.3066047745331323\n",
      "Train Error: 0.15879811194452226, Test Error: 0.30828707672190087\n",
      "Train Error: 0.15986466565516677, Test Error: 0.30649250102068104\n",
      "Train Error: 0.1610033636709259, Test Error: 0.3061132761224588\n",
      "Train Error: 0.1625201497382214, Test Error: 0.30607309639353303\n",
      "Train Error: 0.15881267167802976, Test Error: 0.3081485323333271\n",
      "Train Error: 0.16053713338354478, Test Error: 0.3062352638295626\n",
      "Train Error: 0.1623209404346447, Test Error: 0.3060820716809142\n",
      "Train Error: 0.1646827421247647, Test Error: 0.3064457025827482\n",
      "Train Error: 0.18357707495316813, Test Error: 0.4036238944875026\n",
      "Train Error: 0.18359383945705748, Test Error: 0.40219349777544894\n",
      "Train Error: 0.18360388126243127, Test Error: 0.40147789909534753\n",
      "Train Error: 0.18361573262676387, Test Error: 0.400755470703521\n",
      "Train Error: 0.18358285838144095, Test Error: 0.40303242862615174\n",
      "Train Error: 0.18363376774780613, Test Error: 0.39948594650046376\n",
      "Train Error: 0.18366264860975565, Test Error: 0.3977689626445964\n",
      "Train Error: 0.18369661954840963, Test Error: 0.39606122204329003\n",
      "Train Error: 0.1835960800045881, Test Error: 0.40202560993351016\n",
      "Train Error: 0.1837179402653173, Test Error: 0.39510483888705794\n",
      "Train Error: 0.18380809115833316, Test Error: 0.3917154473118313\n",
      "Train Error: 0.18391835927822997, Test Error: 0.38836432988102754\n",
      "Train Error: 0.18361101954553483, Test Error: 0.40102123165009385\n",
      "Train Error: 0.18383575467392188, Test Error: 0.39077586935107933\n",
      "Train Error: 0.18401819982521966, Test Error: 0.38576981946172817\n",
      "Train Error: 0.18424681834525813, Test Error: 0.38084451543909065\n",
      "Train Error: 0.18362621792150824, Test Error: 0.4000330554880461\n",
      "Train Error: 0.18398755337426387, Test Error: 0.38649787871284524\n",
      "Train Error: 0.18429180457233127, Test Error: 0.3799393909776525\n",
      "Train Error: 0.18467839198974048, Test Error: 0.3735213335930352\n",
      "Train Error: 0.18369307769248885, Test Error: 0.3961882404616396\n",
      "Train Error: 0.18491992111961195, Test Error: 0.36999012991959207\n",
      "Train Error: 0.18597473725149793, Test Error: 0.35764532284433354\n",
      "Train Error: 0.18733418718615782, Test Error: 0.3456892118460503\n",
      "Train Error: 0.18373579939230283, Test Error: 0.3942788010175816\n",
      "Train Error: 0.18555842969416295, Test Error: 0.3621431106343248\n",
      "Train Error: 0.18716278955456406, Test Error: 0.34699308566474263\n",
      "Train Error: 0.1892415729612835, Test Error: 0.3329802056536985\n",
      "Train Error: 0.2628794425739274, Test Error: 3.1219769755545137\n",
      "Train Error: 0.2616267674397684, Test Error: 3.303598400937705\n",
      "Train Error: 0.26098089290919996, Test Error: 3.3998264232885\n",
      "Train Error: 0.2603396991727126, Test Error: 3.4996492804602\n",
      "Train Error: 0.2623820524863561, Test Error: 3.1967099843533244\n",
      "Train Error: 0.2592506103747554, Test Error: 3.7163734712451166\n",
      "Train Error: 0.25775037930463535, Test Error: 4.048042004688573\n",
      "Train Error: 0.2562896991877091, Test Error: 4.394649954700967\n",
      "Train Error: 0.2615908471726538, Test Error: 3.32727771412296\n",
      "Train Error: 0.2556752006219527, Test Error: 4.487939539322208\n",
      "Train Error: 0.2541378368827305, Test Error: 4.935385921436291\n",
      "Train Error: 0.2529801655751869, Test Error: 5.44859057445789\n",
      "Train Error: 0.260764695198999, Test Error: 3.464665427793148\n",
      "Train Error: 0.2538311064847883, Test Error: 5.078931751536055\n",
      "Train Error: 0.2520252175221845, Test Error: 5.853316829575074\n",
      "Train Error: 0.25050038375930755, Test Error: 6.656216692309238\n",
      "Train Error: 0.2599447144405249, Test Error: 3.6082290770366066\n",
      "Train Error: 0.2523985544078158, Test Error: 5.7469373929828285\n",
      "Train Error: 0.25022977728458795, Test Error: 6.809201485214202\n",
      "Train Error: 0.24807407806293028, Test Error: 7.90029594818181\n",
      "Train Error: 0.256877635006684, Test Error: 4.34574983604472\n",
      "Train Error: 0.24699684288372004, Test Error: 8.564921532127636\n",
      "Train Error: 0.2431288348621452, Test Error: 10.947157768160585\n",
      "Train Error: 0.23984967750492647, Test Error: 13.32058634226416\n",
      "Train Error: 0.2555485280371962, Test Error: 4.621682677969029\n",
      "Train Error: 0.24450647535062106, Test Error: 10.08902293273715\n",
      "Train Error: 0.2402293507228546, Test Error: 13.058954987448589\n",
      "Train Error: 0.23712405597419697, Test Error: 15.967100175676016\n",
      "Train Error: 2.619002116593962, Test Error: 1.7685271787134018\n",
      "Train Error: 2.7110736862532083, Test Error: 1.7672436382512688\n",
      "Train Error: 2.7597228104337104, Test Error: 1.766644734829217\n",
      "Train Error: 2.808513538501866, Test Error: 1.7660329533475392\n",
      "Train Error: 2.654345186577709, Test Error: 1.7675103230205853\n",
      "Train Error: 2.8950355981710953, Test Error: 1.7647255416979248\n",
      "Train Error: 3.0162200653748976, Test Error: 1.7628684215876949\n",
      "Train Error: 3.140417218162452, Test Error: 1.761330123580532\n",
      "Train Error: 2.718670967334429, Test Error: 1.767448196190656\n",
      "Train Error: 3.2055230029380213, Test Error: 1.760399993956769\n",
      "Train Error: 3.4551470377397435, Test Error: 1.7570872468931928\n",
      "Train Error: 3.697545017461201, Test Error: 1.751767496537851\n",
      "Train Error: 2.784245124473609, Test Error: 1.7668199210403708\n",
      "Train Error: 3.5212554505619424, Test Error: 1.7562037664275805\n",
      "Train Error: 3.802711215271271, Test Error: 1.7299039899173763\n",
      "Train Error: 3.949604811197992, Test Error: 1.7320728697164127\n",
      "Train Error: 2.848001998668741, Test Error: 1.7658807865869792\n",
      "Train Error: 3.778107624501225, Test Error: 1.7383052311634521\n",
      "Train Error: 3.9687250414298716, Test Error: 1.7323861334299107\n",
      "Train Error: 4.154509810193168, Test Error: 1.736538449209634\n",
      "Train Error: 3.1094156390879415, Test Error: 1.7627443234197089\n",
      "Train Error: 4.2652304422190355, Test Error: 1.7388363358492918\n",
      "Train Error: 4.651355752232398, Test Error: 1.7545884949989703\n",
      "Train Error: 5.030618239345785, Test Error: 1.772218129826405\n",
      "Train Error: 3.2414448506351303, Test Error: 1.7612132474525106\n",
      "Train Error: 4.510678919453464, Test Error: 1.7466534159100657\n",
      "Train Error: 4.9917701056982136, Test Error: 1.770156972552\n",
      "Train Error: 5.511033710739802, Test Error: 1.7931044191454784\n",
      "Train Error: 12.463367544316, Test Error: 0.34278188120576697\n",
      "Train Error: 12.496420057278684, Test Error: 0.34268245878349013\n",
      "Train Error: 12.507877127392414, Test Error: 0.3424870990396932\n",
      "Train Error: 12.519173486373838, Test Error: 0.34228792566639943\n",
      "Train Error: 12.480298814625316, Test Error: 0.34283858262830513\n",
      "Train Error: 12.539753230223127, Test Error: 0.34189112546038863\n",
      "Train Error: 12.56966788765047, Test Error: 0.3414234463327355\n",
      "Train Error: 12.599194699337424, Test Error: 0.34095039981324804\n",
      "Train Error: 12.495373766243906, Test Error: 0.34250264666291086\n",
      "Train Error: 12.614248196298474, Test Error: 0.34061535453448155\n",
      "Train Error: 12.646003159680157, Test Error: 0.3389990538514196\n",
      "Train Error: 12.706031612632264, Test Error: 0.3380576228438978\n",
      "Train Error: 12.509516206807948, Test Error: 0.3421527569407688\n",
      "Train Error: 12.66097911740088, Test Error: 0.33867236163537767\n",
      "Train Error: 12.750843527990062, Test Error: 0.3372555626237923\n",
      "Train Error: 12.842288091311431, Test Error: 0.3359249947749081\n",
      "Train Error: 12.523452818374684, Test Error: 0.3417977768301116\n",
      "Train Error: 12.734866973633402, Test Error: 0.3373539005940528\n",
      "Train Error: 12.857106535905423, Test Error: 0.3355918084818027\n",
      "Train Error: 12.978550313819026, Test Error: 0.3338059839392197\n",
      "Train Error: 12.578512743778406, Test Error: 0.34035860223931\n",
      "Train Error: 13.037780884711848, Test Error: 0.3324618176327839\n",
      "Train Error: 13.281100037041812, Test Error: 0.32899137802472084\n",
      "Train Error: 13.524626149074415, Test Error: 0.325628925869606\n",
      "Train Error: 12.579525778112751, Test Error: 0.33901813738755804\n",
      "Train Error: 13.189298080940597, Test Error: 0.33004590161216585\n",
      "Train Error: 13.49388005283626, Test Error: 0.325794024800858\n",
      "Train Error: 13.798513988635923, Test Error: 0.3216825482895563\n",
      "Train Error: 11.139474255363446, Test Error: 0.15979109751282022\n",
      "Train Error: 11.070303271060599, Test Error: 0.15956278463240794\n",
      "Train Error: 10.993383153733102, Test Error: 0.15953116938702303\n",
      "Train Error: 10.945699242509056, Test Error: 0.15944033334906696\n",
      "Train Error: 11.107385535899525, Test Error: 0.15966718394775908\n",
      "Train Error: 10.862810939784993, Test Error: 0.15925676947540077\n",
      "Train Error: 10.726659688859504, Test Error: 0.1590403100290612\n",
      "Train Error: 10.609445532646363, Test Error: 0.15881547615952857\n",
      "Train Error: 11.02772993635472, Test Error: 0.1595645311365624\n",
      "Train Error: 10.546563361866932, Test Error: 0.15866554127413665\n",
      "Train Error: 10.321852196492696, Test Error: 0.1582272021362332\n",
      "Train Error: 10.11653961597386, Test Error: 0.15783878513464147\n",
      "Train Error: 10.965810872530993, Test Error: 0.15940981188742148\n",
      "Train Error: 10.259630012348945, Test Error: 0.15808058020668037\n",
      "Train Error: 9.99961326815624, Test Error: 0.15762169464026587\n",
      "Train Error: 9.785301332257399, Test Error: 0.15725612637772496\n",
      "Train Error: 10.903258134346817, Test Error: 0.15925633667106825\n",
      "Train Error: 10.032166204573123, Test Error: 0.15764276222200824\n",
      "Train Error: 9.742069217816704, Test Error: 0.15716622191897997\n",
      "Train Error: 9.449022402943838, Test Error: 0.156728612367423\n",
      "Train Error: 10.631276429181158, Test Error: 0.15866095365425187\n",
      "Train Error: 9.280993949842593, Test Error: 0.1563755977461372\n",
      "Train Error: 8.691683625207117, Test Error: 0.15559732438819032\n",
      "Train Error: 8.10730836199703, Test Error: 0.1549065262639326\n",
      "Train Error: 10.50506199707638, Test Error: 0.15836214547890368\n",
      "Train Error: 8.899770322436835, Test Error: 0.155809703541494\n",
      "Train Error: 8.170204302386518, Test Error: 0.15491532818251744\n",
      "Train Error: 7.4373156547696295, Test Error: 0.15416808718456237\n",
      "Train Error: 38.25968571423388, Test Error: 0.16379688764390057\n",
      "Train Error: 38.15185255473137, Test Error: 0.16408657221395495\n",
      "Train Error: 38.0519098640004, Test Error: 0.16415446438867892\n",
      "Train Error: 37.96308894598067, Test Error: 0.16431675996647058\n",
      "Train Error: 38.211841516574395, Test Error: 0.16391667167413085\n",
      "Train Error: 37.865820870683976, Test Error: 0.1644703575704529\n",
      "Train Error: 37.739116843324034, Test Error: 0.16467198254204068\n",
      "Train Error: 37.610303813614934, Test Error: 0.16488798417431277\n",
      "Train Error: 38.13131176916094, Test Error: 0.16409852107198014\n",
      "Train Error: 37.53555726088976, Test Error: 0.1649934837758363\n",
      "Train Error: 37.27856655560742, Test Error: 0.1654320609169142\n",
      "Train Error: 37.023961555545256, Test Error: 0.16586520600132665\n",
      "Train Error: 37.98020040438846, Test Error: 0.16423196795042716\n",
      "Train Error: 37.203717610314754, Test Error: 0.16553995264162813\n",
      "Train Error: 36.821482316458265, Test Error: 0.1662005611532491\n",
      "Train Error: 36.44106844681079, Test Error: 0.16686195132882917\n",
      "Train Error: 37.894200537136975, Test Error: 0.16437846006304027\n",
      "Train Error: 36.87317394280342, Test Error: 0.16609715567220495\n",
      "Train Error: 36.36619015210972, Test Error: 0.16697566213630044\n",
      "Train Error: 35.86076253436107, Test Error: 0.16788550188433335\n",
      "Train Error: 37.59083822677672, Test Error: 0.16481960020476366\n",
      "Train Error: 35.57140928085264, Test Error: 0.1684056356194753\n",
      "Train Error: 34.56860029416134, Test Error: 0.17026214743484794\n",
      "Train Error: 33.57425614727642, Test Error: 0.17218348682999018\n",
      "Train Error: 37.43928481155681, Test Error: 0.1650438046012051\n",
      "Train Error: 34.92100548222873, Test Error: 0.1695732204280614\n",
      "Train Error: 33.674789845984996, Test Error: 0.17195476340884708\n",
      "Train Error: 32.44158776056649, Test Error: 0.17443371206157696\n",
      "Train Error: 0.1418581603097236, Test Error: 0.16081652437936206\n",
      "Train Error: 0.1417776712820821, Test Error: 0.16128266085060872\n",
      "Train Error: 0.14174090343234114, Test Error: 0.16151551427050093\n",
      "Train Error: 0.14170621632689978, Test Error: 0.16174878073327972\n",
      "Train Error: 0.14182025035260593, Test Error: 0.16102706911275844\n",
      "Train Error: 0.14164745763643408, Test Error: 0.16218006749148609\n",
      "Train Error: 0.1415794533864224, Test Error: 0.16276677643462237\n",
      "Train Error: 0.1415246510023044, Test Error: 0.1633561969391664\n",
      "Train Error: 0.14176077828014746, Test Error: 0.1613773070320991\n",
      "Train Error: 0.14149424396640276, Test Error: 0.16371145646477095\n",
      "Train Error: 0.14143958044924604, Test Error: 0.16488757475826518\n",
      "Train Error: 0.14143618513237557, Test Error: 0.1660748914199631\n",
      "Train Error: 0.14170551574575554, Test Error: 0.16172769517035276\n",
      "Train Error: 0.1414274798968892, Test Error: 0.16524619033574703\n",
      "Train Error: 0.14146299848163757, Test Error: 0.16704006297515536\n",
      "Train Error: 0.14161454248654518, Test Error: 0.16886660372752024\n",
      "Train Error: 0.14165711242765372, Test Error: 0.16206518030313405\n",
      "Train Error: 0.14144444557201488, Test Error: 0.16679721410878723\n",
      "Train Error: 0.1416482616812901, Test Error: 0.16922102448753107\n",
      "Train Error: 0.1420563689091913, Test Error: 0.17169410415275288\n",
      "Train Error: 0.14149422923690558, Test Error: 0.16346343697375637\n",
      "Train Error: 0.14234156558693256, Test Error: 0.1731722310811454\n",
      "Train Error: 0.14397732863267154, Test Error: 0.17823816986201096\n",
      "Train Error: 0.1463633364044634, Test Error: 0.18345291172089034\n",
      "Train Error: 0.1414358302925799, Test Error: 0.16417399489617604\n",
      "Train Error: 0.14327712810415363, Test Error: 0.176445639306851\n",
      "Train Error: 0.14604359059227803, Test Error: 0.18291235931651187\n",
      "Train Error: 0.14993097440338185, Test Error: 0.189571352607062\n",
      "Percentile: 0.5, Tracking Error: 0.5180191157876713\n",
      "Starting percentile: 0.6\n",
      "Train Error: 0.03818106365373112, Test Error: 0.30801797250880164\n",
      "Train Error: 0.03900644686735976, Test Error: 0.2989313064242709\n",
      "Train Error: 0.03917420082668009, Test Error: 0.29633190116448604\n",
      "Train Error: 0.0394509836378114, Test Error: 0.2934587515961916\n",
      "Train Error: 0.0386256541204025, Test Error: 0.30345942323914943\n",
      "Train Error: 0.040071975260301755, Test Error: 0.2883307882834416\n",
      "Train Error: 0.0404190403659779, Test Error: 0.28721810553060423\n",
      "Train Error: 0.04076793739149291, Test Error: 0.2863055169021677\n",
      "Train Error: 0.03910375499862409, Test Error: 0.298149931475005\n",
      "Train Error: 0.0409489083757045, Test Error: 0.28571087752018565\n",
      "Train Error: 0.041365382436827594, Test Error: 0.28464449257060825\n",
      "Train Error: 0.04176922846241404, Test Error: 0.283119581305725\n",
      "Train Error: 0.03946504058171056, Test Error: 0.29417781093425127\n",
      "Train Error: 0.04152882218882545, Test Error: 0.2842230961238754\n",
      "Train Error: 0.04201198194863586, Test Error: 0.28081381167978825\n",
      "Train Error: 0.04239030792360599, Test Error: 0.27669060258181616\n",
      "Train Error: 0.04000117605194026, Test Error: 0.2898337596602726\n",
      "Train Error: 0.04199348618893947, Test Error: 0.28131252743325347\n",
      "Train Error: 0.04238899956105535, Test Error: 0.27588928916572086\n",
      "Train Error: 0.042257547258732125, Test Error: 0.2716087464261681\n",
      "Train Error: 0.04102095237215604, Test Error: 0.28499184343706585\n",
      "Train Error: 0.042336279890273526, Test Error: 0.2692437455992899\n",
      "Train Error: 0.04261125182980658, Test Error: 0.26412192916640354\n",
      "Train Error: 0.043350850696438605, Test Error: 0.2636689681795054\n",
      "Train Error: 0.04128392702075673, Test Error: 0.28394520219211883\n",
      "Train Error: 0.042498635086117606, Test Error: 0.26542241615269996\n",
      "Train Error: 0.043256850032613786, Test Error: 0.2632705926401203\n",
      "Train Error: 0.04473523137429313, Test Error: 0.26909193483504273\n",
      "Train Error: 0.23756926310486448, Test Error: 0.3934222927396534\n",
      "Train Error: 0.23772206471205243, Test Error: 0.39252886481914134\n",
      "Train Error: 0.23779915653416822, Test Error: 0.3920736398186634\n",
      "Train Error: 0.23787186505222835, Test Error: 0.39161534399545517\n",
      "Train Error: 0.23762627718664428, Test Error: 0.39306571715924876\n",
      "Train Error: 0.23799443490446018, Test Error: 0.39091313343413614\n",
      "Train Error: 0.2381693641703721, Test Error: 0.3900858926892367\n",
      "Train Error: 0.23834852188009628, Test Error: 0.38926599333796136\n",
      "Train Error: 0.23771915979334488, Test Error: 0.3924542668387751\n",
      "Train Error: 0.238437713722937, Test Error: 0.38881879092306515\n",
      "Train Error: 0.23881111421931545, Test Error: 0.3872049352906208\n",
      "Train Error: 0.23920073775114126, Test Error: 0.3856170758788198\n",
      "Train Error: 0.23781288742882628, Test Error: 0.39183969542077035\n",
      "Train Error: 0.23890554907902972, Test Error: 0.38676643119968623\n",
      "Train Error: 0.23950082800647662, Test Error: 0.38440845295308285\n",
      "Train Error: 0.2401305769719814, Test Error: 0.3821105951764686\n",
      "Train Error: 0.23790247718444166, Test Error: 0.39122320462303606\n",
      "Train Error: 0.23939731595648178, Test Error: 0.38476077651935237\n",
      "Train Error: 0.24023683075589752, Test Error: 0.3816993106430335\n",
      "Train Error: 0.2411366397180406, Test Error: 0.3787460282376605\n",
      "Train Error: 0.2382459738669961, Test Error: 0.38939392205432044\n",
      "Train Error: 0.24153076620110983, Test Error: 0.3773637477598996\n",
      "Train Error: 0.24270599416156743, Test Error: 0.3739439672477568\n",
      "Train Error: 0.24404830864533772, Test Error: 0.37077430651907733\n",
      "Train Error: 0.23842348304810726, Test Error: 0.3885026345788817\n",
      "Train Error: 0.2422394740511758, Test Error: 0.37514417546758844\n",
      "Train Error: 0.24386679107907414, Test Error: 0.3711060702921282\n",
      "Train Error: 0.24575032717684542, Test Error: 0.3674863786433187\n",
      "Train Error: 0.26265319314582675, Test Error: 0.3054353311415639\n",
      "Train Error: 0.2625399842858612, Test Error: 0.30554492088961527\n",
      "Train Error: 0.2624843046852531, Test Error: 0.3055791876320944\n",
      "Train Error: 0.2624282537302768, Test Error: 0.30561064231929325\n",
      "Train Error: 0.26260729311876074, Test Error: 0.30550270747948866\n",
      "Train Error: 0.2623319288927687, Test Error: 0.30568105305605814\n",
      "Train Error: 0.2622098235021836, Test Error: 0.30576897366373\n",
      "Train Error: 0.2620961878848373, Test Error: 0.3058641283253236\n",
      "Train Error: 0.26252435732981616, Test Error: 0.30558153256661763\n",
      "Train Error: 0.26204403525063885, Test Error: 0.30600708510309016\n",
      "Train Error: 0.26195547224996846, Test Error: 0.30690712230809136\n",
      "Train Error: 0.2618780386968153, Test Error: 0.3078286979707926\n",
      "Train Error: 0.2624461436161375, Test Error: 0.3056465031316945\n",
      "Train Error: 0.2619277253748692, Test Error: 0.307153033337009\n",
      "Train Error: 0.2618188631994857, Test Error: 0.3085621781563448\n",
      "Train Error: 0.2617318938458407, Test Error: 0.309956974040152\n",
      "Train Error: 0.262368047723278, Test Error: 0.3057087426608046\n",
      "Train Error: 0.2618280253472269, Test Error: 0.3083562485287152\n",
      "Train Error: 0.26171216833281685, Test Error: 0.310222070516421\n",
      "Train Error: 0.26163430088037937, Test Error: 0.3121166933246238\n",
      "Train Error: 0.2620913944230867, Test Error: 0.30598313856796955\n",
      "Train Error: 0.26158230013435024, Test Error: 0.31318569358334103\n",
      "Train Error: 0.26158224783562445, Test Error: 0.3170072199487036\n",
      "Train Error: 0.2617341602912326, Test Error: 0.32089094220398695\n",
      "Train Error: 0.2619824497507234, Test Error: 0.30622932094419475\n",
      "Train Error: 0.2615512447544169, Test Error: 0.3156216430213099\n",
      "Train Error: 0.2616945414473209, Test Error: 0.3204628347286867\n",
      "Train Error: 0.26207332711407716, Test Error: 0.3253721489176575\n",
      "Train Error: 0.2506635877780429, Test Error: 0.34907829099659093\n",
      "Train Error: 0.2505345892202272, Test Error: 0.34948512422580463\n",
      "Train Error: 0.25046372128757455, Test Error: 0.3496851483339951\n",
      "Train Error: 0.2504251291571802, Test Error: 0.349912306193334\n",
      "Train Error: 0.2506217797034635, Test Error: 0.34922741825086445\n",
      "Train Error: 0.25051423812634604, Test Error: 0.3503786262068855\n",
      "Train Error: 0.2506401596861275, Test Error: 0.35105432441215256\n",
      "Train Error: 0.25076796064667944, Test Error: 0.3517305694889882\n",
      "Train Error: 0.2505330700773293, Test Error: 0.34947592770062064\n",
      "Train Error: 0.2508385897688474, Test Error: 0.352056271687132\n",
      "Train Error: 0.2511107265874084, Test Error: 0.35341059706831895\n",
      "Train Error: 0.2513902823291381, Test Error: 0.3547669499350328\n",
      "Train Error: 0.25044444003691974, Test Error: 0.34969628879690984\n",
      "Train Error: 0.25118496271742613, Test Error: 0.3537368923746884\n",
      "Train Error: 0.2516137797561225, Test Error: 0.35577456092048065\n",
      "Train Error: 0.2520683991185393, Test Error: 0.3578192569931275\n",
      "Train Error: 0.25046238491842393, Test Error: 0.35000589266093785\n",
      "Train Error: 0.25154507629951534, Test Error: 0.3554219508090438\n",
      "Train Error: 0.25215166294926067, Test Error: 0.35814811789703693\n",
      "Train Error: 0.25279593734470246, Test Error: 0.3608835550318722\n",
      "Train Error: 0.250731829942775, Test Error: 0.3513060435421573\n",
      "Train Error: 0.2531583795781196, Test Error: 0.36220530397509154\n",
      "Train Error: 0.2546162200092572, Test Error: 0.3677022297615305\n",
      "Train Error: 0.2562302900854836, Test Error: 0.3732024960747775\n",
      "Train Error: 0.25087344386545085, Test Error: 0.35195705387468995\n",
      "Train Error: 0.25406388772214994, Test Error: 0.3656139103400685\n",
      "Train Error: 0.2560348534459158, Test Error: 0.37248899130073027\n",
      "Train Error: 0.2582405041131221, Test Error: 0.3793410659052798\n",
      "Train Error: 0.23785682420402185, Test Error: 0.17334372594909933\n",
      "Train Error: 0.23819712326905906, Test Error: 0.1732311902608717\n",
      "Train Error: 0.2383605278107862, Test Error: 0.17318048006605377\n",
      "Train Error: 0.23852137259039147, Test Error: 0.1731291230357895\n",
      "Train Error: 0.23800026617418102, Test Error: 0.17329031359947664\n",
      "Train Error: 0.23880991500496254, Test Error: 0.17303351307567835\n",
      "Train Error: 0.2392219658168512, Test Error: 0.17289690642355546\n",
      "Train Error: 0.23963571419168245, Test Error: 0.17276143954108267\n",
      "Train Error: 0.23823083657244534, Test Error: 0.17320598630746475\n",
      "Train Error: 0.2398652122018419, Test Error: 0.1726783283592605\n",
      "Train Error: 0.24069899774598594, Test Error: 0.17241214781371958\n",
      "Train Error: 0.24153290385270745, Test Error: 0.17215560938713595\n",
      "Train Error: 0.23845443005424447, Test Error: 0.17312917265841454\n",
      "Train Error: 0.24093073838582743, Test Error: 0.17233063357186187\n",
      "Train Error: 0.24219036103332708, Test Error: 0.1719465648822991\n",
      "Train Error: 0.24347040898518654, Test Error: 0.17156735648883717\n",
      "Train Error: 0.23867634857481307, Test Error: 0.1730490716478091\n",
      "Train Error: 0.24200078334369438, Test Error: 0.17199524980816142\n",
      "Train Error: 0.24370793230826004, Test Error: 0.17148977811841512\n",
      "Train Error: 0.24543314002281094, Test Error: 0.1710068841212572\n",
      "Train Error: 0.23958190666901477, Test Error: 0.17271878349369002\n",
      "Train Error: 0.24640084592396502, Test Error: 0.17070980297399083\n",
      "Train Error: 0.24995932441309587, Test Error: 0.16980681836636025\n",
      "Train Error: 0.25360252741124023, Test Error: 0.16897577944564676\n",
      "Train Error: 0.24003373011027335, Test Error: 0.17255869937464802\n",
      "Train Error: 0.2486615601536222, Test Error: 0.17010780677236226\n",
      "Train Error: 0.2531843131880012, Test Error: 0.16905159627763078\n",
      "Train Error: 0.25783746265260177, Test Error: 0.1681033221858456\n",
      "Train Error: 0.3024956794104864, Test Error: 0.16725886824030226\n",
      "Train Error: 0.30275335443708734, Test Error: 0.16742602846362337\n",
      "Train Error: 0.3028755762012619, Test Error: 0.16751085909085683\n",
      "Train Error: 0.30299065417640325, Test Error: 0.16759618931268655\n",
      "Train Error: 0.30260793996387236, Test Error: 0.16732750165577892\n",
      "Train Error: 0.3032216312032971, Test Error: 0.16775669149421082\n",
      "Train Error: 0.3035101168414233, Test Error: 0.16797959714222724\n",
      "Train Error: 0.30380111724335485, Test Error: 0.1682086977826633\n",
      "Train Error: 0.3027884919772543, Test Error: 0.16744332116622893\n",
      "Train Error: 0.3039695136041121, Test Error: 0.1683380792493953\n",
      "Train Error: 0.30455074362874984, Test Error: 0.1688224937231291\n",
      "Train Error: 0.3051375743272002, Test Error: 0.16933167525324358\n",
      "Train Error: 0.3029616000411061, Test Error: 0.16756077436975844\n",
      "Train Error: 0.3047202390384356, Test Error: 0.1689609440302101\n",
      "Train Error: 0.30560208430252433, Test Error: 0.16974432009002322\n",
      "Train Error: 0.30648417584440363, Test Error: 0.17058256978585667\n",
      "Train Error: 0.30313436638962177, Test Error: 0.16768030363906625\n",
      "Train Error: 0.3054780422357034, Test Error: 0.16962440037661758\n",
      "Train Error: 0.3066560585246842, Test Error: 0.17074346639321888\n",
      "Train Error: 0.30784672195340496, Test Error: 0.17195709987679988\n",
      "Train Error: 0.3038145394829529, Test Error: 0.16817854498822896\n",
      "Train Error: 0.3085351476824729, Test Error: 0.17267154389072722\n",
      "Train Error: 0.3109446625728006, Test Error: 0.17547072182089432\n",
      "Train Error: 0.3133916029918016, Test Error: 0.17860717872659793\n",
      "Train Error: 0.3041488437812293, Test Error: 0.1684392145769289\n",
      "Train Error: 0.3100831613828276, Test Error: 0.1744202446705912\n",
      "Train Error: 0.3131335421682405, Test Error: 0.17824380131429854\n",
      "Train Error: 0.31622194262349873, Test Error: 0.18256124823842593\n",
      "Train Error: 0.21461617701855493, Test Error: 0.1875212993602204\n",
      "Train Error: 0.2148662647824638, Test Error: 0.1874481127203441\n",
      "Train Error: 0.21498491701225905, Test Error: 0.18741384213444137\n",
      "Train Error: 0.21510395340368968, Test Error: 0.1873798967026774\n",
      "Train Error: 0.2147187898049048, Test Error: 0.18749128988612593\n",
      "Train Error: 0.2153228592784431, Test Error: 0.18731670553714294\n",
      "Train Error: 0.21562426888270705, Test Error: 0.18723468454470502\n",
      "Train Error: 0.21592699686055544, Test Error: 0.18715331448540157\n",
      "Train Error: 0.21489140433908047, Test Error: 0.18743580332872\n",
      "Train Error: 0.21609651686196643, Test Error: 0.18710666989406985\n",
      "Train Error: 0.21671628396240697, Test Error: 0.18695491858746477\n",
      "Train Error: 0.21734678184829323, Test Error: 0.18681134289514392\n",
      "Train Error: 0.21505573428488578, Test Error: 0.1873854370113093\n",
      "Train Error: 0.21688967924460617, Test Error: 0.18691104846328252\n",
      "Train Error: 0.2178438038998365, Test Error: 0.18670207217117224\n",
      "Train Error: 0.21881941944686642, Test Error: 0.18651028532345895\n",
      "Train Error: 0.21522073874777248, Test Error: 0.1873356543776726\n",
      "Train Error: 0.2177002776714288, Test Error: 0.18672859605562395\n",
      "Train Error: 0.2190023283141791, Test Error: 0.18647360389571102\n",
      "Train Error: 0.22034648055619818, Test Error: 0.1862528951250868\n",
      "Train Error: 0.21588832624225823, Test Error: 0.18714106188460056\n",
      "Train Error: 0.22108287916698594, Test Error: 0.18612430467994254\n",
      "Train Error: 0.22392727413563554, Test Error: 0.1858168195467592\n",
      "Train Error: 0.22693751779024274, Test Error: 0.18563418174236737\n",
      "Train Error: 0.21622796288290616, Test Error: 0.18704798947226406\n",
      "Train Error: 0.22288266674942017, Test Error: 0.18590477606200168\n",
      "Train Error: 0.22659098004174863, Test Error: 0.1856399143476656\n",
      "Train Error: 0.23054331437657383, Test Error: 0.1855702677510269\n",
      "Train Error: 0.16710397395506987, Test Error: 0.2972363494760608\n",
      "Train Error: 0.16700122305207343, Test Error: 0.297167135436984\n",
      "Train Error: 0.1669527615658752, Test Error: 0.29713702045624757\n",
      "Train Error: 0.16690624704278648, Test Error: 0.297109897467289\n",
      "Train Error: 0.16705909935033603, Test Error: 0.2972107344464433\n",
      "Train Error: 0.16682455321803105, Test Error: 0.29707200069992984\n",
      "Train Error: 0.1667246936732999, Test Error: 0.29703033832483694\n",
      "Train Error: 0.16666008108198382, Test Error: 0.2970401225994854\n",
      "Train Error: 0.16698705727527097, Test Error: 0.2971722620239304\n",
      "Train Error: 0.1666296444057398, Test Error: 0.2970652539730671\n",
      "Train Error: 0.16656258021245854, Test Error: 0.29715943566971126\n",
      "Train Error: 0.16652240287087725, Test Error: 0.2972723806872629\n",
      "Train Error: 0.1669184568985466, Test Error: 0.29713903625744964\n",
      "Train Error: 0.16654415285851584, Test Error: 0.29719195199167864\n",
      "Train Error: 0.166507225140886, Test Error: 0.2973767989832473\n",
      "Train Error: 0.16653667692212518, Test Error: 0.297605782563386\n",
      "Train Error: 0.16685329380625846, Test Error: 0.29711103555126256\n",
      "Train Error: 0.1665033546471976, Test Error: 0.29734927054947113\n",
      "Train Error: 0.16654393024264919, Test Error: 0.297655220657505\n",
      "Train Error: 0.16669824460567284, Test Error: 0.29803770083761044\n",
      "Train Error: 0.1666315680651162, Test Error: 0.2970585935352751\n",
      "Train Error: 0.16681003852015044, Test Error: 0.2982896253366798\n",
      "Train Error: 0.1675709567047432, Test Error: 0.29935936879829744\n",
      "Train Error: 0.16875894354535442, Test Error: 0.30071878323982293\n",
      "Train Error: 0.16657446773383164, Test Error: 0.2971102057428654\n",
      "Train Error: 0.16723377369611836, Test Error: 0.2989415673440733\n",
      "Train Error: 0.16859338147183406, Test Error: 0.30055395679576685\n",
      "Train Error: 0.17059007688103206, Test Error: 0.3026052493795254\n",
      "Train Error: 0.2125804875249237, Test Error: 0.28971787853386993\n",
      "Train Error: 0.2124360417793434, Test Error: 0.28831414583116494\n",
      "Train Error: 0.21241223918853377, Test Error: 0.28788464609874503\n",
      "Train Error: 0.2123925177029931, Test Error: 0.2874886335013163\n",
      "Train Error: 0.21252718154311764, Test Error: 0.28916617730817284\n",
      "Train Error: 0.2123628067132165, Test Error: 0.2867767259114458\n",
      "Train Error: 0.21232149250348237, Test Error: 0.28582544691569867\n",
      "Train Error: 0.21228431893372243, Test Error: 0.28486737314891514\n",
      "Train Error: 0.21244143037726335, Test Error: 0.28823127826859396\n",
      "Train Error: 0.21227283810283987, Test Error: 0.2843552222294979\n",
      "Train Error: 0.21221867994932858, Test Error: 0.2824919796522259\n",
      "Train Error: 0.21218500405113647, Test Error: 0.2806971937856346\n",
      "Train Error: 0.21241923424941594, Test Error: 0.2876794266203979\n",
      "Train Error: 0.21221388943709737, Test Error: 0.2819974874032894\n",
      "Train Error: 0.21217807780619447, Test Error: 0.27933445474516777\n",
      "Train Error: 0.2121850695576329, Test Error: 0.276763701286668\n",
      "Train Error: 0.21239992129455093, Test Error: 0.28714598501464333\n",
      "Train Error: 0.2121872501125101, Test Error: 0.2797392716508207\n",
      "Train Error: 0.21219721127769406, Test Error: 0.2763146383912037\n",
      "Train Error: 0.2122840695084267, Test Error: 0.2730929606981969\n",
      "Train Error: 0.21233797168128227, Test Error: 0.28507258931838814\n",
      "Train Error: 0.2123869252316508, Test Error: 0.2714515842310174\n",
      "Train Error: 0.21285523515220886, Test Error: 0.26582762723429304\n",
      "Train Error: 0.21360364712957772, Test Error: 0.26103476842502477\n",
      "Train Error: 0.2123152006082848, Test Error: 0.28405036588854243\n",
      "Train Error: 0.21266608265582276, Test Error: 0.26780928281561545\n",
      "Train Error: 0.2135121569828441, Test Error: 0.2615425114571145\n",
      "Train Error: 0.2148109141552428, Test Error: 0.2567916004523103\n",
      "Train Error: 0.4033765839898624, Test Error: 54.31616380725966\n",
      "Train Error: 0.4017796942723151, Test Error: 54.389820436980344\n",
      "Train Error: 0.4009407862440372, Test Error: 54.428684524303286\n",
      "Train Error: 0.40011929530342943, Test Error: 54.46756403290633\n",
      "Train Error: 0.4027526989233144, Test Error: 54.34949007046412\n",
      "Train Error: 0.39861401471479235, Test Error: 54.5441613109859\n",
      "Train Error: 0.39665008388490763, Test Error: 54.639139401040936\n",
      "Train Error: 0.39469940903098927, Test Error: 54.734328248681955\n",
      "Train Error: 0.401676790211503, Test Error: 54.405559977672816\n",
      "Train Error: 0.39367033431277687, Test Error: 54.79170339824694\n",
      "Train Error: 0.3898167882597217, Test Error: 54.98368145636809\n",
      "Train Error: 0.38809356403104084, Test Error: 55.2700950391301\n",
      "Train Error: 0.40057788297960906, Test Error: 54.46365447551121\n",
      "Train Error: 0.3893375072669122, Test Error: 55.0637898583275\n",
      "Train Error: 0.3867911764873633, Test Error: 55.494715938867145\n",
      "Train Error: 0.3843071519015434, Test Error: 55.92294523990169\n",
      "Train Error: 0.3994986295241736, Test Error: 54.52174408786421\n",
      "Train Error: 0.3871597042188352, Test Error: 55.43302262637681\n",
      "Train Error: 0.38384910354656104, Test Error: 56.00388819299347\n",
      "Train Error: 0.3805774640928295, Test Error: 56.572303606931136\n",
      "Train Error: 0.39525283945755074, Test Error: 54.75468860710429\n",
      "Train Error: 0.3787984158381133, Test Error: 56.89319227700594\n",
      "Train Error: 0.3725808894065313, Test Error: 58.01454818031022\n",
      "Train Error: 0.3666810679312089, Test Error: 59.12149207817049\n",
      "Train Error: 0.39317875297128574, Test Error: 54.87077382348131\n",
      "Train Error: 0.3748040438017347, Test Error: 57.61412064365873\n",
      "Train Error: 0.36731938307511475, Test Error: 59.00234354038368\n",
      "Train Error: 0.36032138671932007, Test Error: 60.368925509537036\n",
      "Train Error: 5.136075954615009, Test Error: 2.4751280241444973\n",
      "Train Error: 5.117228407799606, Test Error: 2.466274122605148\n",
      "Train Error: 5.1077601206619585, Test Error: 2.461646467265618\n",
      "Train Error: 5.082790350297518, Test Error: 2.4564195660028036\n",
      "Train Error: 5.121551288418256, Test Error: 2.472595165853614\n",
      "Train Error: 5.026884985408569, Test Error: 2.4473997476540776\n",
      "Train Error: 4.954681428890451, Test Error: 2.4354683794950946\n",
      "Train Error: 4.902319581246835, Test Error: 2.43597761367723\n",
      "Train Error: 5.103108121121568, Test Error: 2.4664967453967086\n",
      "Train Error: 4.866902958954102, Test Error: 2.436313434236623\n",
      "Train Error: 4.764576684055442, Test Error: 2.436872966268779\n",
      "Train Error: 4.662664122931434, Test Error: 2.4374267278056614\n",
      "Train Error: 5.078112909849706, Test Error: 2.4606488902033647\n",
      "Train Error: 4.7299701431616965, Test Error: 2.437076039843048\n",
      "Train Error: 4.576173396759053, Test Error: 2.438053501415204\n",
      "Train Error: 4.42209829164214, Test Error: 2.439098441420762\n",
      "Train Error: 5.0339170359870815, Test Error: 2.4540689925663655\n",
      "Train Error: 4.591949703535359, Test Error: 2.438075843130667\n",
      "Train Error: 4.385885021046769, Test Error: 2.4395425768022396\n",
      "Train Error: 4.1590718971902145, Test Error: 2.442618223892265\n",
      "Train Error: 4.862795858871306, Test Error: 2.4364873135062743\n",
      "Train Error: 4.018653159441548, Test Error: 2.44457827244695\n",
      "Train Error: 3.616315212883439, Test Error: 2.454556555756258\n",
      "Train Error: 3.1381675154747035, Test Error: 2.465196484137204\n",
      "Train Error: 4.791475252142044, Test Error: 2.4372770144066136\n",
      "Train Error: 3.7466644271547134, Test Error: 2.4511507722284875\n",
      "Train Error: 3.1857116454644556, Test Error: 2.46413257051115\n",
      "Train Error: 2.5807281093132635, Test Error: 2.4778586681903687\n",
      "Train Error: 1.652452802936096, Test Error: 1.2778289418651838\n",
      "Train Error: 1.6424346147461446, Test Error: 1.2782642414334053\n",
      "Train Error: 1.6404659472729752, Test Error: 1.2781303402677284\n",
      "Train Error: 1.6392207263632401, Test Error: 1.2779439082633086\n",
      "Train Error: 1.6478943363864629, Test Error: 1.2775346485006531\n",
      "Train Error: 1.6359630562397018, Test Error: 1.2775756187670007\n",
      "Train Error: 1.6369675157765209, Test Error: 1.2772803375324753\n",
      "Train Error: 1.640730216486171, Test Error: 1.2765594460308594\n",
      "Train Error: 1.6369235014606884, Test Error: 1.2777663922843183\n",
      "Train Error: 1.6432947748453328, Test Error: 1.2753222167430633\n",
      "Train Error: 1.6660818593982782, Test Error: 1.2724114738782657\n",
      "Train Error: 1.70656930221279, Test Error: 1.2695977914328511\n",
      "Train Error: 1.6322800347895632, Test Error: 1.2772355283657542\n",
      "Train Error: 1.676032256096875, Test Error: 1.2713826976039921\n",
      "Train Error: 1.7540287523103595, Test Error: 1.2670732080620457\n",
      "Train Error: 1.8654743050847513, Test Error: 1.2629200865242856\n",
      "Train Error: 1.6290577750433382, Test Error: 1.2767385485743807\n",
      "Train Error: 1.7415053058188643, Test Error: 1.267451169378958\n",
      "Train Error: 1.8953005580152753, Test Error: 1.2618593798206577\n",
      "Train Error: 2.112124069345327, Test Error: 1.2571194692020229\n",
      "Train Error: 1.6334257087644628, Test Error: 1.2748003848744798\n",
      "Train Error: 2.209591213664129, Test Error: 1.251022855088993\n",
      "Train Error: 2.4861232732928706, Test Error: 1.2328409134953495\n",
      "Train Error: 2.7944382525906697, Test Error: 1.2146137499370226\n",
      "Train Error: 1.645241332443988, Test Error: 1.2724866817654708\n",
      "Train Error: 2.400265568876348, Test Error: 1.2389921998656483\n",
      "Train Error: 2.779043990238025, Test Error: 1.2161303915334598\n",
      "Train Error: 3.1940647158016864, Test Error: 1.1934584447187415\n",
      "Train Error: 1.5376818429771357, Test Error: 0.2699933925035274\n",
      "Train Error: 1.547766401090523, Test Error: 0.2699503862057244\n",
      "Train Error: 1.5546743599190065, Test Error: 0.2699248680969827\n",
      "Train Error: 1.5616546119932373, Test Error: 0.26989914689758504\n",
      "Train Error: 1.5432490050370729, Test Error: 0.2699490350538307\n",
      "Train Error: 1.5781181059492633, Test Error: 0.2698210167057734\n",
      "Train Error: 1.607799356949867, Test Error: 0.26973085991806417\n",
      "Train Error: 1.632544612493505, Test Error: 0.2696654735770744\n",
      "Train Error: 1.5538796412249722, Test Error: 0.2698718364844596\n",
      "Train Error: 1.6498354455656206, Test Error: 0.2695862336602065\n",
      "Train Error: 1.7053707881646782, Test Error: 0.2694611207753574\n",
      "Train Error: 1.7653648306841458, Test Error: 0.2693413348147816\n",
      "Train Error: 1.5666444146131626, Test Error: 0.26979132243264553\n",
      "Train Error: 1.7256515167225368, Test Error: 0.2693825345369986\n",
      "Train Error: 1.8224080706499546, Test Error: 0.2692011058139616\n",
      "Train Error: 1.9285187690262422, Test Error: 0.26902758884251665\n",
      "Train Error: 1.5803564841222877, Test Error: 0.26970947272853146\n",
      "Train Error: 1.8122620722909364, Test Error: 0.2691820913347137\n",
      "Train Error: 1.9544959143038079, Test Error: 0.2689507424955347\n",
      "Train Error: 2.1147504945590168, Test Error: 0.26872488191285626\n",
      "Train Error: 1.6547817023895899, Test Error: 0.26935872409809136\n",
      "Train Error: 2.234217898228526, Test Error: 0.2684207078066268\n",
      "Train Error: 2.5990388091160463, Test Error: 0.2680089555620393\n",
      "Train Error: 3.001604183734513, Test Error: 0.2676272849818586\n",
      "Train Error: 1.6936451531021395, Test Error: 0.26919778889660745\n",
      "Train Error: 2.4750185956696624, Test Error: 0.26806453452794804\n",
      "Train Error: 2.9719514985833495, Test Error: 0.26757498724617035\n",
      "Train Error: 3.4729244167500335, Test Error: 0.26716887063807554\n",
      "Train Error: 40.21316880638783, Test Error: 0.20713421869299875\n",
      "Train Error: 40.10049184129916, Test Error: 0.20729777275178096\n",
      "Train Error: 40.04104641178858, Test Error: 0.207385124923509\n",
      "Train Error: 39.97593521748439, Test Error: 0.2074778462122872\n",
      "Train Error: 40.15634543564702, Test Error: 0.20717688927605696\n",
      "Train Error: 39.854162694592716, Test Error: 0.2076214493606516\n",
      "Train Error: 39.698920132542774, Test Error: 0.2078601184541899\n",
      "Train Error: 39.545584164555095, Test Error: 0.20809974896850914\n",
      "Train Error: 40.06121667163592, Test Error: 0.2072480512466821\n",
      "Train Error: 39.44490593509382, Test Error: 0.20818947727367146\n",
      "Train Error: 39.154687763412625, Test Error: 0.20848695235677753\n",
      "Train Error: 38.890367752932455, Test Error: 0.2085442778387518\n",
      "Train Error: 39.96289597579579, Test Error: 0.2073253905734171\n",
      "Train Error: 39.06346837442485, Test Error: 0.20846931463196441\n",
      "Train Error: 38.66827483253407, Test Error: 0.20855978253387616\n",
      "Train Error: 38.27588993134366, Test Error: 0.20866273589335396\n",
      "Train Error: 39.85671355414346, Test Error: 0.2074131102008544\n",
      "Train Error: 38.71011337682113, Test Error: 0.20851166574143004\n",
      "Train Error: 38.18708574852382, Test Error: 0.20864962983170154\n",
      "Train Error: 37.64816742874536, Test Error: 0.20884777032253313\n",
      "Train Error: 39.45239420692111, Test Error: 0.20776316169508452\n",
      "Train Error: 37.30959251994222, Test Error: 0.20878817645122266\n",
      "Train Error: 36.261849057761225, Test Error: 0.2092540441000825\n",
      "Train Error: 35.23705719950369, Test Error: 0.20979218430023944\n",
      "Train Error: 39.25138941887958, Test Error: 0.20794340814901638\n",
      "Train Error: 36.601644592936815, Test Error: 0.20902061356105114\n",
      "Train Error: 35.31889362032259, Test Error: 0.20966457705346395\n",
      "Train Error: 34.04964069129653, Test Error: 0.21046143853872434\n",
      "Train Error: 0.18304880011234312, Test Error: 0.16674323582136158\n",
      "Train Error: 0.1825013252487709, Test Error: 0.1671660360084465\n",
      "Train Error: 0.1822055025159303, Test Error: 0.16739315796162904\n",
      "Train Error: 0.18199360791551197, Test Error: 0.1676037355846927\n",
      "Train Error: 0.18281508132243054, Test Error: 0.1669357998225866\n",
      "Train Error: 0.18160352667297686, Test Error: 0.16801147858356058\n",
      "Train Error: 0.18109095123090085, Test Error: 0.16854180291284349\n",
      "Train Error: 0.18059073221102448, Test Error: 0.16907449108964862\n",
      "Train Error: 0.18243522140600776, Test Error: 0.16726804011904378\n",
      "Train Error: 0.1803079372163954, Test Error: 0.16940602490944515\n",
      "Train Error: 0.1793210303483488, Test Error: 0.1704927358330569\n",
      "Train Error: 0.17842112453210857, Test Error: 0.17157833981990137\n",
      "Train Error: 0.18204705675403104, Test Error: 0.1676109996410864\n",
      "Train Error: 0.17905671760323752, Test Error: 0.17082788836936785\n",
      "Train Error: 0.17774708519400514, Test Error: 0.17246381250564813\n",
      "Train Error: 0.17652067503153282, Test Error: 0.1741325530956028\n",
      "Train Error: 0.18174450999365846, Test Error: 0.16793838661761643\n",
      "Train Error: 0.17792559820965376, Test Error: 0.17225472177461731\n",
      "Train Error: 0.1763045393488765, Test Error: 0.1744762702144271\n",
      "Train Error: 0.1748976273628232, Test Error: 0.1767339003103067\n",
      "Train Error: 0.18057512984270885, Test Error: 0.1692566614881245\n",
      "Train Error: 0.17417783289248207, Test Error: 0.17814781013013556\n",
      "Train Error: 0.17229779683360913, Test Error: 0.18277124666550582\n",
      "Train Error: 0.1712690377243895, Test Error: 0.18752557345165327\n",
      "Train Error: 0.17997706045969689, Test Error: 0.16993080617169062\n",
      "Train Error: 0.17286934226668357, Test Error: 0.1811567595011703\n",
      "Train Error: 0.17133551979096376, Test Error: 0.18706237323018932\n",
      "Train Error: 0.17115669497332414, Test Error: 0.19314553006258992\n",
      "Percentile: 0.6, Tracking Error: 4.063077463133457\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MI_ElasticNet_plot(hourly_X,hourly_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca62a652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T12:19:23.441535Z",
     "iopub.status.busy": "2024-12-30T12:19:23.441152Z",
     "iopub.status.idle": "2024-12-30T12:19:23.446543Z",
     "shell.execute_reply": "2024-12-30T12:19:23.445418Z"
    },
    "papermill": {
     "duration": 0.182422,
     "end_time": "2024-12-30T12:19:23.448774",
     "exception": false,
     "start_time": "2024-12-30T12:19:23.266352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MI_XGBoost_plot(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac0dbb",
   "metadata": {
    "papermill": {
     "duration": 0.172955,
     "end_time": "2024-12-30T12:19:23.795955",
     "exception": false,
     "start_time": "2024-12-30T12:19:23.623000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sparse Autoencoder - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f414745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T12:19:24.151430Z",
     "iopub.status.busy": "2024-12-30T12:19:24.151065Z",
     "iopub.status.idle": "2024-12-30T12:19:24.161801Z",
     "shell.execute_reply": "2024-12-30T12:19:24.160777Z"
    },
    "papermill": {
     "duration": 0.191025,
     "end_time": "2024-12-30T12:19:24.164418",
     "exception": false,
     "start_time": "2024-12-30T12:19:23.973393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autoencoder_feature_selection(X, input_dim, encoding_dim, K):\n",
    "    # Autoencoder Model\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(input_dim, activation='relu', \n",
    "                    activity_regularizer=regularizers.l1(10e-2))(input_layer)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    # Train autoencoder\n",
    "    autoencoder.fit(X, X, epochs=50, batch_size=256, validation_split=0.1, verbose=0, shuffle=False)\n",
    "\n",
    "    # Calculate baseline reconstruction error\n",
    "    baseline_reconstruction = autoencoder.predict(X, verbose=0)\n",
    "    baseline_error = np.mean(np.square(X - baseline_reconstruction), axis=0)\n",
    "    \n",
    "    # Evaluate contribution of each stock\n",
    "    importance_scores = []\n",
    "    \n",
    "    for i in range(input_dim):\n",
    "        X_modified = X.copy()\n",
    "        X_modified[:, i] = 0  # Zero out the i-th feature\n",
    "        modified_reconstruction = autoencoder.predict(X_modified, verbose=0)\n",
    "        modified_error = np.mean(np.square(X - modified_reconstruction), axis=0)\n",
    "        information_loss = np.sum(modified_error - baseline_error)\n",
    "        importance_scores.append((i, information_loss))\n",
    "    \n",
    "    # Rank features based on importance scores\n",
    "    importance_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select top K features\n",
    "    top_feature_indices = [index for index, _ in importance_scores[:K]]\n",
    "    print(f\"Selected {len(top_feature_indices)} features\")\n",
    "    \n",
    "    return top_feature_indices, encoder, autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "060e560f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T12:19:24.542827Z",
     "iopub.status.busy": "2024-12-30T12:19:24.542413Z",
     "iopub.status.idle": "2024-12-30T12:19:24.553750Z",
     "shell.execute_reply": "2024-12-30T12:19:24.552445Z"
    },
    "papermill": {
     "duration": 0.209926,
     "end_time": "2024-12-30T12:19:24.555835",
     "exception": false,
     "start_time": "2024-12-30T12:19:24.345909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def AE_Experiment(K, X, y, model_class, params, debug=False):\n",
    "    tscv = TimeSeriesSplit(n_splits=15, max_train_size=15000, test_size=4500)\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_train_cv)\n",
    "        X_test_scaled = scaler.transform(X_test_cv)\n",
    "\n",
    "        # Create and train Sparse Autoencoder\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        features, autoencoder, encoder = autoencoder_feature_selection(X_scaled, input_dim, input_dim, K)\n",
    "\n",
    "        # Reduce the dataset to use only selected feature indices\n",
    "        X_train_reduced = X_scaled.copy()\n",
    "        X_test_reduced = X_test_scaled.copy()\n",
    "        mask = np.ones(X_train_reduced.shape[1], dtype=bool)\n",
    "        mask[features] = False\n",
    "        X_train_reduced[:, mask] = 0\n",
    "        X_test_reduced[:, mask] = 0\n",
    "    \n",
    "\n",
    "        # Encode the data\n",
    "        X_train_encoded = autoencoder.predict(X_train_reduced, verbose=0)\n",
    "        X_test_encoded = autoencoder.predict(X_test_reduced, verbose=0)\n",
    "\n",
    "        best_params = None\n",
    "        best_score = float('inf')\n",
    "        for param in ParameterGrid(params):\n",
    "            if debug:\n",
    "                print(f\"Trying combination {param}\")\n",
    "            model = model_class(**param)\n",
    "            model.fit(X_train_encoded, y_train_cv)\n",
    "            y_pred = model.predict(X_test_encoded)\n",
    "\n",
    "            train_fold_score = tracking_error(y_train_cv, model.predict(X_train_encoded))\n",
    "            fold_score = tracking_error(y_test_cv, y_pred)\n",
    "            if debug:\n",
    "                print(f\"Train Error: {train_fold_score}, Test Error: {fold_score}\")\n",
    "\n",
    "            if fold_score < best_score:\n",
    "                best_score = fold_score\n",
    "                best_params = param\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Best Params for this fold: {best_params}, Best Test Score: {best_score}\")\n",
    "        fold_scores.append(best_score)\n",
    "\n",
    "    average_score = np.mean(fold_scores)\n",
    "\n",
    "    print(f\"Number of stocks: {K}, Tracking Error: {average_score}\")\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "197420e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T12:19:24.987488Z",
     "iopub.status.busy": "2024-12-30T12:19:24.987031Z",
     "iopub.status.idle": "2024-12-30T12:19:24.995715Z",
     "shell.execute_reply": "2024-12-30T12:19:24.994632Z"
    },
    "papermill": {
     "duration": 0.260485,
     "end_time": "2024-12-30T12:19:24.998729",
     "exception": false,
     "start_time": "2024-12-30T12:19:24.738244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_AE_Lasso(X,y):\n",
    "    tracking_errors = []\n",
    "    num_stocks_list = []\n",
    "    \n",
    "    num_stocks = [20, 30, 40, 50, 60]\n",
    "    param_grid = {\n",
    "        'alpha': [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025],\n",
    "        'max_iter': [15000]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for K in num_stocks:\n",
    "        print(f\"Starting portofolio with {K} stocks\")\n",
    "        tracking_err = AE_Experiment(K, X, y, Lasso, param_grid, debug=True)\n",
    "        tracking_errors.append(tracking_err)\n",
    "\n",
    "    print(num_stocks_list, tracking_errors)\n",
    "    \n",
    "    # Plotting the results\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    # Plotting number of stocks on the primary y-axis\n",
    "    plt.plot(num_stocks, tracking_errors, marker='x')\n",
    "    plt.xlabel('Number of Stocks')\n",
    "    plt.ylabel('Tracking Error')\n",
    "    plt.title('AE&Lasso - Tracking Errors vs Number of Stocks ')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"AE-Lasso.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac6a44ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T12:19:25.413902Z",
     "iopub.status.busy": "2024-12-30T12:19:25.412504Z",
     "iopub.status.idle": "2024-12-30T13:43:43.442456Z",
     "shell.execute_reply": "2024-12-30T13:43:43.441219Z"
    },
    "papermill": {
     "duration": 5058.241942,
     "end_time": "2024-12-30T13:43:43.444980",
     "exception": false,
     "start_time": "2024-12-30T12:19:25.203038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting portofolio with 20 stocks\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.04946659870648567, Test Error: 0.8497358528228706\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.08016583478136073, Test Error: 0.7170650557712424\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.07423212879540173, Test Error: 0.6636342960639532\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.07605804760080705, Test Error: 0.6473876852022779\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.07840385854564295, Test Error: 0.6280862959646968\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.11650393402040514, Test Error: 0.6273285603453372\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.12855457991978966, Test Error: 0.6474268801217773\n",
      "Best Params for this fold: {'alpha': 0.002, 'max_iter': 15000}, Best Test Score: 0.6273285603453372\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6931682540193387, Test Error: 0.9282436035593677\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7222939776722078, Test Error: 0.841328749117368\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7364288074281077, Test Error: 0.7604703587148053\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7556617982351933, Test Error: 0.7497755452677614\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.78002749612305, Test Error: 0.7776069642833131\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8447020620593335, Test Error: 0.8878555781373259\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8491727983111034, Test Error: 0.8992196994613812\n",
      "Best Params for this fold: {'alpha': 0.00075, 'max_iter': 15000}, Best Test Score: 0.7497755452677614\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7578474986346747, Test Error: 0.9641352530319841\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7911330378657055, Test Error: 0.9688781146215422\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.8143637594665133, Test Error: 0.9958717666696834\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8272120926009688, Test Error: 1.0125499692430615\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.837415529187433, Test Error: 1.0263777572467976\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8638717189414717, Test Error: 1.0650870633220484\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8755130839003272, Test Error: 1.076667269789145\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.9641352530319841\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.8574655404151299, Test Error: 0.8266735253204895\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.8636086167620141, Test Error: 0.8178572330656783\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.8771985637304767, Test Error: 0.806126410772627\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8945995512426776, Test Error: 0.7986368932830331\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.9154964060801277, Test Error: 0.7960086594034794\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.949251714468526, Test Error: 0.8004139502187094\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.949251714468526, Test Error: 0.8004139502187094\n",
      "Best Params for this fold: {'alpha': 0.001, 'max_iter': 15000}, Best Test Score: 0.7960086594034794\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.8655871111535886, Test Error: 0.4644909258327091\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.8828861675006319, Test Error: 0.4675955071037556\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.897410783141114, Test Error: 0.47170654813714324\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.9111656310569689, Test Error: 0.47627678025306175\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.9144654984953637, Test Error: 0.4788909862910179\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.9385246619622221, Test Error: 0.491392892514707\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.9449432212113063, Test Error: 0.49408523433479323\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.4644909258327091\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7649027540376104, Test Error: 0.6660294612595582\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7756965782323639, Test Error: 0.6999820839626993\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7899447426565301, Test Error: 0.74154518792886\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8067987945390269, Test Error: 0.7847164422624011\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8241090430146868, Test Error: 0.8281310589860371\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8439234993490597, Test Error: 0.8729176589648964\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8439234993490597, Test Error: 0.8729176589648964\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.6660294612595582\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6202670080678667, Test Error: 0.5999077055379369\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.634279493314052, Test Error: 0.5916964791668\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6487420836340783, Test Error: 0.5854411071931912\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.6641695680952583, Test Error: 0.5835251683948285\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.6805330096326128, Test Error: 0.5857606363283845\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.724751511670933, Test Error: 0.6067068266463878\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.724751511670933, Test Error: 0.6067068266463878\n",
      "Best Params for this fold: {'alpha': 0.00075, 'max_iter': 15000}, Best Test Score: 0.5835251683948285\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.5941845587464134, Test Error: 0.7863219996222245\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.619736201818455, Test Error: 0.8060520309311296\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6647260645336873, Test Error: 0.8603989764822867\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7863219996222245\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6801496602591353, Test Error: 1.5430294545769687\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.6975745974029027, Test Error: 1.5481057239525964\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7225289626962765, Test Error: 1.551868385950747\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7526148402654159, Test Error: 1.5561047050161407\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 1.5430294545769687\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 1.043728643145549, Test Error: 116.37400301970187\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 1.0596056872554311, Test Error: 117.59997647449953\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 1.0781316882124203, Test Error: 117.89119496814597\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 1.0912792843127745, Test Error: 118.05920296784582\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 1.104677884922435, Test Error: 118.19693135698675\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 1.121658673014405, Test Error: 118.33432326825329\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 1.121658673014405, Test Error: 118.33432326825329\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 116.37400301970187\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.63568334676684, Test Error: 5.435926281179368\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.65775110333482, Test Error: 5.486421434909383\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.68309225444773, Test Error: 5.487666166483776\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.70832309851949, Test Error: 5.488947243485299\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.73331525739711, Test Error: 5.49025172515757\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.83085991572474, Test Error: 5.495685772233266\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.87585768984974, Test Error: 5.498561894785007\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 5.435926281179368\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.78930370996669, Test Error: 2.1761962631208394\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.6962250021637, Test Error: 2.1825261947068606\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.69273059161765, Test Error: 2.183465668835819\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.70948566391216, Test Error: 2.1829013481110406\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.72409968769924, Test Error: 2.182583025599898\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.78375825102603, Test Error: 2.1824527555365356\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.81501610162341, Test Error: 2.182615854779112\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 2.1761962631208394\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.35582703169896, Test Error: 0.7397335809839196\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.64876116503488, Test Error: 0.7572549933074525\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.66987605409327, Test Error: 0.758459417341873\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.69070008494523, Test Error: 0.7598470145955403\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.71123286528214, Test Error: 0.7614281006597321\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.79054099946477, Test Error: 0.7699528092286783\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.82853845501324, Test Error: 0.7757070345571573\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7397335809839196\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.6636222748278, Test Error: 1.0967488819222986\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.65166020015357, Test Error: 1.1046421179881216\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.66608428348133, Test Error: 1.1048208554697154\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.68041922366713, Test Error: 1.1050323559744137\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.69466693454646, Test Error: 1.1052765859899267\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.75075615378769, Test Error: 1.1065803045938845\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.77825138022553, Test Error: 1.1074287943852124\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 1.0967488819222986\n",
      "Selected 20 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 1.3957875849060224, Test Error: 0.95535192052504\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 1.4037172791361248, Test Error: 0.9695080944922996\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 1.4077668868175286, Test Error: 0.9803302695541593\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 1.4095119700393937, Test Error: 0.982952775797101\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 1.4114389050925624, Test Error: 0.9867196573934168\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 1.4242584126552422, Test Error: 1.0116865988694892\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 1.4341345073649967, Test Error: 1.0295171236891785\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.95535192052504\n",
      "Number of stocks: 20, Tracking Error: 8.930573665011213\n",
      "Starting portofolio with 30 stocks\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.017154561089509704, Test Error: 0.8535586052275214\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.035340052372979804, Test Error: 0.8162634907041595\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.05447123145537301, Test Error: 0.7252290762866173\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.0620218400641051, Test Error: 0.6753458227355082\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.061493768999042936, Test Error: 0.6442879070251949\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.07114286999183145, Test Error: 0.5045755809209193\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.07413739025371875, Test Error: 0.5034481071333388\n",
      "Best Params for this fold: {'alpha': 0.0025, 'max_iter': 15000}, Best Test Score: 0.5034481071333388\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7069748330535112, Test Error: 0.7898404771870076\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7246740006863319, Test Error: 0.7955772487520977\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7428014422797867, Test Error: 0.7815377024141098\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7520896598860942, Test Error: 0.7749187165792076\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7602383409736228, Test Error: 0.774974332101733\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8049354826309132, Test Error: 0.8229622617374317\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8144143478988908, Test Error: 0.8408539155022171\n",
      "Best Params for this fold: {'alpha': 0.00075, 'max_iter': 15000}, Best Test Score: 0.7749187165792076\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7641114752522347, Test Error: 0.9530462574680846\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7909438167041226, Test Error: 0.9690903612849343\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7985507710968116, Test Error: 0.9826007968834632\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8091124554284493, Test Error: 0.998197655738898\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8217729286638632, Test Error: 1.0158980549524261\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8755130839003272, Test Error: 1.076667269789145\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8755130839003272, Test Error: 1.076667269789145\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.9530462574680846\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.8403275987215122, Test Error: 0.7572539533830769\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.8490040599094217, Test Error: 0.7746895304265041\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.8687995419883248, Test Error: 0.7927955026999216\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8922469314258863, Test Error: 0.7958640052165018\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.9147112929049556, Test Error: 0.7963183530117858\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.949251714468526, Test Error: 0.8004139502187094\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.949251714468526, Test Error: 0.8004139502187094\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7572539533830769\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.8732554907867799, Test Error: 0.4529686759875109\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.8770526532568338, Test Error: 0.4595288031816178\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.8858651231397766, Test Error: 0.4642522706539316\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8958424427402728, Test Error: 0.4685208602432667\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.9055626539456556, Test Error: 0.4731311423154853\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.9449432212113063, Test Error: 0.49408523433479323\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.9449432212113063, Test Error: 0.49408523433479323\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.4529686759875109\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7422340342899839, Test Error: 0.6103809404995292\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7457174104514057, Test Error: 0.6226592971015763\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7520740000899009, Test Error: 0.6440241043903459\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7590717738007644, Test Error: 0.6663500969306165\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7666829876337129, Test Error: 0.6894723955215227\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8035883334793031, Test Error: 0.7875160894058163\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8266865997424644, Test Error: 0.8387760651165572\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.6103809404995292\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6047147559471657, Test Error: 0.6019544974505114\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.61270829709022, Test Error: 0.5966919609995303\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6263513441414442, Test Error: 0.5863651496348026\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.6402475004495822, Test Error: 0.5798414726031529\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.6543645296778526, Test Error: 0.5770535855976027\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.715094076696245, Test Error: 0.5997096740216638\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.724751511670933, Test Error: 0.6067068266463878\n",
      "Best Params for this fold: {'alpha': 0.001, 'max_iter': 15000}, Best Test Score: 0.5770535855976027\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.5821800896798701, Test Error: 0.7820889636107979\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.6075302337257699, Test Error: 0.7947733713319737\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.633683079904478, Test Error: 0.8200509481340558\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.6601994096538187, Test Error: 0.8537278559641717\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7820889636107979\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6530177053654422, Test Error: 1.5298409782763789\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.6711944889946676, Test Error: 1.5367936589371856\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6927176958785498, Test Error: 1.5412884239332518\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7175884258133345, Test Error: 1.545756495189222\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7463801755073964, Test Error: 1.5519436683520262\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 1.5298409782763789\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 1.0524627095977417, Test Error: 117.41730119815783\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 1.0631113433990447, Test Error: 117.67920966439074\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 1.078199587613725, Test Error: 117.91975044059959\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 1.0935557203322246, Test Error: 118.09544044923106\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 1.1115079702484296, Test Error: 118.25845184762692\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 1.121658673014405, Test Error: 118.33432326825329\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 1.121658673014405, Test Error: 118.33432326825329\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 117.41730119815783\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 59.6072873149351, Test Error: 5.418225934909861\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 60.975254039847904, Test Error: 5.445175147305025\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 62.81574099437065, Test Error: 5.45571855911881\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.30149343020594, Test Error: 5.467997264761306\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.68525883156087, Test Error: 5.4726032260048285\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.72620225766993, Test Error: 5.476107603762585\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.74621444889995, Test Error: 5.477907457334442\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 5.418225934909861\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.69715412112754, Test Error: 2.155502301653155\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.67280290406833, Test Error: 2.1629138750258994\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.69041263992027, Test Error: 2.1634638366580776\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.70785530798241, Test Error: 2.1640173352652234\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.72513054412703, Test Error: 2.1645742809932207\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.77951347027172, Test Error: 2.17107680583722\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.80265921623999, Test Error: 2.1758505578088188\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 2.155502301653155\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 56.2561699628812, Test Error: 0.7381458450386351\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 59.49389415593191, Test Error: 0.7461133511336357\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 62.937972658498886, Test Error: 0.7464890715047028\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.63219717957419, Test Error: 0.7470795506488283\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.69340873293844, Test Error: 0.7479307336896605\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.75573233296424, Test Error: 0.7516117923248848\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.78592696895953, Test Error: 0.7536021722207086\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7381458450386351\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 63.70599254638091, Test Error: 1.0786688369655546\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.61764429413329, Test Error: 1.0847057631460637\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.62662682728508, Test Error: 1.0881717723246347\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.62846249573678, Test Error: 1.0913220420735041\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.63707167506493, Test Error: 1.0927853382884833\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.69360389806631, Test Error: 1.0891412698182517\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.71790335005909, Test Error: 1.089976364973833\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 1.0786688369655546\n",
      "Selected 30 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 1.395591443186124, Test Error: 0.9632623799143639\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 1.3972922851568501, Test Error: 0.9635523481720945\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 1.401467198304463, Test Error: 0.9679324214300704\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 1.4062843522045672, Test Error: 0.9766530673881145\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 1.4110580373376607, Test Error: 0.9870340063572735\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 1.4325056207110927, Test Error: 1.027095314808612\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 1.448434424765091, Test Error: 1.052739289185346\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.9632623799143639\n",
      "Number of stocks: 30, Tracking Error: 8.98080711167833\n",
      "Starting portofolio with 40 stocks\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.009735497593870598, Test Error: 0.7861084720666194\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.020500717076151583, Test Error: 0.7064795670401924\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.036661549949445475, Test Error: 0.666815401506882\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.05530636644974607, Test Error: 0.5830504869075058\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.06500533083820814, Test Error: 0.5907123138155022\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.07359442928689876, Test Error: 0.584393277112632\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.0777307223231496, Test Error: 0.7203461748274845\n",
      "Best Params for this fold: {'alpha': 0.00075, 'max_iter': 15000}, Best Test Score: 0.5830504869075058\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6988853576209096, Test Error: 0.8194364034045285\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7427617291624004, Test Error: 0.784328464272236\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7513803155688253, Test Error: 0.7834949597419871\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7612792618541, Test Error: 0.7862776372877065\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7739047499124898, Test Error: 0.7966729172071765\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8124103299307275, Test Error: 0.8438523370528412\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8369599500386168, Test Error: 0.8799344323450922\n",
      "Best Params for this fold: {'alpha': 0.0005, 'max_iter': 15000}, Best Test Score: 0.7834949597419871\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7634527220368146, Test Error: 0.9422753774276346\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.775904538640271, Test Error: 0.9584355939707075\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7866323860508969, Test Error: 0.9763499324485506\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8007382230855293, Test Error: 0.9923182615213386\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8112468160882713, Test Error: 1.0053134210111387\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8565221464174576, Test Error: 1.0559593902791709\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8741923420490814, Test Error: 1.0753903363058341\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.9422753774276346\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.8117635141597852, Test Error: 0.738391423153274\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.8209524720307986, Test Error: 0.7388349968085662\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.8375106422132105, Test Error: 0.7494138870129587\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8460904620732607, Test Error: 0.7587900781401007\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8559936719772165, Test Error: 0.7664713886932001\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.9079632275014079, Test Error: 0.7854071541568434\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.9417892980978315, Test Error: 0.7970198981261747\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.738391423153274\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.8406280527227338, Test Error: 0.45835628283955526\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.8449492549490438, Test Error: 0.457759050091019\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.8509961253540462, Test Error: 0.45781762139625426\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8578986934547895, Test Error: 0.4587405875005691\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8656427336463706, Test Error: 0.4605085118592969\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.9051335787244393, Test Error: 0.47552833258151006\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.9306398157007212, Test Error: 0.48738691990775185\n",
      "Best Params for this fold: {'alpha': 0.00025, 'max_iter': 15000}, Best Test Score: 0.457759050091019\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7624329974790915, Test Error: 0.6244351208140683\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7664339293486936, Test Error: 0.6440605663385529\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7745412514779887, Test Error: 0.6786082753419596\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7844494122677329, Test Error: 0.7149902342052434\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7961964936342789, Test Error: 0.7527713549714541\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8439234993490597, Test Error: 0.8729176589648964\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8439234993490597, Test Error: 0.8729176589648964\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.6244351208140683\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6106770595588237, Test Error: 0.5835070336425093\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.6205549162720227, Test Error: 0.5793211442358103\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6344302389295831, Test Error: 0.5749010210454446\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.6486662513661364, Test Error: 0.5752668992318113\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.6626248151793158, Test Error: 0.5780890819789827\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.724751511670933, Test Error: 0.6067068266463878\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.724751511670933, Test Error: 0.6067068266463878\n",
      "Best Params for this fold: {'alpha': 0.0005, 'max_iter': 15000}, Best Test Score: 0.5749010210454446\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.5562768247436876, Test Error: 0.7693578704386903\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.5680886765876733, Test Error: 0.7718541042538632\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.5914842494226279, Test Error: 0.7840007292835666\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.6190396535732889, Test Error: 0.8068444118428036\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.650298793593854, Test Error: 0.8420216578713597\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7693578704386903\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6650974700756941, Test Error: 1.5366882293411062\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.6836558701508547, Test Error: 1.53873928767578\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7211894250988828, Test Error: 1.547277843827729\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7527643619517643, Test Error: 1.55447367578966\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 1.5366882293411062\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 1.0283020428208045, Test Error: 117.31547846649728\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 1.034866699410967, Test Error: 117.43343397196116\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 1.048079943927922, Test Error: 117.63261983803626\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 1.0630724075426312, Test Error: 117.8121734079342\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 1.0793358043592163, Test Error: 117.97629384734239\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 1.121658673014405, Test Error: 118.33432326825329\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 1.121658673014405, Test Error: 118.33432326825329\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 117.31547846649728\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.29703849694965, Test Error: 5.469693575283877\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.62995636869138, Test Error: 5.475081816265852\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.64811606843912, Test Error: 5.477598395606813\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.66526181291194, Test Error: 5.479079227675036\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.67741183245307, Test Error: 5.480280349060816\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.72479376678686, Test Error: 5.483948412578134\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.74861070874367, Test Error: 5.485722629651612\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 5.469693575283877\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.61468150023673, Test Error: 2.1535949920445727\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.6327499014238, Test Error: 2.1645321654675382\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.6466854291695, Test Error: 2.164622980700766\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.66050198679103, Test Error: 2.1647341816388517\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.67419750551652, Test Error: 2.164864654457639\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.72769155831611, Test Error: 2.1655654210549464\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.75430849583776, Test Error: 2.16631628486475\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 2.1535949920445727\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.62687989824497, Test Error: 0.7346366633299138\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.61079158577627, Test Error: 0.7402745519961723\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.62186495656894, Test Error: 0.7423429989367778\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.63846903591072, Test Error: 0.7430185280717312\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.65490784655701, Test Error: 0.74372749208011\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.71901555808884, Test Error: 0.7468870676843619\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.75008350477253, Test Error: 0.7486543063993297\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7346366633299138\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.53873173739305, Test Error: 1.0702225722803902\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.58402359850197, Test Error: 1.0748984298330346\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.59780054231352, Test Error: 1.0752159757494268\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.61222846595304, Test Error: 1.0755533904373342\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.62653954497812, Test Error: 1.0759555296385697\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.68263881086537, Test Error: 1.0782202774867229\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.71000130740406, Test Error: 1.0797263815126494\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 1.0702225722803902\n",
      "Selected 40 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 1.3919846550288077, Test Error: 0.9416836914092327\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 1.3959464587402635, Test Error: 0.9448444178640315\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 1.396754038503368, Test Error: 0.9456854586819551\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 1.3978574513531665, Test Error: 0.9474382093479659\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 1.3992522135303878, Test Error: 0.9500659839503884\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 1.407706170620102, Test Error: 0.9685690500336616\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 1.4136908536912325, Test Error: 0.9820776023611608\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.9416836914092327\n",
      "Number of stocks: 40, Tracking Error: 8.979710899987065\n",
      "Starting portofolio with 50 stocks\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.004608580710167674, Test Error: 1.2819622418535177\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.007701327325853519, Test Error: 1.1893031666236793\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.015346734102490378, Test Error: 1.1649470652695653\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.024235696077491802, Test Error: 1.0531793604099535\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.03258579082285787, Test Error: 0.9854809136457551\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.06566638416640175, Test Error: 0.9093629052143287\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.07887163815179153, Test Error: 0.8587312485990878\n",
      "Best Params for this fold: {'alpha': 0.0025, 'max_iter': 15000}, Best Test Score: 0.8587312485990878\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6295231543660631, Test Error: 0.7254478747041605\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.652227542393451, Test Error: 0.7104317643052914\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6926505239917735, Test Error: 0.7205437006560742\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7314827584621226, Test Error: 0.7459642421802728\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7401051350402258, Test Error: 0.7480663931974978\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.7872491783278941, Test Error: 0.7998792142164447\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.805892760545729, Test Error: 0.8333542093865316\n",
      "Best Params for this fold: {'alpha': 0.00025, 'max_iter': 15000}, Best Test Score: 0.7104317643052914\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7447986853383755, Test Error: 0.9344898367796347\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7659314783952087, Test Error: 0.9489774630007577\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.812539174938632, Test Error: 0.9957377843812387\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8213803979306541, Test Error: 1.01008581949927\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8280658189809948, Test Error: 1.0198156856029235\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8588517828207285, Test Error: 1.0555132220396726\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8651523574178225, Test Error: 1.0650002674108077\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.9344898367796347\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.8246279138220208, Test Error: 0.7515952708362671\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.8329158701590728, Test Error: 0.7602959740566458\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.8438922891237576, Test Error: 0.7688996578224997\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8540925821739843, Test Error: 0.7744431480027745\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8661441352015472, Test Error: 0.7790159322201782\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.9313591456785864, Test Error: 0.7943644962408112\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.949251714468526, Test Error: 0.8004139502187094\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7515952708362671\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.843766673638145, Test Error: 0.45899252023849063\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.8485844767128042, Test Error: 0.4613537038616406\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.8568113362705571, Test Error: 0.46479251657254034\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8649766595997449, Test Error: 0.46633053744764047\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8742306460269819, Test Error: 0.46860404186856564\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.9220866299832714, Test Error: 0.48504572615780267\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.9449432212113063, Test Error: 0.49408523433479323\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.45899252023849063\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.738920533726653, Test Error: 0.5933911096837717\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7426412453160803, Test Error: 0.6064193287065901\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7493249929823155, Test Error: 0.6291275683327534\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7565762865742495, Test Error: 0.652880217551014\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7643827076500729, Test Error: 0.6774783206666645\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8018478568571584, Test Error: 0.7814429527805568\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8253661346672079, Test Error: 0.8354324472658903\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.5933911096837717\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6003744051263314, Test Error: 0.5668799487404038\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.6068602311642094, Test Error: 0.5647810574352016\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6166668694580952, Test Error: 0.5629720545447938\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.6267353425651764, Test Error: 0.56276210106718\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.6371129941572151, Test Error: 0.5637610427680311\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.6818443202323585, Test Error: 0.5793512558372758\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.705499635001686, Test Error: 0.5928591093508823\n",
      "Best Params for this fold: {'alpha': 0.00075, 'max_iter': 15000}, Best Test Score: 0.56276210106718\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.5568438301972111, Test Error: 0.7618189249803691\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.5721209628408591, Test Error: 0.7633595900960605\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6029498973755977, Test Error: 0.7809281096331236\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.6386528929035761, Test Error: 0.8185391922679432\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.6771638164655034, Test Error: 0.8791636906990058\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7618189249803691\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6576862997150438, Test Error: 1.5627664232891763\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.6706995016691449, Test Error: 1.5670120866241741\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6924917497432755, Test Error: 1.563985774895383\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7173492641477593, Test Error: 1.5610133639559989\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7459811352131563, Test Error: 1.5595155751753116\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Best Params for this fold: {'alpha': 0.002, 'max_iter': 15000}, Best Test Score: 1.5595095551463602\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 1.0379631413597399, Test Error: 117.52839331997298\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 1.049716155173432, Test Error: 117.68833834741103\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 1.059727333545071, Test Error: 117.78491146676824\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 1.0703631780697498, Test Error: 117.8828258518872\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 1.0811581162026835, Test Error: 117.98597487505185\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 1.1212032227642716, Test Error: 118.33133068887688\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 1.121658673014405, Test Error: 118.33432326825329\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 117.52839331997298\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.0879559066347, Test Error: 5.467621515673988\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.31181146066722, Test Error: 5.4685604638874645\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.60947966551414, Test Error: 5.470446613513069\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.61570471790981, Test Error: 5.470560168486772\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.62679171623483, Test Error: 5.471015914004893\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.66784214190581, Test Error: 5.473763919552488\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.68796867494305, Test Error: 5.475179427440903\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 5.467621515673988\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.60270880391045, Test Error: 2.149953887475047\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.63766784781512, Test Error: 2.154089363547127\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.63330317443811, Test Error: 2.156563357751575\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.64285023110047, Test Error: 2.1568832530479227\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.6523455813869, Test Error: 2.15720674048743\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.68980682072423, Test Error: 2.158533035208891\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.70822588620685, Test Error: 2.159213410856688\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 2.149953887475047\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 59.46254243171045, Test Error: 0.7094692997776597\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 59.69439576142577, Test Error: 0.7205844211524771\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 60.340868179743005, Test Error: 0.7224914546428562\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 60.99257076665097, Test Error: 0.7244166697719413\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 61.62818073264051, Test Error: 0.7267199201442585\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 63.94981157088992, Test Error: 0.7391265803382013\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.76482157877555, Test Error: 0.745288868577775\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7094692997776597\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.63880322568576, Test Error: 1.0697029244601026\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.63897775190895, Test Error: 1.0718934988460633\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.64775875039287, Test Error: 1.0733933563467541\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.66775784079285, Test Error: 1.0737727076409378\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.68612549074625, Test Error: 1.074512370046863\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.75737084660217, Test Error: 1.078614103239579\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.79086788119767, Test Error: 1.0815806443879523\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 1.0697029244601026\n",
      "Selected 50 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 1.3972878339791417, Test Error: 0.9502055133753028\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 1.3992677057672052, Test Error: 0.9534443579272376\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 1.4022990210331605, Test Error: 0.9582782833227741\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 1.4056471234780121, Test Error: 0.96510085153219\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 1.4084352983542883, Test Error: 0.9717904368118238\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 1.42120944121133, Test Error: 1.000892850427143\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 1.4291122718886524, Test Error: 1.0171316829056265\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.9502055133753028\n",
      "Number of stocks: 50, Tracking Error: 9.004471252824768\n",
      "Starting portofolio with 60 stocks\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.008538345770949347, Test Error: 0.5221444203977329\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.018626847855354484, Test Error: 0.49530936716966256\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.031373136045096894, Test Error: 0.491957446624239\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.040020475176941725, Test Error: 0.5007498324682628\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.04216205224147551, Test Error: 0.5046993933842928\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.03056868655585579, Test Error: 0.5103792219848192\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.03369844259213747, Test Error: 0.5099776422350657\n",
      "Best Params for this fold: {'alpha': 0.0005, 'max_iter': 15000}, Best Test Score: 0.491957446624239\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7240539272220304, Test Error: 0.8380366241654648\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7367940454248333, Test Error: 0.8128237327379745\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.752204038297184, Test Error: 0.7733405863405853\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7707428182450476, Test Error: 0.7749556898425979\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.7938650115892251, Test Error: 0.7975892054826431\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8491727983111034, Test Error: 0.8992196994613812\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8491727983111034, Test Error: 0.8992196994613812\n",
      "Best Params for this fold: {'alpha': 0.0005, 'max_iter': 15000}, Best Test Score: 0.7733405863405853\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7423409688942569, Test Error: 0.9391586781743505\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7726200539746361, Test Error: 0.9540083032623575\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7903409447002613, Test Error: 0.9722210881146847\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8001774677263406, Test Error: 0.9878115478383717\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.81070749085202, Test Error: 1.0033662007745259\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8482450882506004, Test Error: 1.0511889896804008\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8606489453822754, Test Error: 1.0638167852031448\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.9391586781743505\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.8348265325151745, Test Error: 0.7658893672561984\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.8436691885228753, Test Error: 0.7759390323596779\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.8563402477393149, Test Error: 0.7869458588489292\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8705600107482162, Test Error: 0.7933803870099991\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8874475586106642, Test Error: 0.7963249689538664\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.949251714468526, Test Error: 0.8004139502187094\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.949251714468526, Test Error: 0.8004139502187094\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7658893672561984\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.8414798885913537, Test Error: 0.45332448769918915\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.8485535223993803, Test Error: 0.4531260236414745\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.8566979815966862, Test Error: 0.45506764156490503\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.8652337153664718, Test Error: 0.45760180098794995\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8741039886268503, Test Error: 0.46070897081841233\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.9132816611759385, Test Error: 0.4784286230154762\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.9367216911833219, Test Error: 0.49013074801507794\n",
      "Best Params for this fold: {'alpha': 0.00025, 'max_iter': 15000}, Best Test Score: 0.4531260236414745\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.7672464210489542, Test Error: 0.6706878559187206\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.7754007946040107, Test Error: 0.6962792777759995\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.7860067203976213, Test Error: 0.7306688427001149\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7924247705734145, Test Error: 0.7530135182683261\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.8000981031678122, Test Error: 0.775943563635413\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.8433220714707347, Test Error: 0.8718191915614952\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.8439234993490597, Test Error: 0.8729176589648964\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.6706878559187206\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.5977811591155632, Test Error: 0.5642733645120461\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.604151284721774, Test Error: 0.5632787892017824\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.615303709066132, Test Error: 0.5628816143383794\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.6270661284289987, Test Error: 0.5640137337200674\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.6394088429394408, Test Error: 0.566612418765203\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.6952085635192555, Test Error: 0.5901566916049412\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.724751511670933, Test Error: 0.6067068266463878\n",
      "Best Params for this fold: {'alpha': 0.0005, 'max_iter': 15000}, Best Test Score: 0.5628816143383794\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.5649253794188575, Test Error: 0.7663225671030414\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.5894025942320917, Test Error: 0.7759135014678632\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6356873019219751, Test Error: 0.8176363758967683\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.6802912623000261, Test Error: 0.8846865200115173\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7663225671030414\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 0.6498049377824485, Test Error: 1.5415289218940809\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 0.6662129244098891, Test Error: 1.5447292459555906\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 0.6898203463606902, Test Error: 1.5493994941478317\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 0.7164678009455812, Test Error: 1.552740149933144\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 0.747338128090091, Test Error: 1.5566384304517118\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 0.7784606459526128, Test Error: 1.5595095551463602\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 1.5415289218940809\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 1.0355030247591113, Test Error: 116.64691841009183\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 1.0470174215609538, Test Error: 117.3751164964498\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 1.0594068659438163, Test Error: 117.6110324181875\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 1.073079449203127, Test Error: 117.82166417693536\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 1.0866572055650388, Test Error: 117.99896665952961\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 1.121658673014405, Test Error: 118.33432326825329\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 1.121658673014405, Test Error: 118.33432326825329\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 116.64691841009183\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 59.006454611285264, Test Error: 5.486621065507992\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.13356988759327, Test Error: 5.457721634078525\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.63583804298642, Test Error: 5.478747181726705\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.65519575990254, Test Error: 5.481518790569416\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.67182536810526, Test Error: 5.483552860798669\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.72490833540466, Test Error: 5.486818516230698\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.7507945641438, Test Error: 5.48850905650891\n",
      "Best Params for this fold: {'alpha': 0.00025, 'max_iter': 15000}, Best Test Score: 5.457721634078525\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 63.0552031398332, Test Error: 2.182312916008326\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 63.97341052808885, Test Error: 2.153708601499566\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.64063581790546, Test Error: 2.1566144397219915\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.65101599235024, Test Error: 2.156999822769813\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.66133571424373, Test Error: 2.157387237691025\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.70201249686173, Test Error: 2.1589556693232534\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.72198871867305, Test Error: 2.1597503088553367\n",
      "Best Params for this fold: {'alpha': 0.00025, 'max_iter': 15000}, Best Test Score: 2.153708601499566\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 63.262192878354824, Test Error: 0.7337686720510428\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.63195639571492, Test Error: 0.7382566855288676\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.64932688564323, Test Error: 0.739044401911654\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.66651062581103, Test Error: 0.7398737307899282\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.68350739929681, Test Error: 0.7407438993333008\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.74962599864793, Test Error: 0.7446177847115213\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.78156936399921, Test Error: 0.7467810930276613\n",
      "Best Params for this fold: {'alpha': 0.0001, 'max_iter': 15000}, Best Test Score: 0.7337686720510428\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 64.68853781135815, Test Error: 1.0744300971565164\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 64.69236374972516, Test Error: 1.0701730676586945\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 64.71014906747664, Test Error: 1.0695976203222197\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 64.72796014681276, Test Error: 1.069080033614881\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 64.74270266106785, Test Error: 1.0690052437738171\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 64.7943955268059, Test Error: 1.0704278290448008\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 64.81974014850516, Test Error: 1.0716766887997708\n",
      "Best Params for this fold: {'alpha': 0.001, 'max_iter': 15000}, Best Test Score: 1.0690052437738171\n",
      "Selected 60 features\n",
      "Trying combination {'alpha': 0.0001, 'max_iter': 15000}\n",
      "Train Error: 1.3946262999608918, Test Error: 0.9540617423426084\n",
      "Trying combination {'alpha': 0.00025, 'max_iter': 15000}\n",
      "Train Error: 1.3958410796663758, Test Error: 0.9531704502355128\n",
      "Trying combination {'alpha': 0.0005, 'max_iter': 15000}\n",
      "Train Error: 1.3986326371867142, Test Error: 0.9542657806343928\n",
      "Trying combination {'alpha': 0.00075, 'max_iter': 15000}\n",
      "Train Error: 1.4023512573453427, Test Error: 0.95849190213991\n",
      "Trying combination {'alpha': 0.001, 'max_iter': 15000}\n",
      "Train Error: 1.4070948685938733, Test Error: 0.9660299347749056\n",
      "Trying combination {'alpha': 0.002, 'max_iter': 15000}\n",
      "Train Error: 1.436302713539706, Test Error: 1.0285888090798443\n",
      "Trying combination {'alpha': 0.0025, 'max_iter': 15000}\n",
      "Train Error: 1.4546191522264755, Test Error: 1.0612770829058231\n",
      "Best Params for this fold: {'alpha': 0.00025, 'max_iter': 15000}, Best Test Score: 0.9531704502355128\n",
      "Number of stocks: 60, Tracking Error: 8.931945738201426\n",
      "[] [8.930573665011213, 8.98080711167833, 8.979710899987065, 9.004471252824768, 8.931945738201426]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_AE_Lasso(hourly_X,hourly_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96880e20",
   "metadata": {
    "papermill": {
     "duration": 0.274378,
     "end_time": "2024-12-30T13:43:43.928133",
     "exception": false,
     "start_time": "2024-12-30T13:43:43.653755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6677f",
   "metadata": {
    "papermill": {
     "duration": 0.216529,
     "end_time": "2024-12-30T13:43:44.354843",
     "exception": false,
     "start_time": "2024-12-30T13:43:44.138314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6024843,
     "sourceId": 9824880,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2401195,
     "sourceId": 9867252,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2490157,
     "sourceId": 9876776,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7559.735012,
   "end_time": "2024-12-30T13:43:47.305724",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-30T11:37:47.570712",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
